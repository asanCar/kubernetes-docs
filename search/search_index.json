{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apuntes Kubernetes Documentaci\u00f3n Kubernetes \u00cdndice examen CKAD Core Concepts (13%) Entender las primitivas de Kubernetes (API objects) Crear y configurar Pods b\u00e1sicos Configuration (18%) Entender ConfigMaps Entender SecurityContexts Definir los recursos que requiere una aplicaci\u00f3n Crear y consumir Secrets Entender ServiceAccounts Multi-Container Pods (10%) Entender los patrones de dise\u00f1o Multi-contenedor para Pods (ambassador, adapter, sidecar) Observability (18%) Entender LivenessProbes y ReadinessProbes Entender logging de los contenedores Entender c\u00f3mo monitorizar aplicaciones en Kubernetes Entender c\u00f3mo debugar en Kubernetes Pod Design (20%) Entender Deployments y saber hacer rolling updates y rollbacks Entender Jobs y CronJobs Entender c\u00f3mo utilizar Labels, Selectors y Annotations Services & Networking (13%) Entender Services Demostrar conocimiento b\u00e1sico de NetworkPolicies State Persistence (8%) Entender PersistentVolumeClaims \u00cdndice examen CKA (2020) \u00cdndice oficial Cluster Architecture, Installation & Configuration (25%) Gestionar RBAC (Role Based Access Control) Saber utilizar kubeadm para instalar un cluster b\u00e1sico Saber gestionar un cluster en HA (alta disponibilidad) Saber aprovisionar la infraestructura necesaria para desplegar un cluster Kubernetes Saber realizar una actualizaci\u00f3n de versi\u00f3n de Kubernetes con kubeadm Saber realizar backups y restores del etcd Workloads & Scheduling (15%) Entender los Deployment y saber realizar rolling updates y rollbacks Saber utilizar ConfigMaps y Secrets para configurar aplicaciones Saber c\u00f3mo escalar aplicaciones Entender las primitivas (API objects) que permiten crear despliegues de aplicaciones robustas y auto-reparadoras Entender c\u00f3mo los l\u00edmites de recursos pueden afectar al despliegue de Pods (Scheduler) Estar al d\u00eda de la gesti\u00f3n de manifests y las herramientas de plantillado m\u00e1s com\u00fanes Services & Networking (20%) Entender la configuraci\u00f3n de networking de los nodos del cluster Entender la conectividad entre Pods Entender los Endpoints y los tipos de Service ClusterIP, NodePort y LoadBalancer Saber c\u00f3mo utilizar los Ingress Controllers y los recursos Ingress Saber configurar y utilizar CoreDNS Elegir un plugin Container Network Interface apropiado Storage (10%) Entender Storage Classes, Persistent Volumes Entender Volume Modes, Access Modes y Reclaim Policies para Volumes Entender objetos Persistent Volume Claims Saber configurar aplicaciones con almacenamiento persistente Troubleshooting (30%) Evaluar cluster y logs del nodo Entender como monitorizar aplicaciones Gestionar stdout y stderr de los logs de los contenedores Troubleshoot fallos en una aplicaci\u00f3n Troubleshoot fallos en un componente del cluster Troubleshoot networking Tips No escribir los YAML desde 0. Utilizar kubectl run --generator=run-pod/v1 mypod --image=nginx -oyaml --dry-run . Utilizar kubectl explain antes de recurrir a la Documentaci\u00f3n oficial Por ejemplo, para ver las opciones de un LivenessProbe: kubectl explain pod.spec.containers.livenessProbe Utilizar alias: alias k = kubectl alias kd = 'kubectl describe' alias ke = 'kubectl explain' ## Consultar documentaci\u00f3n alias kc = 'kubectl config use-context' ## Cambiar de contexto alias ks = 'kubectl config set-context --current --namespace' ## Crear nuevo contexto - No esperar a que los Pods se eliminen: kubeclt delete pod nginx --grace-period = 0 --force - Preparar un bookmark con muchos ejemplos que poder consultar r\u00e1pidamente: ejemplo Dejar las preguntas largas y con poca puntuaci\u00f3n para el final.","title":"Home"},{"location":"#apuntes-kubernetes","text":"Documentaci\u00f3n Kubernetes","title":"Apuntes Kubernetes"},{"location":"#indice-examen-ckad","text":"Core Concepts (13%) Entender las primitivas de Kubernetes (API objects) Crear y configurar Pods b\u00e1sicos Configuration (18%) Entender ConfigMaps Entender SecurityContexts Definir los recursos que requiere una aplicaci\u00f3n Crear y consumir Secrets Entender ServiceAccounts Multi-Container Pods (10%) Entender los patrones de dise\u00f1o Multi-contenedor para Pods (ambassador, adapter, sidecar) Observability (18%) Entender LivenessProbes y ReadinessProbes Entender logging de los contenedores Entender c\u00f3mo monitorizar aplicaciones en Kubernetes Entender c\u00f3mo debugar en Kubernetes Pod Design (20%) Entender Deployments y saber hacer rolling updates y rollbacks Entender Jobs y CronJobs Entender c\u00f3mo utilizar Labels, Selectors y Annotations Services & Networking (13%) Entender Services Demostrar conocimiento b\u00e1sico de NetworkPolicies State Persistence (8%) Entender PersistentVolumeClaims","title":"\u00cdndice examen CKAD"},{"location":"#indice-examen-cka-2020","text":"\u00cdndice oficial Cluster Architecture, Installation & Configuration (25%) Gestionar RBAC (Role Based Access Control) Saber utilizar kubeadm para instalar un cluster b\u00e1sico Saber gestionar un cluster en HA (alta disponibilidad) Saber aprovisionar la infraestructura necesaria para desplegar un cluster Kubernetes Saber realizar una actualizaci\u00f3n de versi\u00f3n de Kubernetes con kubeadm Saber realizar backups y restores del etcd Workloads & Scheduling (15%) Entender los Deployment y saber realizar rolling updates y rollbacks Saber utilizar ConfigMaps y Secrets para configurar aplicaciones Saber c\u00f3mo escalar aplicaciones Entender las primitivas (API objects) que permiten crear despliegues de aplicaciones robustas y auto-reparadoras Entender c\u00f3mo los l\u00edmites de recursos pueden afectar al despliegue de Pods (Scheduler) Estar al d\u00eda de la gesti\u00f3n de manifests y las herramientas de plantillado m\u00e1s com\u00fanes Services & Networking (20%) Entender la configuraci\u00f3n de networking de los nodos del cluster Entender la conectividad entre Pods Entender los Endpoints y los tipos de Service ClusterIP, NodePort y LoadBalancer Saber c\u00f3mo utilizar los Ingress Controllers y los recursos Ingress Saber configurar y utilizar CoreDNS Elegir un plugin Container Network Interface apropiado Storage (10%) Entender Storage Classes, Persistent Volumes Entender Volume Modes, Access Modes y Reclaim Policies para Volumes Entender objetos Persistent Volume Claims Saber configurar aplicaciones con almacenamiento persistente Troubleshooting (30%) Evaluar cluster y logs del nodo Entender como monitorizar aplicaciones Gestionar stdout y stderr de los logs de los contenedores Troubleshoot fallos en una aplicaci\u00f3n Troubleshoot fallos en un componente del cluster Troubleshoot networking","title":"\u00cdndice examen CKA (2020)"},{"location":"#tips","text":"No escribir los YAML desde 0. Utilizar kubectl run --generator=run-pod/v1 mypod --image=nginx -oyaml --dry-run . Utilizar kubectl explain antes de recurrir a la Documentaci\u00f3n oficial Por ejemplo, para ver las opciones de un LivenessProbe: kubectl explain pod.spec.containers.livenessProbe Utilizar alias: alias k = kubectl alias kd = 'kubectl describe' alias ke = 'kubectl explain' ## Consultar documentaci\u00f3n alias kc = 'kubectl config use-context' ## Cambiar de contexto alias ks = 'kubectl config set-context --current --namespace' ## Crear nuevo contexto - No esperar a que los Pods se eliminen: kubeclt delete pod nginx --grace-period = 0 --force - Preparar un bookmark con muchos ejemplos que poder consultar r\u00e1pidamente: ejemplo Dejar las preguntas largas y con poca puntuaci\u00f3n para el final.","title":"Tips"},{"location":"components/components/","text":"Overview Un cluster Kubernetes esta formado por un grupo de nodos worker y un \"Control Plane\" encargado de orquestar esos nodos y todo el ciclo de vida de los contenedores mediante una API. Los componentes que forman un cluster de Kubernetes se podr\u00edan agrupar en dos grupos: Componentes del Control Plane Componentes de los Nodos","title":"Overview"},{"location":"components/components/#overview","text":"Un cluster Kubernetes esta formado por un grupo de nodos worker y un \"Control Plane\" encargado de orquestar esos nodos y todo el ciclo de vida de los contenedores mediante una API. Los componentes que forman un cluster de Kubernetes se podr\u00edan agrupar en dos grupos: Componentes del Control Plane Componentes de los Nodos","title":"Overview"},{"location":"components/control-components/","text":"Componentes del Control Plane Estos componentes son los encargados de tomar decisiones globales sobre el cluster (p.ej. scheduling), asi como detectar y responder ante eventos (p.ej. si una replica de un Pod se cae, levantar una nueva replica). Componentes: kube-apiserver etcd kube-scheduler kube-controller-manager cloud-controller-manager kube-apiserver Es la pieza principal del cluster Kubernetes. Es el componente encargado de exponer la API de Kubernetes y act\u00faa como front end del Control Plane. Se trata del \u00fanico componente que interact\u00faa directamente con etcd . Esta pensado para que escale horizontalmente, por lo que pueden existir m\u00faltiples instancias a la vez y balancear las peticiones que reciba el cluster. Ejemplo del proceso de creaci\u00f3n de un nuevo Pod Autenticaci\u00f3n Autorizaci\u00f3n Obtener el estado actual del etcd Se crea el objeto Pod pero sin alojarse en ning\u00fan worker Se actualiza el estado del etcd y se notifica al usuario que el Pod se ha creado El Scheduler que constantemente esta observando al kube-apiserver , observa que hay un nuevo Pod sin asignar. Busca e identifica un nodo en el que poder desplegar el Pod y se lo notifica al kube-apiserver . El kube-apiserver actualiza el etcd y ordena al kubelet del woker seleccionado que cree un nuevo Pod. El kubelet ordena al Container Runtime Engine que cree un nuevo contenedor con la imagen especificada. Una vez creado el contenedor, el kubelet notifica al kube-apiserver , el cual a su vez actualiza el etcd con la nueva informaci\u00f3n. etcd Base de datos clave-valor en alta disponibilidad utilizada para almacenar todos los datos del cluster. Toda la informaci\u00f3n que devuelve el comando kubectl get [node|pod|secret...] procede del etcd. Cualquier cambio que se aplique sobre el cluster, se considerar\u00e1 finalizado cuando se acabe de actualizarse etcd . etcd se puede instalar directamente sobre una m\u00e1quina del cl\u00faster, para ejecutarse como servicio, o desplegarse como contenedor (p.ej. con kubeadm ). kube-scheduler Componente encargado de observar los Pods reci\u00e9n creados a\u00fan no tengan un nodo asignado y seleccionar un nodo disponible que cumpla los requisitos, para que sea desplegado en \u00e9l. Cuidado Solo se encarga de decidir en qu\u00e9 nodo deben ir los Pods. Quien se encarga de crearlos son los kubelet . Para tomar la decisi\u00f3n se basa en factores como el uso de recursos que requiere el Pod, los recursos disponibles en el nodo, especificaciones de affinity y anti-affinity, localidad de los datos, restricciones por software, hardware, pol\u00edticas... Proceso de elecci\u00f3n de nodo Filtrado de nodos : en funci\u00f3n de los requisitos de memoria y CPU de un Pod (u otros requisitos), se filtrar\u00e1 el listado de nodos disponibles del cluster para quedarse \u00fanicamente con los que cumplan los requisitos del Pod. Ordenar por prioridad: en funci\u00f3n de los recursos disponibles de los nodos, se ordena el listado, puntuando los nodos de 0 a 10, dando mayor puntuaci\u00f3n a los que una vez alojaran el Pod, les quedara m\u00e1s recursos disponibles. kube-controller-manager Este componente representado en un \u00fanico binario, se encarga de ejecutar varios procesos de control, encargados de observar constantemente el estado del cluster mediante el kube-apiserver y aplicar cambios en el estado actual para conseguir el estado deseado. Los Controllers que incluye son: Node Controller: Responsable de notificar y actuar cuando alg\u00fan nodo se cae. Por defecto, monitoriza los nodos cada 5 segundos. Si alg\u00fan nodo no responde, se le da un periodo de gracia de 40 segundos. Si pasado este tiempo no responde, se marca el nodo como UNREACHABLE . Si al cabo de 5 minutos el nodo no se ha recuperado, los Pods que formaran parte de un ReplicaSet, se realojar\u00e1n en el resto de nodos disponibles. Replication Controller: Responsable de mantener el n\u00famero correcto de Pods indicado en cada uno de los objetos \"Replication Controller\" del sistema. Endpoint Controller: Genera los objetos Endpoints (para unir Services con Pods). Service Account & Token Controllers: Responsable de generar las cuentas y API access tokens por defecto de los nuevos Namespaces. cloud-controller-manager Este componente \u00fanicamente se utiliza cuando se despliega Kubernetes en un proveedor Cloud. Contiene l\u00f3gica de control espec\u00edfica del proveedor Cloud para integrarse con su API y gestionar los componentes que interact\u00faan con el proveedor Cloud de forma independiente, respecto al resto de componentes que \u00fanicamente interact\u00faan con el cluster Kubernetes.","title":"Componentes del Control Plane"},{"location":"components/control-components/#componentes-del-control-plane","text":"Estos componentes son los encargados de tomar decisiones globales sobre el cluster (p.ej. scheduling), asi como detectar y responder ante eventos (p.ej. si una replica de un Pod se cae, levantar una nueva replica). Componentes: kube-apiserver etcd kube-scheduler kube-controller-manager cloud-controller-manager","title":"Componentes del Control Plane"},{"location":"components/control-components/#kube-apiserver","text":"Es la pieza principal del cluster Kubernetes. Es el componente encargado de exponer la API de Kubernetes y act\u00faa como front end del Control Plane. Se trata del \u00fanico componente que interact\u00faa directamente con etcd . Esta pensado para que escale horizontalmente, por lo que pueden existir m\u00faltiples instancias a la vez y balancear las peticiones que reciba el cluster. Ejemplo del proceso de creaci\u00f3n de un nuevo Pod Autenticaci\u00f3n Autorizaci\u00f3n Obtener el estado actual del etcd Se crea el objeto Pod pero sin alojarse en ning\u00fan worker Se actualiza el estado del etcd y se notifica al usuario que el Pod se ha creado El Scheduler que constantemente esta observando al kube-apiserver , observa que hay un nuevo Pod sin asignar. Busca e identifica un nodo en el que poder desplegar el Pod y se lo notifica al kube-apiserver . El kube-apiserver actualiza el etcd y ordena al kubelet del woker seleccionado que cree un nuevo Pod. El kubelet ordena al Container Runtime Engine que cree un nuevo contenedor con la imagen especificada. Una vez creado el contenedor, el kubelet notifica al kube-apiserver , el cual a su vez actualiza el etcd con la nueva informaci\u00f3n.","title":"kube-apiserver"},{"location":"components/control-components/#etcd","text":"Base de datos clave-valor en alta disponibilidad utilizada para almacenar todos los datos del cluster. Toda la informaci\u00f3n que devuelve el comando kubectl get [node|pod|secret...] procede del etcd. Cualquier cambio que se aplique sobre el cluster, se considerar\u00e1 finalizado cuando se acabe de actualizarse etcd . etcd se puede instalar directamente sobre una m\u00e1quina del cl\u00faster, para ejecutarse como servicio, o desplegarse como contenedor (p.ej. con kubeadm ).","title":"etcd"},{"location":"components/control-components/#kube-scheduler","text":"Componente encargado de observar los Pods reci\u00e9n creados a\u00fan no tengan un nodo asignado y seleccionar un nodo disponible que cumpla los requisitos, para que sea desplegado en \u00e9l. Cuidado Solo se encarga de decidir en qu\u00e9 nodo deben ir los Pods. Quien se encarga de crearlos son los kubelet . Para tomar la decisi\u00f3n se basa en factores como el uso de recursos que requiere el Pod, los recursos disponibles en el nodo, especificaciones de affinity y anti-affinity, localidad de los datos, restricciones por software, hardware, pol\u00edticas... Proceso de elecci\u00f3n de nodo Filtrado de nodos : en funci\u00f3n de los requisitos de memoria y CPU de un Pod (u otros requisitos), se filtrar\u00e1 el listado de nodos disponibles del cluster para quedarse \u00fanicamente con los que cumplan los requisitos del Pod. Ordenar por prioridad: en funci\u00f3n de los recursos disponibles de los nodos, se ordena el listado, puntuando los nodos de 0 a 10, dando mayor puntuaci\u00f3n a los que una vez alojaran el Pod, les quedara m\u00e1s recursos disponibles.","title":"kube-scheduler"},{"location":"components/control-components/#kube-controller-manager","text":"Este componente representado en un \u00fanico binario, se encarga de ejecutar varios procesos de control, encargados de observar constantemente el estado del cluster mediante el kube-apiserver y aplicar cambios en el estado actual para conseguir el estado deseado. Los Controllers que incluye son: Node Controller: Responsable de notificar y actuar cuando alg\u00fan nodo se cae. Por defecto, monitoriza los nodos cada 5 segundos. Si alg\u00fan nodo no responde, se le da un periodo de gracia de 40 segundos. Si pasado este tiempo no responde, se marca el nodo como UNREACHABLE . Si al cabo de 5 minutos el nodo no se ha recuperado, los Pods que formaran parte de un ReplicaSet, se realojar\u00e1n en el resto de nodos disponibles. Replication Controller: Responsable de mantener el n\u00famero correcto de Pods indicado en cada uno de los objetos \"Replication Controller\" del sistema. Endpoint Controller: Genera los objetos Endpoints (para unir Services con Pods). Service Account & Token Controllers: Responsable de generar las cuentas y API access tokens por defecto de los nuevos Namespaces.","title":"kube-controller-manager"},{"location":"components/control-components/#cloud-controller-manager","text":"Este componente \u00fanicamente se utiliza cuando se despliega Kubernetes en un proveedor Cloud. Contiene l\u00f3gica de control espec\u00edfica del proveedor Cloud para integrarse con su API y gestionar los componentes que interact\u00faan con el proveedor Cloud de forma independiente, respecto al resto de componentes que \u00fanicamente interact\u00faan con el cluster Kubernetes.","title":"cloud-controller-manager"},{"location":"components/node-components/","text":"Componentes de los nodos Estos componentes est\u00e1n presentes en cada nodo, y se encargan de que los nodos formen parte del entorno de ejecuci\u00f3n de Kubernetes, as\u00ed como de mantener los Pods ejecut\u00e1ndose. Componentes: kubelet kube-proyx Container Runtime Engine kubelet Agente que corre en cada uno de los nodos del cluster, encargado de que los contenedores de un Pod se est\u00e9n ejecutando, siguiendo las especificaciones que indique ese mismo Pod. Tambi\u00e9n se encarga de informar regularmente al kube-apiserver del estado del nodo y de los contenedores que se est\u00e1n ejecutando, as\u00ed como de crear o eliminar Pods en funci\u00f3n de los que indique el Scheduler a trav\u00e9s del kube-apiserver . Instalaci\u00f3n Los kubelet no son servicios instalados por kubeadm , por lo que deben ser instalados de forma manual en cada uno de los nodos. kube-proxy Se trata de un proceso que est\u00e1 a la escucha de que se cree un nuevo Service. Cada vez que detecte un nuevo Service, kube-proxy crear\u00e1 las reglas necesarias en el iptables del nodo para redirigir todo el tr\u00e1fico con destino la IP del Service, hacia las IP de los Pods que balancee ese Service. Este proxy se encarga de gestionar las reglas de red de los nodos que permiten la conexi\u00f3n con los Pods, desde dentro y fuera del cluster. Instalaci\u00f3n A diferencia de los kubelet , los kube-proxy se despliegan como Pods a modo DaemonSet, con kubeadm . Container Runtime Engine Software encargado de ejecutar los contenedores en los nodos. Kubernetes soporta varios Runtime: Docker, CRI-O, containerd...","title":"Componentes de los Nodos"},{"location":"components/node-components/#componentes-de-los-nodos","text":"Estos componentes est\u00e1n presentes en cada nodo, y se encargan de que los nodos formen parte del entorno de ejecuci\u00f3n de Kubernetes, as\u00ed como de mantener los Pods ejecut\u00e1ndose. Componentes: kubelet kube-proyx Container Runtime Engine","title":"Componentes de los nodos"},{"location":"components/node-components/#kubelet","text":"Agente que corre en cada uno de los nodos del cluster, encargado de que los contenedores de un Pod se est\u00e9n ejecutando, siguiendo las especificaciones que indique ese mismo Pod. Tambi\u00e9n se encarga de informar regularmente al kube-apiserver del estado del nodo y de los contenedores que se est\u00e1n ejecutando, as\u00ed como de crear o eliminar Pods en funci\u00f3n de los que indique el Scheduler a trav\u00e9s del kube-apiserver . Instalaci\u00f3n Los kubelet no son servicios instalados por kubeadm , por lo que deben ser instalados de forma manual en cada uno de los nodos.","title":"kubelet"},{"location":"components/node-components/#kube-proxy","text":"Se trata de un proceso que est\u00e1 a la escucha de que se cree un nuevo Service. Cada vez que detecte un nuevo Service, kube-proxy crear\u00e1 las reglas necesarias en el iptables del nodo para redirigir todo el tr\u00e1fico con destino la IP del Service, hacia las IP de los Pods que balancee ese Service. Este proxy se encarga de gestionar las reglas de red de los nodos que permiten la conexi\u00f3n con los Pods, desde dentro y fuera del cluster. Instalaci\u00f3n A diferencia de los kubelet , los kube-proxy se despliegan como Pods a modo DaemonSet, con kubeadm .","title":"kube-proxy"},{"location":"components/node-components/#container-runtime-engine","text":"Software encargado de ejecutar los contenedores en los nodos. Kubernetes soporta varios Runtime: Docker, CRI-O, containerd...","title":"Container Runtime Engine"},{"location":"configuration/best-practices/","text":"Best Practices Services Cuando el load balancing del kube-proxy no es necesario, se puede utilizar un Headless Service para facilitar el \"Service Discovery\". Labels Definir m\u00faltiples labels en los Pods que identifiquen varios atributos de la aplicaci\u00f3n, permite afinar qu\u00e9 Pods deben seleccionar otros recursos. (p.ej. app: myapp, tier: frontend, phase: test, deployment: v3 ) Exponer a trav\u00e9s de un \u00fanico Service , m\u00faltiples Deployments de una misma aplicaci\u00f3n (cada uno de ellos con una version diferente de release), utilizando labels que no difieran entre releases, permite actualizar una aplicaci\u00f3n sin ning\u00fan tipo de downtime . Se pueden manipular las labels de un Pod para tareas de debugging. Se puede \"aislar\" un Pod modificando sus labels. De esta forma si hab\u00eda alg\u00fan Controller que lo gestionara, dejar\u00e1 de tener control sobre \u00e9l, y si tuviera alg\u00fan Service, dejar\u00eda de enviarle tr\u00e1fico. As\u00ed se puede debugar un Pod \"en vivo\", sin afectar a los usuarios. Image Pull Policy Evitar el uso de la tag :latest . Dificulta identificar la versi\u00f3n que se ha desplegado. Para verificar que siempre se utiliza exactamente la misma versi\u00f3n de una im\u00e1gen, se puede utilizar el digest en vez de la tag de la imagen. En vez de utilizar <image-name>:<tag> , utilizar <image-name>@<digest> (p.ej. myimage@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 ). La propiedad imagePullPolicy: Always se sustenta en la cache Docker, por lo que a la hora de descargar una nueva versi\u00f3n de una im\u00e1gen Docker, reutiliza capas anteriores que est\u00e9n en la cache. Para evitar descargar siempre la im\u00e1gen Docker, se puede utilizar imagePullPolicy: IfNotPresent .","title":"Best Practices"},{"location":"configuration/best-practices/#best-practices","text":"","title":"Best Practices"},{"location":"configuration/best-practices/#services","text":"Cuando el load balancing del kube-proxy no es necesario, se puede utilizar un Headless Service para facilitar el \"Service Discovery\".","title":"Services"},{"location":"configuration/best-practices/#labels","text":"Definir m\u00faltiples labels en los Pods que identifiquen varios atributos de la aplicaci\u00f3n, permite afinar qu\u00e9 Pods deben seleccionar otros recursos. (p.ej. app: myapp, tier: frontend, phase: test, deployment: v3 ) Exponer a trav\u00e9s de un \u00fanico Service , m\u00faltiples Deployments de una misma aplicaci\u00f3n (cada uno de ellos con una version diferente de release), utilizando labels que no difieran entre releases, permite actualizar una aplicaci\u00f3n sin ning\u00fan tipo de downtime . Se pueden manipular las labels de un Pod para tareas de debugging. Se puede \"aislar\" un Pod modificando sus labels. De esta forma si hab\u00eda alg\u00fan Controller que lo gestionara, dejar\u00e1 de tener control sobre \u00e9l, y si tuviera alg\u00fan Service, dejar\u00eda de enviarle tr\u00e1fico. As\u00ed se puede debugar un Pod \"en vivo\", sin afectar a los usuarios.","title":"Labels"},{"location":"configuration/best-practices/#image-pull-policy","text":"Evitar el uso de la tag :latest . Dificulta identificar la versi\u00f3n que se ha desplegado. Para verificar que siempre se utiliza exactamente la misma versi\u00f3n de una im\u00e1gen, se puede utilizar el digest en vez de la tag de la imagen. En vez de utilizar <image-name>:<tag> , utilizar <image-name>@<digest> (p.ej. myimage@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 ). La propiedad imagePullPolicy: Always se sustenta en la cache Docker, por lo que a la hora de descargar una nueva versi\u00f3n de una im\u00e1gen Docker, reutiliza capas anteriores que est\u00e9n en la cache. Para evitar descargar siempre la im\u00e1gen Docker, se puede utilizar imagePullPolicy: IfNotPresent .","title":"Image Pull Policy"},{"location":"configuration/configmap/","text":"ConfigMap Este es un recurso de la API de Kubernetes que sirve para almacenar datos \"no confidenciales\", en parejas clave-valor. Los ConfigMap permiten desacoplar de la im\u00e1gen del contenedor, toda la configuraci\u00f3n espec\u00edfica del entorno de despliegue. Haciendo as\u00ed que las aplicaciones que contienen, sean m\u00e1s portables entre distintos entornos. Informaci\u00f3n Los Pods pueden consumir ConfigMaps de 3 formas (pueden utilizarse conjuntamente): Como variables de entorno: Utilizado solo algunas de las keys del ConfigMap apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game env : - name : APP_COLOR valueFrom : configMapKeyRef : name : game-demo key : APP_COLOR - name : UI_PROPERTIES_FILE_NAME valueFrom : configMapKeyRef : name : game-demo key : ui_properties_file_name Utilizado todas las keys del ConfigMap apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game envFrom : - configMapKeyRef : name : game-demo Como argumentos para el ENTRYPOINT del contenedor: apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game args : [ \"$(UI_PROPERTIES_FILE_NAME)\" ] # El operador \"$()\" en Kubernetes sirve env : # para expandir variables en las stanzas - name : UI_PROPERTIES_FILE_NAME # 'command' o 'args' valueFrom : configMapKeyRef : name : game-demo key : ui_properties_file_name Como ficheros de configuraci\u00f3n montados en un Volume: apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game volumeMounts : - name : config mountPath : \"/config\" readOnly : true volumes : - name : config configMap : name : game-demo items : - key : \"game.properties\" path : \"game.properties\" - key : \"user-interface.properties\" path : \"user-interface.properties\" En este ejemplo, mediante la stanza items , se especifican \u00fanicamente las keys del ConfigMap que se quieren montar como ficheros en el Volume. Si no se especificara, montar\u00eda un fichero por cada key que contuviera el ConfigMap . Notas Los ConfigMap en vez de utilizar la stanza .spec utilizan .data . Un Pod solo puede consumir los ConfigMap que est\u00e9n en su mismo Namespace (a no ser que la aplicaci\u00f3n del Pod utilice directamente la API de Kubernetes, en cuyo caso podr\u00eda utilizar cualquier ConfigMap ). La stanza volumeMounts de un Pod que monte un ConfigMap como Volume, debe contener la propiedad readOnly: true . Cada vez que se actualiza un ConfigMap , el kubelet se encarga de actualizar todos los Pods que lo monten como Volume. El tiempo que tarda en actualizar se ve afectado por el tipo de cache que tenga configurada el kubelet . Ejemplo apiVersion : v1 kind : ConfigMap metadata : name : game-demo data : # property-like keys; each key maps to a simple value player_initial_lives : \"3\" ui_properties_file_name : \"user-interface.properties\" APP_COLOR : \"red\" # # file-like keys game.properties : | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties : | color.good=purple color.bad=yellow allow.textmode=true Creaci\u00f3n de ConfigMap con kubectl (Imperativa) Este tipo de recurso se puede crear de forma imperativa mediante comandos kubectl (a diferencia del modo declarativo kubectl apply -f template.yaml ). Se puede crear un ConfigMap de dos formas: A partir de un literal: kubectl create configmap app-config --from-literal = my-key1 = my-value1 --from-literal = my-key2 = my-value2 A partir de un fichero: kubectl create configmap app-config --from-file = my-file.properties Informaci\u00f3n Por defecto para crear las keys se utiliza el nombre de los ficheros. Si se desea especificar otro se debe utilizar el argumento de la forma siguiente --from-file=my-key=./my-file .","title":"ConfigMap"},{"location":"configuration/configmap/#configmap","text":"Este es un recurso de la API de Kubernetes que sirve para almacenar datos \"no confidenciales\", en parejas clave-valor. Los ConfigMap permiten desacoplar de la im\u00e1gen del contenedor, toda la configuraci\u00f3n espec\u00edfica del entorno de despliegue. Haciendo as\u00ed que las aplicaciones que contienen, sean m\u00e1s portables entre distintos entornos. Informaci\u00f3n Los Pods pueden consumir ConfigMaps de 3 formas (pueden utilizarse conjuntamente): Como variables de entorno: Utilizado solo algunas de las keys del ConfigMap apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game env : - name : APP_COLOR valueFrom : configMapKeyRef : name : game-demo key : APP_COLOR - name : UI_PROPERTIES_FILE_NAME valueFrom : configMapKeyRef : name : game-demo key : ui_properties_file_name Utilizado todas las keys del ConfigMap apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game envFrom : - configMapKeyRef : name : game-demo Como argumentos para el ENTRYPOINT del contenedor: apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game args : [ \"$(UI_PROPERTIES_FILE_NAME)\" ] # El operador \"$()\" en Kubernetes sirve env : # para expandir variables en las stanzas - name : UI_PROPERTIES_FILE_NAME # 'command' o 'args' valueFrom : configMapKeyRef : name : game-demo key : ui_properties_file_name Como ficheros de configuraci\u00f3n montados en un Volume: apiVersion : v1 kind : Pod metadata : name : configmap-demo-pod spec : containers : - name : demo image : game.example/demo-game volumeMounts : - name : config mountPath : \"/config\" readOnly : true volumes : - name : config configMap : name : game-demo items : - key : \"game.properties\" path : \"game.properties\" - key : \"user-interface.properties\" path : \"user-interface.properties\" En este ejemplo, mediante la stanza items , se especifican \u00fanicamente las keys del ConfigMap que se quieren montar como ficheros en el Volume. Si no se especificara, montar\u00eda un fichero por cada key que contuviera el ConfigMap . Notas Los ConfigMap en vez de utilizar la stanza .spec utilizan .data . Un Pod solo puede consumir los ConfigMap que est\u00e9n en su mismo Namespace (a no ser que la aplicaci\u00f3n del Pod utilice directamente la API de Kubernetes, en cuyo caso podr\u00eda utilizar cualquier ConfigMap ). La stanza volumeMounts de un Pod que monte un ConfigMap como Volume, debe contener la propiedad readOnly: true . Cada vez que se actualiza un ConfigMap , el kubelet se encarga de actualizar todos los Pods que lo monten como Volume. El tiempo que tarda en actualizar se ve afectado por el tipo de cache que tenga configurada el kubelet . Ejemplo apiVersion : v1 kind : ConfigMap metadata : name : game-demo data : # property-like keys; each key maps to a simple value player_initial_lives : \"3\" ui_properties_file_name : \"user-interface.properties\" APP_COLOR : \"red\" # # file-like keys game.properties : | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties : | color.good=purple color.bad=yellow allow.textmode=true","title":"ConfigMap"},{"location":"configuration/configmap/#creacion-de-configmap-con-kubectl-imperativa","text":"Este tipo de recurso se puede crear de forma imperativa mediante comandos kubectl (a diferencia del modo declarativo kubectl apply -f template.yaml ). Se puede crear un ConfigMap de dos formas: A partir de un literal: kubectl create configmap app-config --from-literal = my-key1 = my-value1 --from-literal = my-key2 = my-value2 A partir de un fichero: kubectl create configmap app-config --from-file = my-file.properties Informaci\u00f3n Por defecto para crear las keys se utiliza el nombre de los ficheros. Si se desea especificar otro se debe utilizar el argumento de la forma siguiente --from-file=my-key=./my-file .","title":"Creaci\u00f3n de ConfigMap con kubectl (Imperativa)"},{"location":"configuration/kubeconfig/","text":"Configurar context del cluster Los context de Kubernetes pueden estar configurados en uno o varios ficheros. Por defecto, se configuran en $HOME/.kube/config . Un context de Kubernetes est\u00e1 compuesto de tres propiedades: cluster : Nombre del cluster. namespace : Nombre del Namespace actual. user : Nombre del usuario con el que autenticarse en el cluster. Los clusters y las credenciales de los usuarios tambi\u00e9n est\u00e1n definidas en el mismo fichero y contienen: Clusters : name : Nombre del cluster. cluster : Contiene las propiedades server con el dominio/IP del cluster y certificate-authority con el path del certificado CA. Tambi\u00e9n se pueden utilizar las propiedades insecure-skip-tls-verify: true para saltarse la validaci\u00f3n SSL o certificate-authority-data para incrustar el certificado CA en el mismo fichero. Users : name : Nombre del usuario. user : Contiene las propiedades username / password o client-certificate / client-key en funci\u00f3n del m\u00e9todo de autenticaci\u00f3n elegido. Tambi\u00e9n se pueden utilizar las propiedades client-certificate-data / client-key-data si se incrustan los certificados en el mismo fichero. Configurar context con kubectl A\u00f1adir cluster: kubectl config --kubeconfig <file_name> set-cluster <cluster_name> --server = https://1.2.3.4 --certificate-authority = fake-ca-file kubectl config --kubeconfig config-demo set-cluster scratch --server = https://5.6.7.8 --insecure-skip-tls-verify A\u00f1adir usuario: kubectl config --kubeconfig <file_name> set-credentials <user_name> --client-certificate = fake-cert-file --client-key = fake-key-seefile kubectl config --kubeconfig config-demo set-credentials experimenter --username = exp --password = some-password Crear context : kubectl config --kubeconfig <file_name> set-context <context_name> --cluster = <cluster_name> --namespace = <namespace> --user = <user_name> kubectl config --kubeconfig config-demo set-context exp-scratch --cluster = scratch --namespace = default --user = experimenter Visualizar fichero kubeconfig : kubectl config --kubeconfig config-demo view Obtener listado de contexts definidos: kubectl config get-contexts Cambiar de context : kubectl config use-context my-context","title":"Configurar context del cluster"},{"location":"configuration/kubeconfig/#configurar-context-del-cluster","text":"Los context de Kubernetes pueden estar configurados en uno o varios ficheros. Por defecto, se configuran en $HOME/.kube/config . Un context de Kubernetes est\u00e1 compuesto de tres propiedades: cluster : Nombre del cluster. namespace : Nombre del Namespace actual. user : Nombre del usuario con el que autenticarse en el cluster. Los clusters y las credenciales de los usuarios tambi\u00e9n est\u00e1n definidas en el mismo fichero y contienen: Clusters : name : Nombre del cluster. cluster : Contiene las propiedades server con el dominio/IP del cluster y certificate-authority con el path del certificado CA. Tambi\u00e9n se pueden utilizar las propiedades insecure-skip-tls-verify: true para saltarse la validaci\u00f3n SSL o certificate-authority-data para incrustar el certificado CA en el mismo fichero. Users : name : Nombre del usuario. user : Contiene las propiedades username / password o client-certificate / client-key en funci\u00f3n del m\u00e9todo de autenticaci\u00f3n elegido. Tambi\u00e9n se pueden utilizar las propiedades client-certificate-data / client-key-data si se incrustan los certificados en el mismo fichero.","title":"Configurar context del cluster"},{"location":"configuration/kubeconfig/#configurar-context-con-kubectl","text":"A\u00f1adir cluster: kubectl config --kubeconfig <file_name> set-cluster <cluster_name> --server = https://1.2.3.4 --certificate-authority = fake-ca-file kubectl config --kubeconfig config-demo set-cluster scratch --server = https://5.6.7.8 --insecure-skip-tls-verify A\u00f1adir usuario: kubectl config --kubeconfig <file_name> set-credentials <user_name> --client-certificate = fake-cert-file --client-key = fake-key-seefile kubectl config --kubeconfig config-demo set-credentials experimenter --username = exp --password = some-password Crear context : kubectl config --kubeconfig <file_name> set-context <context_name> --cluster = <cluster_name> --namespace = <namespace> --user = <user_name> kubectl config --kubeconfig config-demo set-context exp-scratch --cluster = scratch --namespace = default --user = experimenter Visualizar fichero kubeconfig : kubectl config --kubeconfig config-demo view Obtener listado de contexts definidos: kubectl config get-contexts Cambiar de context : kubectl config use-context my-context","title":"Configurar context con kubectl"},{"location":"configuration/pod-overhead/","text":"Pod Overhead Cuando se ejecuta un Pod en un nodo, este consume una cantidad de recursos adicional a los que requieren los contenedores para operar. Pod Overhead es una funcionalidad que permite contabilizar estos consumos adicionales, derivados de la infraestructura del Pod, y que no se tienen en cuenta en los requests/limits de los contenedores. Cuando esta funcionalidad se activa, este consumo adicional se toma en consideraci\u00f3n para el computo total de recursos a reservar que debe tener en cuenta el Scheduler . Esta caracter\u00edstica se configura en los objetos RuntimeClass con la stanza overhead . Esto hace que a todos los Pods creados con esta RuntimeClass , se les a\u00f1ada la propiedad overhead en tiempo de creaci\u00f3n. Cuidado Si un Pod ya tiene definida la stanza overhead , cuando la RuntimeClass intente sobrescribirla dar\u00e1 un error y el Pod no se desplegar\u00e1. Notas El Scheduler toma en consideraci\u00f3n la suma de las request de los contenedores del Pod y el overhead . Ejemplo kind : RuntimeClass apiVersion : node.k8s.io/v1beta1 metadata : name : kata-fc handler : kata-fc overhead : podFixed : memory : \"120Mi\" cpu : \"250m\"","title":"Pod Overhead"},{"location":"configuration/pod-overhead/#pod-overhead","text":"Cuando se ejecuta un Pod en un nodo, este consume una cantidad de recursos adicional a los que requieren los contenedores para operar. Pod Overhead es una funcionalidad que permite contabilizar estos consumos adicionales, derivados de la infraestructura del Pod, y que no se tienen en cuenta en los requests/limits de los contenedores. Cuando esta funcionalidad se activa, este consumo adicional se toma en consideraci\u00f3n para el computo total de recursos a reservar que debe tener en cuenta el Scheduler . Esta caracter\u00edstica se configura en los objetos RuntimeClass con la stanza overhead . Esto hace que a todos los Pods creados con esta RuntimeClass , se les a\u00f1ada la propiedad overhead en tiempo de creaci\u00f3n. Cuidado Si un Pod ya tiene definida la stanza overhead , cuando la RuntimeClass intente sobrescribirla dar\u00e1 un error y el Pod no se desplegar\u00e1. Notas El Scheduler toma en consideraci\u00f3n la suma de las request de los contenedores del Pod y el overhead . Ejemplo kind : RuntimeClass apiVersion : node.k8s.io/v1beta1 metadata : name : kata-fc handler : kata-fc overhead : podFixed : memory : \"120Mi\" cpu : \"250m\"","title":"Pod Overhead"},{"location":"configuration/priority/","text":"Prioridad de los Pods y Apropiaci\u00f3n A los Pods se les puede asignar una prioridad. Esto permite a los Pods m\u00e1s prioritarios, \"apropiarse\" de los recursos que est\u00e9n utilizando otros Pods. El Scheduler desaloja los Pods menos prioritarios de un nodo, liber\u00e1ndo recursos para los Pods m\u00e1s prioritarios y que no tengan espacio disponible para ser desplegados. Para configurar la prioridad de un Pod, primero hay que crear un objeto PriorityClass y a continuaci\u00f3n a\u00f1adir la stanza spec.priorityClassName en el Pod, con el nombre de la PriorityClass . Por defecto, Kubernetes trae ya dos PriorityClass creadas: system-cluster-crucial y system-node-crucial . Siendo la segunda la clase que tiene m\u00e1s prioridad de las dos. Notas Las PriorityClass son recursos que no pertenecen a un Namespace concreto. El nombre de la PriorityClass no puede empezar por system-* . La prioridad de la clase se especifica con la propiedad value . Contra m\u00e1s alto es el valor, m\u00e1s prioridad tiene la clase. Por defecto, los Pods que no lleven la stanza spec.priorityClassName , tendr\u00e1n prioridad con valor 0. Se puede configurar una PriorityClass por defecto para este tipo de Pods, especificando en la clase, la stanza globalDefault: true . Si se configura una clase por defecto, solo se utilizar\u00e1 para los Pods que se vayan a crear nuevos. Los Pods ya existentes seguir\u00e1n con prioridad 0. Las PriorityClass pueden tener una propiedad description para a\u00f1adir una descripci\u00f3n a la clase. Si se elimina una PriorityClass los Pods creados con esa clase, mantendr\u00e1n la prioridad, pero ese nombre de clase no podr\u00e1 utilizarse para nuevos Pods. Cuidado! En un cluster en el que no todos los usuarios son de confianza, un usuario malicioso podr\u00eda crear Pods con la m\u00e1xima prioridad, desalojando al resto de Pods, o evitando que puedan ser desplegados. Para evitar esto, los administradores pueden crear ResourceQuota que impidan a los usuarios crear Pods con la m\u00e1xima prioridad. Ejemplo apiVersion : scheduling.k8s.io/v1 kind : PriorityClass metadata : name : high-priority value : 1000000 globalDefault : false description : \"This priority class should be used for XYZ service pods only.\" PriorityClass sin \"apropiaci\u00f3n\" En nuevas versiones de Kubernetes, se puede a\u00f1adir la stanza preemtionPolicy: Never para desactivar el desalojo de Pods y de esta manera los Pods de esta clase, en vez de desalojar los Pods menos prioritarios, se a\u00f1aden a la cola del Scheduler , por delante de otros Pods menos prioritarios. Por defecto, las PriorityClass tienen la propiedad preemtionPolicy con valor PreemptLowerPolicy . Advertencia Pese a que Pods prioritarios con la \"apropiaci\u00f3n\" desactivada, tomen una posici\u00f3n m\u00e1s adelantada en la cola del Scheduler , esto no significa que no se les aplique un \"back-off\". Esto quiere decir que si el Scheduler no es capaz de desplegar el Pod, con cada reintento se a\u00f1adir\u00e1 un tiempo de \"back-off\". Esto puede permitir que un Pod menos prioritario pueda llegar a ser desplegado antes que un Pod m\u00e1s prioritario. Scheduling Sin \"apropiaci\u00f3n\" El Scheduler se encarga de ordenar por prioridad los Pods que queden pendientes de despliegue. De esta forma, un Pod de mayor prioridad puede que sea desplegado antes que uno de menor prioridad. Si los Pods de mayor prioridad no se pueden desplegar, el Scheduler recorrer\u00e1 la cola, intent\u00e1ndolo con los de menor prioridad. Con \"apropiaci\u00f3n\" Cuando se crea un nuevo Pod de una clase con \"apropiaci\u00f3n\" se a\u00f1ade a la cola del Scheduler . Una vez el Scheduler intente desplegar ese Pod, buscar\u00e1 un nodo que cumpla los requisitos del Pod. Si no encuentra ning\u00fan nodo, el Scheduler buscar\u00e1 un nodo en el que desalojando uno o varios Pods de menor prioridad, se liberen los recursos que necesite el Pod. Si encuentra un nodo en el que pueda desalojar Pods de menor prioridad, empezar\u00e1 el proceso de desalojo y a\u00f1adir\u00e1 en la stanza status del Pod, una propiedad nominatedNodeName con el nombre del nodo. Si durante el proceso de desalojo el Scheduler detecta que un nodo ha liberado los recursos necesarios para el Pod, desplegar\u00e1 el Pod en ese otro nodo. Esto puede dar lugar a que la propiedad nominatedNodeName del Pod no coincida con el nombre del nodo en el que est\u00e1 desplegado. Tambi\u00e9n se puede dar el caso en el que una vez se desalojen los Pods de un nodo, el Scheduler asigne ese nodo a un Pod con m\u00e1s prioridad. En ese caso, se borra la propiedad nominatedNodeName del Pod y se inicia de nuevo el proceso de desalojo en otro nodo. Notas Para reducir el tiempo entre que los Pods de menor prioridad son desalojados y el de mayor prioridad es desplegado, se puede eliminar o reducir el tiempo de gracia de finalizaci\u00f3n de los Pods (por defecto son 30 segundos). Este es el tiempo de gracia que se da a los Pods para finalizar sus procesos (si no se finalizan forzosamente). Si hay PodDisruptionBudget definidos, el Scheduler intentar\u00e1 respetarlos, pero si no encuentra otros Pods que desalojar, terminar\u00e1 por desobedecer las limitaciones de los PodDisruptionBudget . Solo los nodos en los que pueda desplegar un Pod de mayor prioridad con \"apropiaci\u00f3n\", ser\u00e1n procesados por el Scheduler para realizar un desalojo. Si un Pod tiene definidas reglas de afinidad con otros Pods de menor prioridad, el Scheduler no realizar\u00e1 un desalojo, si no puede respetar la afinidad. Es por ello que se recomienda que las afinidades entre Pods se realicen entre Pods con la misma o mayor prioridad. Advertencia El desalojo de Pods no se puede realizar entre nodos. Esto quiere decir, que si un nodo se marca para el desalojo de Pods para dejar sitio a un Pod A, y el Pod tiene una topolog\u00eda que impide coincidir en la misma zona con un Pod de tipo B, si existe un Pod de tipo B con menor prioridad desplegado en otro nodo de la misma zona, el Pod B no podr\u00e1 ser desalojado para que pueda desplegarse el pod A, dado que est\u00e1 en un nodo diferente del que ha elegido el Scheduler para el desalojo. En definitiva, el Pod A se quedar\u00e1 pendiente hasta que no haya un nodo que tenga los recursos necesarios o que desalojando Pods de menor prioridad, se puedan cumplir los requisitos del Pod. Desalojo por falta de recursos Cuando el kubelet detecta que un nodo se est\u00e1 quedando sin recursos, inicia un proceso de desalojo de Pods. Para ello ordena los Pods en funci\u00f3n de su QoS (Pods que consuman m\u00e1s recursos que los request que tenga definidos), luego por su prioridad y finalmente por el uso del recurso demandado. Los Pods que no excedan sus request no ser\u00e1n desalojados. Por lo que puede ser que un Pod de mayor prioridad se desaloje, si este excede sus request .","title":"Prioridad de los Pods y Apropiaci\u00f3n"},{"location":"configuration/priority/#prioridad-de-los-pods-y-apropiacion","text":"A los Pods se les puede asignar una prioridad. Esto permite a los Pods m\u00e1s prioritarios, \"apropiarse\" de los recursos que est\u00e9n utilizando otros Pods. El Scheduler desaloja los Pods menos prioritarios de un nodo, liber\u00e1ndo recursos para los Pods m\u00e1s prioritarios y que no tengan espacio disponible para ser desplegados. Para configurar la prioridad de un Pod, primero hay que crear un objeto PriorityClass y a continuaci\u00f3n a\u00f1adir la stanza spec.priorityClassName en el Pod, con el nombre de la PriorityClass . Por defecto, Kubernetes trae ya dos PriorityClass creadas: system-cluster-crucial y system-node-crucial . Siendo la segunda la clase que tiene m\u00e1s prioridad de las dos. Notas Las PriorityClass son recursos que no pertenecen a un Namespace concreto. El nombre de la PriorityClass no puede empezar por system-* . La prioridad de la clase se especifica con la propiedad value . Contra m\u00e1s alto es el valor, m\u00e1s prioridad tiene la clase. Por defecto, los Pods que no lleven la stanza spec.priorityClassName , tendr\u00e1n prioridad con valor 0. Se puede configurar una PriorityClass por defecto para este tipo de Pods, especificando en la clase, la stanza globalDefault: true . Si se configura una clase por defecto, solo se utilizar\u00e1 para los Pods que se vayan a crear nuevos. Los Pods ya existentes seguir\u00e1n con prioridad 0. Las PriorityClass pueden tener una propiedad description para a\u00f1adir una descripci\u00f3n a la clase. Si se elimina una PriorityClass los Pods creados con esa clase, mantendr\u00e1n la prioridad, pero ese nombre de clase no podr\u00e1 utilizarse para nuevos Pods. Cuidado! En un cluster en el que no todos los usuarios son de confianza, un usuario malicioso podr\u00eda crear Pods con la m\u00e1xima prioridad, desalojando al resto de Pods, o evitando que puedan ser desplegados. Para evitar esto, los administradores pueden crear ResourceQuota que impidan a los usuarios crear Pods con la m\u00e1xima prioridad. Ejemplo apiVersion : scheduling.k8s.io/v1 kind : PriorityClass metadata : name : high-priority value : 1000000 globalDefault : false description : \"This priority class should be used for XYZ service pods only.\"","title":"Prioridad de los Pods y Apropiaci\u00f3n"},{"location":"configuration/priority/#priorityclass-sin-apropiacion","text":"En nuevas versiones de Kubernetes, se puede a\u00f1adir la stanza preemtionPolicy: Never para desactivar el desalojo de Pods y de esta manera los Pods de esta clase, en vez de desalojar los Pods menos prioritarios, se a\u00f1aden a la cola del Scheduler , por delante de otros Pods menos prioritarios. Por defecto, las PriorityClass tienen la propiedad preemtionPolicy con valor PreemptLowerPolicy . Advertencia Pese a que Pods prioritarios con la \"apropiaci\u00f3n\" desactivada, tomen una posici\u00f3n m\u00e1s adelantada en la cola del Scheduler , esto no significa que no se les aplique un \"back-off\". Esto quiere decir que si el Scheduler no es capaz de desplegar el Pod, con cada reintento se a\u00f1adir\u00e1 un tiempo de \"back-off\". Esto puede permitir que un Pod menos prioritario pueda llegar a ser desplegado antes que un Pod m\u00e1s prioritario.","title":"PriorityClass sin \"apropiaci\u00f3n\""},{"location":"configuration/priority/#scheduling","text":"","title":"Scheduling"},{"location":"configuration/priority/#sin-apropiacion","text":"El Scheduler se encarga de ordenar por prioridad los Pods que queden pendientes de despliegue. De esta forma, un Pod de mayor prioridad puede que sea desplegado antes que uno de menor prioridad. Si los Pods de mayor prioridad no se pueden desplegar, el Scheduler recorrer\u00e1 la cola, intent\u00e1ndolo con los de menor prioridad.","title":"Sin \"apropiaci\u00f3n\""},{"location":"configuration/priority/#con-apropiacion","text":"Cuando se crea un nuevo Pod de una clase con \"apropiaci\u00f3n\" se a\u00f1ade a la cola del Scheduler . Una vez el Scheduler intente desplegar ese Pod, buscar\u00e1 un nodo que cumpla los requisitos del Pod. Si no encuentra ning\u00fan nodo, el Scheduler buscar\u00e1 un nodo en el que desalojando uno o varios Pods de menor prioridad, se liberen los recursos que necesite el Pod. Si encuentra un nodo en el que pueda desalojar Pods de menor prioridad, empezar\u00e1 el proceso de desalojo y a\u00f1adir\u00e1 en la stanza status del Pod, una propiedad nominatedNodeName con el nombre del nodo. Si durante el proceso de desalojo el Scheduler detecta que un nodo ha liberado los recursos necesarios para el Pod, desplegar\u00e1 el Pod en ese otro nodo. Esto puede dar lugar a que la propiedad nominatedNodeName del Pod no coincida con el nombre del nodo en el que est\u00e1 desplegado. Tambi\u00e9n se puede dar el caso en el que una vez se desalojen los Pods de un nodo, el Scheduler asigne ese nodo a un Pod con m\u00e1s prioridad. En ese caso, se borra la propiedad nominatedNodeName del Pod y se inicia de nuevo el proceso de desalojo en otro nodo. Notas Para reducir el tiempo entre que los Pods de menor prioridad son desalojados y el de mayor prioridad es desplegado, se puede eliminar o reducir el tiempo de gracia de finalizaci\u00f3n de los Pods (por defecto son 30 segundos). Este es el tiempo de gracia que se da a los Pods para finalizar sus procesos (si no se finalizan forzosamente). Si hay PodDisruptionBudget definidos, el Scheduler intentar\u00e1 respetarlos, pero si no encuentra otros Pods que desalojar, terminar\u00e1 por desobedecer las limitaciones de los PodDisruptionBudget . Solo los nodos en los que pueda desplegar un Pod de mayor prioridad con \"apropiaci\u00f3n\", ser\u00e1n procesados por el Scheduler para realizar un desalojo. Si un Pod tiene definidas reglas de afinidad con otros Pods de menor prioridad, el Scheduler no realizar\u00e1 un desalojo, si no puede respetar la afinidad. Es por ello que se recomienda que las afinidades entre Pods se realicen entre Pods con la misma o mayor prioridad. Advertencia El desalojo de Pods no se puede realizar entre nodos. Esto quiere decir, que si un nodo se marca para el desalojo de Pods para dejar sitio a un Pod A, y el Pod tiene una topolog\u00eda que impide coincidir en la misma zona con un Pod de tipo B, si existe un Pod de tipo B con menor prioridad desplegado en otro nodo de la misma zona, el Pod B no podr\u00e1 ser desalojado para que pueda desplegarse el pod A, dado que est\u00e1 en un nodo diferente del que ha elegido el Scheduler para el desalojo. En definitiva, el Pod A se quedar\u00e1 pendiente hasta que no haya un nodo que tenga los recursos necesarios o que desalojando Pods de menor prioridad, se puedan cumplir los requisitos del Pod.","title":"Con \"apropiaci\u00f3n\""},{"location":"configuration/priority/#desalojo-por-falta-de-recursos","text":"Cuando el kubelet detecta que un nodo se est\u00e1 quedando sin recursos, inicia un proceso de desalojo de Pods. Para ello ordena los Pods en funci\u00f3n de su QoS (Pods que consuman m\u00e1s recursos que los request que tenga definidos), luego por su prioridad y finalmente por el uso del recurso demandado. Los Pods que no excedan sus request no ser\u00e1n desalojados. Por lo que puede ser que un Pod de mayor prioridad se desaloje, si este excede sus request .","title":"Desalojo por falta de recursos"},{"location":"configuration/resources/","text":"Gestionar recursos de los Containers Cuando se define un Pod, existe la posibilidad de reservar recursos (CPU, RAM...) del nodo para cada uno de los contenedores del Pod, o limitar el uso m\u00e1ximo que pueda consumir cada contenedor. Esto se consigue especificando un resource request en la definici\u00f3n del contenedor para reservar los recursos que vaya a necesitar, y un resource limit para limitar el uso m\u00e1ximo de recursos que pueda utilizar. Es decir, el kubelet reservar\u00e1 para el contenedor, tantos recursos del nodo como se haya especificado en el resource request , pudiendo utilizar m\u00e1s si el nodo tiene recursos disponibles, pero sin llegar a sobrepasar el resource limit (si se ha definido uno). Tipos de recursos Se pueden reservar/limitar los siguientes recursos: CPU : Cantidad de CPU del nodo que se puede consumir. Se mide en unidades cpu , que equivale a las unidades vCPU de los proveedores Cloud. Siempre se especifica en valores absolutos, que pueden ser fracciones. Por ejemplo 0.1 (o lo que es lo mismo 100m ). Las stanzas a definir en el Pod son: spec.containers[].resources.limits.cpu spec.containers[].resources.requests.cpu Memoria RAM : Cantidad de memoria RAM del nodo que se puede consumir, medida en bytes. Se puede especificar en unidades base 10 (p.ej. 1K=1000 bytes) o en unidades base 2 (p.ej. 1Ki=1024 bytes). Las stanzas a definir en el Pod son: spec.containers[].resources.limits.memory spec.containers[].resources.requests.memory Nota Si se especifica un limit pero no un request , por defecto se le asigna al contenedor, un request igual a su limit . Ejemplo Pese a que los recursos se especifican por contenedor, se podr\u00eda decir que el Pod de este ejemplo tiene una request de 0.5 cpu y 128MiB de memoria, y un limit de 1 cpu y 256MiB de memoria: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : app image : images.my-company.example/app:v4 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" - name : log-aggregator image : images.my-company.example/log-aggregator:v6 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" Scheduling El Scheduler toma en consideraci\u00f3n los request de los contenedores de un Pod a la hora de decidir en qu\u00e9 nodo debe alojar el Pod. Tomar\u00e1 la suma de requests de sus contenedores y s\u00f3lo permitir\u00e1 desplegar el Pod en un nodo con los suficientes recursos para alojar todos sus contenedores. Resource Limits Cuando el kubelet inicia un contenedor de un Pod, le pasa los l\u00edmites de CPU y memoria definidos, al Container Runtime. Si un contenedor excede el l\u00edmite de memoria, ser\u00e1 finalizado. A continuaci\u00f3n, si est\u00e1 configurado para ello, el kubelet reiniciar\u00e1 el contenedor. Si un contenedor excede su request de memoria, es probable que si el nodo se queda sin memoria, desaloje el Pod de ese nodo. Por otro lado, un contenedor nunca se eliminar\u00e1 por un uso excesivo de CPU, en ese caso se limitar\u00e1 el uso que puede hacer de la CPU (CPU throttling). Almacenamiento ef\u00edmero local Existen diferentes tipos de almacenamiento ef\u00edmero que Kubernetes almacena de forma local a los nodos del cluster (Volumes emptyDir que no sean de tipo tmpfs , container images, capas con permisos de escritura de los contenedores, logs). Este tipo de almacenamiento tambi\u00e9n se puede reservar/limitar para los contenedores mediante las siguientes stanzas. spec.containers[].resources.limits.ephemeral-storage spec.containers[].resources.requests.ephemeral-storage Notas Cada nodo tiene una capacidad m\u00e1xima de almacenamiento que puedo dedicar al almacenamiento ef\u00edmero de los Pods. El Scheduler se asegura que el nodo que elija para un Pod, tiene almacenamiento suficiente para cubrir la suma de todos los request de sus contenedores. Si la capa de escritura de un contenedor y el uso de logs excede el l\u00edmite de almacenamiento, el kubelet desalojar\u00e1 el Pod al que pertenece. Si el Pod utiliza m\u00e1s almacenamiento ef\u00edmero del permitido (sumando el almacenamiento ef\u00edmero de todos sus contenedores y los vol\u00famenes emptyDir ), el kubelet lo desalojar\u00e1. Si el kubelet no se encarga de medir el almacenamiento ef\u00edmero local, no habr\u00e1 desalojo de Pods. Aun as\u00ed, si el nodo detecta que queda poco espacio del sistema de ficheros reservado para capas de escritura de los contenedores, logs o vol\u00famenes emptyDir , se aplicar\u00e1 un taint sobre el nodo y se disparar\u00e1 un desalojo para todo Pod que no admita el taint . Advertencia Si una aplicaci\u00f3n abre un fichero de un vol\u00famen emptyDir y posteriormente el fichero se elimina, el kubelet considerar\u00e1 ese espacio como en desuso, pero el inode seguir\u00e1 existiendo. Ejemplo apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : app image : images.my-company.example/app:v4 resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : log-aggregator image : images.my-company.example/log-aggregator:v6 resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" Requests y Limits para initContainers Tambi\u00e9n se pueden definir requests y limits en los initContainers de los Pods. Dado que este tipo de contenedores no coexisten con los contenedores de aplicaciones, y que se ejecutan en orden, se aplican las siguientes reglas: El mayor request/limit de cada recurso de entre los initContainers ser\u00e1n los init request/limit \"efectivos\". Los request/limit \"efectivos\" de un Pod ser\u00e1n los mayores entre: La suma de todos los request/limit de los contenedores de aplicaciones. Los init request/limit \"efectivos\". El Scheduler utiliza los request \"efectivos\" del Pod, por lo que los initContainer pueden forzar a reservar m\u00e1s recursos de los que van a utilizarse en tiempo de ejecuci\u00f3n de las aplicaciones. Quality of Service (QoS) En funci\u00f3n de los request/limit que se definan en un Pod, se les asigna una clase de QoS u otra, pra indicar si los recursos est\u00e1n garantizados o no. Existen tres clases de Qos : Guaranteed : Cuando los limits son igual a los request . Esto significa que se garantiza que los contenedores siempre van a disponer de sus recursos. Se consideran los contenedores de mayor prioridad y solo son eliminados si superan su limit de memoria o el nodo se queda sin memoria y no hay contenedores de menor prioridad para eliminar. Burstable : Cuando los limits son superiores a los request . Esto significa que se reservan unos recursos para los contenedores, y si el nodo tiene recursos disponibles adicionales, los contenedores podr\u00e1n hacer uso de ellos, siempre que no superen los limits definidos. No se garantiza que el contenedor pueda hacer uso de esos recursos adicionales. Si se consume m\u00e1s memoria que la definida en la request y se requieren recursos para otros contenedores, es posible que se destruyan los contenedores para liberar memoria. Si el nodo se quedara sin memoria, se eliminar\u00edan estos contenedores si no quedan otros contenedores con QoS BestEffort por eliminar. Best-Effort . Cuando no hay limits ni requests definidos. Pueden consumir todos los recursos disponibles del nodo. Por otro lado, son los primeros en ser eliminados cuando el nodo se queda sin memoria. Se consideran contenedores con poca prioridad.","title":"Gestionar recursos de los Containers"},{"location":"configuration/resources/#gestionar-recursos-de-los-containers","text":"Cuando se define un Pod, existe la posibilidad de reservar recursos (CPU, RAM...) del nodo para cada uno de los contenedores del Pod, o limitar el uso m\u00e1ximo que pueda consumir cada contenedor. Esto se consigue especificando un resource request en la definici\u00f3n del contenedor para reservar los recursos que vaya a necesitar, y un resource limit para limitar el uso m\u00e1ximo de recursos que pueda utilizar. Es decir, el kubelet reservar\u00e1 para el contenedor, tantos recursos del nodo como se haya especificado en el resource request , pudiendo utilizar m\u00e1s si el nodo tiene recursos disponibles, pero sin llegar a sobrepasar el resource limit (si se ha definido uno).","title":"Gestionar recursos de los Containers"},{"location":"configuration/resources/#tipos-de-recursos","text":"Se pueden reservar/limitar los siguientes recursos: CPU : Cantidad de CPU del nodo que se puede consumir. Se mide en unidades cpu , que equivale a las unidades vCPU de los proveedores Cloud. Siempre se especifica en valores absolutos, que pueden ser fracciones. Por ejemplo 0.1 (o lo que es lo mismo 100m ). Las stanzas a definir en el Pod son: spec.containers[].resources.limits.cpu spec.containers[].resources.requests.cpu Memoria RAM : Cantidad de memoria RAM del nodo que se puede consumir, medida en bytes. Se puede especificar en unidades base 10 (p.ej. 1K=1000 bytes) o en unidades base 2 (p.ej. 1Ki=1024 bytes). Las stanzas a definir en el Pod son: spec.containers[].resources.limits.memory spec.containers[].resources.requests.memory Nota Si se especifica un limit pero no un request , por defecto se le asigna al contenedor, un request igual a su limit . Ejemplo Pese a que los recursos se especifican por contenedor, se podr\u00eda decir que el Pod de este ejemplo tiene una request de 0.5 cpu y 128MiB de memoria, y un limit de 1 cpu y 256MiB de memoria: apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : app image : images.my-company.example/app:v4 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" - name : log-aggregator image : images.my-company.example/log-aggregator:v6 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\"","title":"Tipos de recursos"},{"location":"configuration/resources/#scheduling","text":"El Scheduler toma en consideraci\u00f3n los request de los contenedores de un Pod a la hora de decidir en qu\u00e9 nodo debe alojar el Pod. Tomar\u00e1 la suma de requests de sus contenedores y s\u00f3lo permitir\u00e1 desplegar el Pod en un nodo con los suficientes recursos para alojar todos sus contenedores.","title":"Scheduling"},{"location":"configuration/resources/#resource-limits","text":"Cuando el kubelet inicia un contenedor de un Pod, le pasa los l\u00edmites de CPU y memoria definidos, al Container Runtime. Si un contenedor excede el l\u00edmite de memoria, ser\u00e1 finalizado. A continuaci\u00f3n, si est\u00e1 configurado para ello, el kubelet reiniciar\u00e1 el contenedor. Si un contenedor excede su request de memoria, es probable que si el nodo se queda sin memoria, desaloje el Pod de ese nodo. Por otro lado, un contenedor nunca se eliminar\u00e1 por un uso excesivo de CPU, en ese caso se limitar\u00e1 el uso que puede hacer de la CPU (CPU throttling).","title":"Resource Limits"},{"location":"configuration/resources/#almacenamiento-efimero-local","text":"Existen diferentes tipos de almacenamiento ef\u00edmero que Kubernetes almacena de forma local a los nodos del cluster (Volumes emptyDir que no sean de tipo tmpfs , container images, capas con permisos de escritura de los contenedores, logs). Este tipo de almacenamiento tambi\u00e9n se puede reservar/limitar para los contenedores mediante las siguientes stanzas. spec.containers[].resources.limits.ephemeral-storage spec.containers[].resources.requests.ephemeral-storage Notas Cada nodo tiene una capacidad m\u00e1xima de almacenamiento que puedo dedicar al almacenamiento ef\u00edmero de los Pods. El Scheduler se asegura que el nodo que elija para un Pod, tiene almacenamiento suficiente para cubrir la suma de todos los request de sus contenedores. Si la capa de escritura de un contenedor y el uso de logs excede el l\u00edmite de almacenamiento, el kubelet desalojar\u00e1 el Pod al que pertenece. Si el Pod utiliza m\u00e1s almacenamiento ef\u00edmero del permitido (sumando el almacenamiento ef\u00edmero de todos sus contenedores y los vol\u00famenes emptyDir ), el kubelet lo desalojar\u00e1. Si el kubelet no se encarga de medir el almacenamiento ef\u00edmero local, no habr\u00e1 desalojo de Pods. Aun as\u00ed, si el nodo detecta que queda poco espacio del sistema de ficheros reservado para capas de escritura de los contenedores, logs o vol\u00famenes emptyDir , se aplicar\u00e1 un taint sobre el nodo y se disparar\u00e1 un desalojo para todo Pod que no admita el taint . Advertencia Si una aplicaci\u00f3n abre un fichero de un vol\u00famen emptyDir y posteriormente el fichero se elimina, el kubelet considerar\u00e1 ese espacio como en desuso, pero el inode seguir\u00e1 existiendo. Ejemplo apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : app image : images.my-company.example/app:v4 resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\" - name : log-aggregator image : images.my-company.example/log-aggregator:v6 resources : requests : ephemeral-storage : \"2Gi\" limits : ephemeral-storage : \"4Gi\"","title":"Almacenamiento ef\u00edmero local"},{"location":"configuration/resources/#requests-y-limits-para-initcontainers","text":"Tambi\u00e9n se pueden definir requests y limits en los initContainers de los Pods. Dado que este tipo de contenedores no coexisten con los contenedores de aplicaciones, y que se ejecutan en orden, se aplican las siguientes reglas: El mayor request/limit de cada recurso de entre los initContainers ser\u00e1n los init request/limit \"efectivos\". Los request/limit \"efectivos\" de un Pod ser\u00e1n los mayores entre: La suma de todos los request/limit de los contenedores de aplicaciones. Los init request/limit \"efectivos\". El Scheduler utiliza los request \"efectivos\" del Pod, por lo que los initContainer pueden forzar a reservar m\u00e1s recursos de los que van a utilizarse en tiempo de ejecuci\u00f3n de las aplicaciones.","title":"Requests y Limits para initContainers"},{"location":"configuration/resources/#quality-of-service-qos","text":"En funci\u00f3n de los request/limit que se definan en un Pod, se les asigna una clase de QoS u otra, pra indicar si los recursos est\u00e1n garantizados o no. Existen tres clases de Qos : Guaranteed : Cuando los limits son igual a los request . Esto significa que se garantiza que los contenedores siempre van a disponer de sus recursos. Se consideran los contenedores de mayor prioridad y solo son eliminados si superan su limit de memoria o el nodo se queda sin memoria y no hay contenedores de menor prioridad para eliminar. Burstable : Cuando los limits son superiores a los request . Esto significa que se reservan unos recursos para los contenedores, y si el nodo tiene recursos disponibles adicionales, los contenedores podr\u00e1n hacer uso de ellos, siempre que no superen los limits definidos. No se garantiza que el contenedor pueda hacer uso de esos recursos adicionales. Si se consume m\u00e1s memoria que la definida en la request y se requieren recursos para otros contenedores, es posible que se destruyan los contenedores para liberar memoria. Si el nodo se quedara sin memoria, se eliminar\u00edan estos contenedores si no quedan otros contenedores con QoS BestEffort por eliminar. Best-Effort . Cuando no hay limits ni requests definidos. Pueden consumir todos los recursos disponibles del nodo. Por otro lado, son los primeros en ser eliminados cuando el nodo se queda sin memoria. Se consideran contenedores con poca prioridad.","title":"Quality of Service (QoS)"},{"location":"configuration/secret/","text":"Secret Este recurso de Kubernetes permite almacenar datos sensibles, codific\u00e1ndolos en base64 para que no queden expuestos. Los Secret permiten que la informaci\u00f3n sensible no viaje junto con la definici\u00f3n del Pod o en la im\u00e1gen del contenedor. Informaci\u00f3n Los Pods pueden consumir Secrets de 3 formas: Como variables de entorno: Utilizado solo algunas de las keys del Secret apiVersion : v1 kind : Pod metadata : name : secret-env-pod spec : containers : - name : mycontainer image : redis env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : mysecret key : username - name : SECRET_PASSWORD valueFrom : secretKeyRef : name : mysecret key : password restartPolicy : Never Utilizado todas las keys del Secret apiVersion : v1 kind : Pod metadata : name : secret-env-pod spec : containers : - name : mycontainer image : redis envFrom : - secretKeyRef : name : mysecret restartPolicy : Never Para hacer el pull de im\u00e1genes del Pod: apiVersion : v1 kind : Pod metadata : name : private-reg spec : containers : - name : private-reg-container image : my-private-registry/custom-nginx imagePullSecrets : - name : mysecret Como ficheros montados en un Volume: Utilizado solo algunas de las keys del Secret apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : mypod image : redis volumeMounts : - name : foo mountPath : \"/etc/foo\" readOnly : true volumes : - name : foo secret : secretName : mysecret items : - key : username path : my-group/my-username mode : 0777 Utilizado todas las keys del Secret apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : mypod image : redis volumeMounts : - name : foo mountPath : \"/etc/foo\" readOnly : true volumes : - name : foo secret : secretName : mysecret defaultMode : 0644 Notas Por defecto, en cada Namespace se crea un Secret con un token de acceso a la API de Kubernetes, que autom\u00e1ticamente se monta en los Pods. Los Secret en vez de utilizar la stanza .spec utilizan .data . Por defecto, los ficheros que se crean al montar un Secret como Volume en un Pod, tienen permisos de acceso 0644 . Se puede modificar con la propiedad defaultMode o si solo se quiere modificar un \u00fanico fichero con mode . Los ficheros generados en el Volume del Pod contienen los valores decodificados. Un Pod solo puede consumir los Secret que est\u00e9n en su mismo Namespace (a no ser que la aplicaci\u00f3n del Pod utilice directamente la API de Kubernetes, en cuyo caso podr\u00eda utilizar cualquier Secret ). La stanza volumeMounts de un Pod que monte un Secret como Volume, debe contener la propiedad readOnly: true . Cada vez que se actualiza un Secret , el kubelet se encarga de actualizar todos los Pods que lo monten como Volume. El tiempo que tarda en actualizar se ve afectado por el tipo de cache que tenga configurada el kubelet . Para evitar escribir en el disco de los nodos el contenido de los Secret , solo se manda a los nodos que lo requieren, y se almacena en un tmpfs , que es eliminado una vez el Pod que lo solicit\u00f3 se elimina. El tama\u00f1o m\u00e1ximo de los Secret es de 1MiB. Ejemplo apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= password : MWYyZDFlMmU2N2Rm Creaci\u00f3n de Secret con kubectl (Imperativa) Este tipo de recurso se puede crear de forma imperativa mediante comandos kubectl (a diferencia del modo declarativo kubectl apply -f template.yaml ). Se puede crear un Secret de dos formas: A partir de un literal: kubectl create secret generic dev-db-secret --from-literal = username = devuser --from-literal = password = 'S!B\\*d$zDsb=' Cuidado! Para evitar tener que escapar car\u00e1cteres especiales de las contrase\u00f1as, se deben envolver con single quote ' . A partir de un fichero: kubectl create secret generic db-user-pass --from-file = ./username.txt --from-file = ./password.txt Informaci\u00f3n Por defecto para crear las keys se utiliza el nombre de los ficheros. Si se desea especificar otro se debe utilizar el argumento de la forma siguiente --from-file=my-key=./my-file . Decodificar datos Para poder decodificar uno de los valores del Secret se puede utilizar este comando: kubectl get secret my-secret -o jsonpath = '{.data.my-key}' | base64 --decode Riesgos Al igual que cualquier recurso de la API de Kubernetes, el contenido de los Secret es almacenado en el etcd . Por lo que los administradores deber\u00edan activar la funcionalidad \"encryption at rest\". Adicionalmente, si etcd opera en modo cluster, se deber\u00eda utilizar SSL/TLS para la comunicaci\u00f3n entre nodos. La codificaci\u00f3n en base64 no se considera encriptaci\u00f3n, por lo que si un Secret se crea a partir de un manifest ( secret.yaml ), compartir o almacenar en un repositorio ese fichero compromete su contenido. Las aplicaciones deben tratar los datos obtenidos del Secret como datos sensibles y evitar escribir esos valores en los logs. Hay que tener cuidado con los usuarios que puedan crear Pods que lean Secrets , pues aunque el usuario no tenga permisos para leer un Secret , si puede hacerlo a traves del Pod. Cualquier usuario con acceso root en un nodo, puede obtener un Secret , imperson\u00e1ndose como kubelet . imagePullSecrets por defecto Se pueden a\u00f1adir autom\u00e1ticamente imagePullSecrets a un Pod, a trav\u00e9s de su ServiceAccount . Si en un ServiceAccount se a\u00f1ade la stanza imagePullSecrets con un listado de Secrets con las credenciales para acceder a repositorios de im\u00e1genes privados, todos los Pods que tengan asociada esa ServiceAccount heredar\u00e1n la stanza imagePullSecrets .","title":"Secret"},{"location":"configuration/secret/#secret","text":"Este recurso de Kubernetes permite almacenar datos sensibles, codific\u00e1ndolos en base64 para que no queden expuestos. Los Secret permiten que la informaci\u00f3n sensible no viaje junto con la definici\u00f3n del Pod o en la im\u00e1gen del contenedor. Informaci\u00f3n Los Pods pueden consumir Secrets de 3 formas: Como variables de entorno: Utilizado solo algunas de las keys del Secret apiVersion : v1 kind : Pod metadata : name : secret-env-pod spec : containers : - name : mycontainer image : redis env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : mysecret key : username - name : SECRET_PASSWORD valueFrom : secretKeyRef : name : mysecret key : password restartPolicy : Never Utilizado todas las keys del Secret apiVersion : v1 kind : Pod metadata : name : secret-env-pod spec : containers : - name : mycontainer image : redis envFrom : - secretKeyRef : name : mysecret restartPolicy : Never Para hacer el pull de im\u00e1genes del Pod: apiVersion : v1 kind : Pod metadata : name : private-reg spec : containers : - name : private-reg-container image : my-private-registry/custom-nginx imagePullSecrets : - name : mysecret Como ficheros montados en un Volume: Utilizado solo algunas de las keys del Secret apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : mypod image : redis volumeMounts : - name : foo mountPath : \"/etc/foo\" readOnly : true volumes : - name : foo secret : secretName : mysecret items : - key : username path : my-group/my-username mode : 0777 Utilizado todas las keys del Secret apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : mypod image : redis volumeMounts : - name : foo mountPath : \"/etc/foo\" readOnly : true volumes : - name : foo secret : secretName : mysecret defaultMode : 0644 Notas Por defecto, en cada Namespace se crea un Secret con un token de acceso a la API de Kubernetes, que autom\u00e1ticamente se monta en los Pods. Los Secret en vez de utilizar la stanza .spec utilizan .data . Por defecto, los ficheros que se crean al montar un Secret como Volume en un Pod, tienen permisos de acceso 0644 . Se puede modificar con la propiedad defaultMode o si solo se quiere modificar un \u00fanico fichero con mode . Los ficheros generados en el Volume del Pod contienen los valores decodificados. Un Pod solo puede consumir los Secret que est\u00e9n en su mismo Namespace (a no ser que la aplicaci\u00f3n del Pod utilice directamente la API de Kubernetes, en cuyo caso podr\u00eda utilizar cualquier Secret ). La stanza volumeMounts de un Pod que monte un Secret como Volume, debe contener la propiedad readOnly: true . Cada vez que se actualiza un Secret , el kubelet se encarga de actualizar todos los Pods que lo monten como Volume. El tiempo que tarda en actualizar se ve afectado por el tipo de cache que tenga configurada el kubelet . Para evitar escribir en el disco de los nodos el contenido de los Secret , solo se manda a los nodos que lo requieren, y se almacena en un tmpfs , que es eliminado una vez el Pod que lo solicit\u00f3 se elimina. El tama\u00f1o m\u00e1ximo de los Secret es de 1MiB. Ejemplo apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= password : MWYyZDFlMmU2N2Rm","title":"Secret"},{"location":"configuration/secret/#creacion-de-secret-con-kubectl-imperativa","text":"Este tipo de recurso se puede crear de forma imperativa mediante comandos kubectl (a diferencia del modo declarativo kubectl apply -f template.yaml ). Se puede crear un Secret de dos formas: A partir de un literal: kubectl create secret generic dev-db-secret --from-literal = username = devuser --from-literal = password = 'S!B\\*d$zDsb=' Cuidado! Para evitar tener que escapar car\u00e1cteres especiales de las contrase\u00f1as, se deben envolver con single quote ' . A partir de un fichero: kubectl create secret generic db-user-pass --from-file = ./username.txt --from-file = ./password.txt Informaci\u00f3n Por defecto para crear las keys se utiliza el nombre de los ficheros. Si se desea especificar otro se debe utilizar el argumento de la forma siguiente --from-file=my-key=./my-file .","title":"Creaci\u00f3n de Secret con kubectl (Imperativa)"},{"location":"configuration/secret/#decodificar-datos","text":"Para poder decodificar uno de los valores del Secret se puede utilizar este comando: kubectl get secret my-secret -o jsonpath = '{.data.my-key}' | base64 --decode","title":"Decodificar datos"},{"location":"configuration/secret/#riesgos","text":"Al igual que cualquier recurso de la API de Kubernetes, el contenido de los Secret es almacenado en el etcd . Por lo que los administradores deber\u00edan activar la funcionalidad \"encryption at rest\". Adicionalmente, si etcd opera en modo cluster, se deber\u00eda utilizar SSL/TLS para la comunicaci\u00f3n entre nodos. La codificaci\u00f3n en base64 no se considera encriptaci\u00f3n, por lo que si un Secret se crea a partir de un manifest ( secret.yaml ), compartir o almacenar en un repositorio ese fichero compromete su contenido. Las aplicaciones deben tratar los datos obtenidos del Secret como datos sensibles y evitar escribir esos valores en los logs. Hay que tener cuidado con los usuarios que puedan crear Pods que lean Secrets , pues aunque el usuario no tenga permisos para leer un Secret , si puede hacerlo a traves del Pod. Cualquier usuario con acceso root en un nodo, puede obtener un Secret , imperson\u00e1ndose como kubelet .","title":"Riesgos"},{"location":"configuration/secret/#imagepullsecrets-por-defecto","text":"Se pueden a\u00f1adir autom\u00e1ticamente imagePullSecrets a un Pod, a trav\u00e9s de su ServiceAccount . Si en un ServiceAccount se a\u00f1ade la stanza imagePullSecrets con un listado de Secrets con las credenciales para acceder a repositorios de im\u00e1genes privados, todos los Pods que tengan asociada esa ServiceAccount heredar\u00e1n la stanza imagePullSecrets .","title":"imagePullSecrets por defecto"},{"location":"kubectl/edit-patch/","text":"Editar/Actualizar recursos kubectl patch Con este comando se pueden aplicar cambios sobre un recurso Kubernetes. Cada vez que se aplique un cambio, Kubernetes eliminar\u00e1 el recurso anterior y crear\u00e1 uno nuevo con los cambios aplicados. Para indicar los cambios a aplicar, se puede crear un fichero YAML/JSON con los cambios, o pas\u00e1rselo directamente en el comando kubectl . Ejemplo Fichero de cambios: spec : template : spec : containers : - name : patch-demo-ctr-2 image : redis Comando kubectl : kubectl patch deployment patch-demo --patch \" $( cat patch-file.yaml ) \" Advertencia En funci\u00f3n del valor de patchStrategy que haya definido en la API de Kubernetes para el campo que se intenta modificar, el ejemplo anterior a\u00f1adir\u00e1 un nuevo contenedor (si es merge ), o sustituir\u00e1 los contenedores del Pod por uno nuevo (si patchStrategy no est\u00e1 definido). Informaci\u00f3n Al comando kubectl patch se le puede pasar un par\u00e1metro --type para indicar que metodologia de patching utilizar (por defecto ser\u00e1 strategic ): kubectl patch deployment patch-demo --type merge --patch \" $( cat patch-file.yaml ) \" El par\u00e1metro --type puede tomar los valores de la tabla siguiente: Tipo Descripci\u00f3n json Se debe especificar la operaci\u00f3n a realizar, el JSON path a modificar y el nuevo valor. Operaciones permtidas: add , remove , replace , move , copy y test . M\u00e1s informaci\u00f3n merge Las propiedades que se especifiquen en el fichero de cambios, sustituir\u00e1n las propiedades anteriores, por lo que las propiedades que no se indiquen en el fichero, ser\u00e1n eliminadas. strategic Por defecto aplicar\u00e1 una estrategia de reemplazo de propiedades. Si en la API de Kubernetes el campo a modificar, tiene definida la propiedad patchStrategy , se aplicar\u00e1 la estrategia que indique. Por ejemplo, si tiene valor merge , s\u00f3lo crear\u00e1/actualizar\u00e1 los valores que se indiquen en el fichero de cambios. Ejemplos Reemplazar la imagen de un contenedor con type json : kubectl patch pod valid-pod --type = 'json' -p = '[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]' Eliminar un contenedor con type json : kubectl patch pod valid-pod --type = 'json' -p = '[{\"op\": \"remove\", \"path\": \"/spec/containers/1\"}]' Actualizar parcialmente un nodo con type strategic : kubectl patch node k8s-node-1 -p '{\"spec\":{\"unschedulable\":true}}' Actualizar la imagen de un contenedor con type strategic : (se indica el nombre del contenedor a modificar) kubectl patch pod valid-pod -p '{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}' Sustituir el array de contenedores por uno nuevo con type merge : kubectl patch pod valid-pod --type = 'merge' -p '{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}' kubectl edit Con este comando se puede abrir un editor para modificar en caliente un recurso Kuberentes. Esta operaci\u00f3n es la misma que realizar un kubectl get -o yaml , editar el recurso y posteriormente ejecutar kubectl apply -f changes.yaml . kubectl edit deployment my-nginx Al guardar los cambios en el editor, los cambios se aplicar\u00e1n en el recurso. Al realizarse por debajo un kubectl apply , el recurso no se recrear\u00e1, por lo que habr\u00e1 ciertas propiedades que no puedan ser modificadas. El propio editor notificar\u00e1 con un error si no se han podido aplicar los cambios sobre el recurso.","title":"Editar/Actualizar recursos"},{"location":"kubectl/edit-patch/#editaractualizar-recursos","text":"","title":"Editar/Actualizar recursos"},{"location":"kubectl/edit-patch/#kubectl-patch","text":"Con este comando se pueden aplicar cambios sobre un recurso Kubernetes. Cada vez que se aplique un cambio, Kubernetes eliminar\u00e1 el recurso anterior y crear\u00e1 uno nuevo con los cambios aplicados. Para indicar los cambios a aplicar, se puede crear un fichero YAML/JSON con los cambios, o pas\u00e1rselo directamente en el comando kubectl . Ejemplo Fichero de cambios: spec : template : spec : containers : - name : patch-demo-ctr-2 image : redis Comando kubectl : kubectl patch deployment patch-demo --patch \" $( cat patch-file.yaml ) \" Advertencia En funci\u00f3n del valor de patchStrategy que haya definido en la API de Kubernetes para el campo que se intenta modificar, el ejemplo anterior a\u00f1adir\u00e1 un nuevo contenedor (si es merge ), o sustituir\u00e1 los contenedores del Pod por uno nuevo (si patchStrategy no est\u00e1 definido). Informaci\u00f3n Al comando kubectl patch se le puede pasar un par\u00e1metro --type para indicar que metodologia de patching utilizar (por defecto ser\u00e1 strategic ): kubectl patch deployment patch-demo --type merge --patch \" $( cat patch-file.yaml ) \" El par\u00e1metro --type puede tomar los valores de la tabla siguiente: Tipo Descripci\u00f3n json Se debe especificar la operaci\u00f3n a realizar, el JSON path a modificar y el nuevo valor. Operaciones permtidas: add , remove , replace , move , copy y test . M\u00e1s informaci\u00f3n merge Las propiedades que se especifiquen en el fichero de cambios, sustituir\u00e1n las propiedades anteriores, por lo que las propiedades que no se indiquen en el fichero, ser\u00e1n eliminadas. strategic Por defecto aplicar\u00e1 una estrategia de reemplazo de propiedades. Si en la API de Kubernetes el campo a modificar, tiene definida la propiedad patchStrategy , se aplicar\u00e1 la estrategia que indique. Por ejemplo, si tiene valor merge , s\u00f3lo crear\u00e1/actualizar\u00e1 los valores que se indiquen en el fichero de cambios. Ejemplos Reemplazar la imagen de un contenedor con type json : kubectl patch pod valid-pod --type = 'json' -p = '[{\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new image\"}]' Eliminar un contenedor con type json : kubectl patch pod valid-pod --type = 'json' -p = '[{\"op\": \"remove\", \"path\": \"/spec/containers/1\"}]' Actualizar parcialmente un nodo con type strategic : kubectl patch node k8s-node-1 -p '{\"spec\":{\"unschedulable\":true}}' Actualizar la imagen de un contenedor con type strategic : (se indica el nombre del contenedor a modificar) kubectl patch pod valid-pod -p '{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}' Sustituir el array de contenedores por uno nuevo con type merge : kubectl patch pod valid-pod --type = 'merge' -p '{\"spec\":{\"containers\":[{\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"}]}}'","title":"kubectl patch"},{"location":"kubectl/edit-patch/#kubectl-edit","text":"Con este comando se puede abrir un editor para modificar en caliente un recurso Kuberentes. Esta operaci\u00f3n es la misma que realizar un kubectl get -o yaml , editar el recurso y posteriormente ejecutar kubectl apply -f changes.yaml . kubectl edit deployment my-nginx Al guardar los cambios en el editor, los cambios se aplicar\u00e1n en el recurso. Al realizarse por debajo un kubectl apply , el recurso no se recrear\u00e1, por lo que habr\u00e1 ciertas propiedades que no puedan ser modificadas. El propio editor notificar\u00e1 con un error si no se han podido aplicar los cambios sobre el recurso.","title":"kubectl edit"},{"location":"kubectl/imperative/","text":"Creaci\u00f3n imperativa de recursos Objetos Pod kubectl run mypod --image = nginx --env = \"var1=value1\" --env = \"var2=value2\" \\ --labels = \"label1=value1,label2=value2\" --restart = Never --port = 8080 \\ --serviceaccount = my-account \\ --requests = \"cpu=100m,memory=256Mi\" --limits = \"cpu=200m,memory=500Mi\" ## Pasarle argumentos al comando de la imagen kubectl run nginx --image = nginx -- <arg1> <arg2> ... <argN> ## Sobrescribir el comando de la imagen kubectl run nginx --image = nginx --command -- <cmd> <arg1> ... <argN> ## Crear Pod y Service kubectl run nginx --image = nginx --port 8080 --expose ## Sobrescribir una o varias propiedades en formato JSON kubectl run nginx --image = nginx --overrides = '{ \"apiVersion\": \"v1\", \"spec\": { ... } }' Namespace kubectl create namespace my-namespace Controladores Deployment kubectl create deployment myapp --image = nginx --port = 8080 --replicas = 3 ## Ejecutar con argumentos kubectl create deployment myapp --image = nginx --port = 8080 --replicas = 3 -- <arg1> <arg2> ... <argN> Statefulset Cuidado! Este recurso solo se puede crear a partir de un descriptor YAML: kubectl apply -f resource.yaml DaemonTool Cuidado! Este recurso solo se puede crear a partir de un descriptor YAML: kubectl apply -f resource.yaml Job kubectl create job test-job --image = nginx ## Ejecutar con argumentos kubectl create job test-job --image = nginx -- <arg1> <arg2> ... <argN> ## Crear desde un CronJob kubectl create job test-job --from = cronjob/my-cronjob CronJob kubectl create cronjob my-job --image = busybox --schedule = \"*/1 * * * *\" --restart = Never ## Ejecutar con argumentos kubectl create cronjob my-job --image = busybox --schedule = \"*/1 * * * *\" -- <arg1> <arg2> ... <argN> Networking Service ## Service creado a partir de cualquier Replication Controller o Pod kubectl expose deployment my-deploy --port = <service-port> --protocol = TCP \\ --target-port = <pod-port> --name = my-service \\ --type =[ ClusterIP | NodePort | LoadBalancer | ExternalName ] ## ClusterIP Service kubectl create service clusterip my-svc --tcp = <service-port>:<pod-port> ## Headless Service kubectl create service clusterip my-svc --clusterip = \"None\" ## NodePort Service kubectl create service nodeport my-svc --tcp = <service-port>:<pod-port> ## LoadBalancer Service kubectl create service loadbalancer my-svc --tcp = <service-port>:<pod-port> ## ExternalName Service kubectl create service externalname my-svc --external-name bar.com --tcp = <service-port>:<pod-port> Almacenamiento Persistent Volume Claim (PVC) Cuidado! Este recurso solo se puede crear a partir de un descriptor YAML: kubectl apply -f resource.yaml Configuraci\u00f3n ConfigMap ## Crear a partir de un fichero, sobrescribiendo el nombre del segundo fichero kubectl create configmap my-config --from-file = /path/file1.txt --from-file = key2 = /path/file2.txt ## Crear a partir de literales kubectl create configmap my-config --from-literal = key1 = config1 --from-literal = key2 = config2 ## Crear a partir de fichero .env kubectl create configmap my-config --from-env-file = path/file.env Secret ## Crear a partir de un fichero, sobrescribiendo el nombre del segundo fichero kubectl create secret generic my-secret --from-file = /path/file1.txt --from-file = key2 = /path/file2.txt ## Crear a partir de literales kubectl create secret generic my-secret --from-literal = key1 = secret1 --from-literal = key2 = secret2 ## Crear a partir de fichero .env kubectl create secret generic my-secret --from-env-file = path/file.env ## Crear Docker Registry Secret kubectl create secret docker-registry my-secret --docker-server = DOCKER_REGISTRY_SERVER \\ --docker-username = DOCKER_USER --docker-password = DOCKER_PASSWORD --docker-email = DOCKER_EMAIL ## Crear TLS Secret kubectl create secret tls my-secret --cert = path/tls.cert --key = path/tls.key Quota kubectl create quota my-quota --hard = \"cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10\" ## Aplicar restricci\u00f3n sobre un tipo de Pods kubectl create quota my-quota --hard = pods = 100 --scopes = BestEffort PriorityClass kubectl create priorityclass high-priority --value = 1000 --preemption-policy = \"Never\" --global-default = false --description = \"high priority\" Pod Disruption Budget (PDB) kubectl create poddisruptionbudget my-pdb --selector = app = rails --min-available = 1 ## Con valor porcentual kubectl create pdb my-pdb --selector = app = nginx --min-available = 50 % Seguridad ServiceAccount kubectl create serviceaccount my-service-account Role/ClusterRole ## Role sobre recursos concretos kubectl create role pod-reader --verb = get,list,watch --resource = pods --resource-name = readablepod --resource-name = anotherpod ## ClusterRole sobre recursos concretos kubectl create clusterrole pod-reader --verb = get --resource = pods --resource-name = readablepod --resource-name = anotherpod ## ClusterRole para API endpoints kubectl create clusterrole \"foo\" --verb = get --non-resource-url = /logs/* ## ClusterRole agregado kubectl create clusterrole monitoring --aggregation-rule = \"rbac.example.com/aggregate-to-monitoring=true\" RoleBinding/ClusterRoleBinding ## RoleBinding para dar permisos de admin en un Namespace a user1, user2, grupo1 y service1 kubectl create rolebinding admin --clusterrole = admin --user = user1 --user = user2 --group = group1 --serviceaccount = my-namespace:service1 ## RoleBinding para dar permisos de lectura a user1 kubectl create rolebinding reader --role = reader-role --user = user1 ## RoleBinding para dar permisos de admin-cluster a user1, user2, grupo1 y service1 kubectl create clusterrolebinding admin --clusterrole = admin --user = user1 --user = user2 --group = group1 --serviceaccount = my-namespace:service1 ## RoleBinding para dar permisos de lectura de todo el cluster a user1 kubectl create clusterrolebinding reader --clusterrole = reader-role --user = user1","title":"Creaci\u00f3n imperativa de recursos"},{"location":"kubectl/imperative/#creacion-imperativa-de-recursos","text":"","title":"Creaci\u00f3n imperativa de recursos"},{"location":"kubectl/imperative/#objetos","text":"","title":"Objetos"},{"location":"kubectl/imperative/#pod","text":"kubectl run mypod --image = nginx --env = \"var1=value1\" --env = \"var2=value2\" \\ --labels = \"label1=value1,label2=value2\" --restart = Never --port = 8080 \\ --serviceaccount = my-account \\ --requests = \"cpu=100m,memory=256Mi\" --limits = \"cpu=200m,memory=500Mi\" ## Pasarle argumentos al comando de la imagen kubectl run nginx --image = nginx -- <arg1> <arg2> ... <argN> ## Sobrescribir el comando de la imagen kubectl run nginx --image = nginx --command -- <cmd> <arg1> ... <argN> ## Crear Pod y Service kubectl run nginx --image = nginx --port 8080 --expose ## Sobrescribir una o varias propiedades en formato JSON kubectl run nginx --image = nginx --overrides = '{ \"apiVersion\": \"v1\", \"spec\": { ... } }'","title":"Pod"},{"location":"kubectl/imperative/#namespace","text":"kubectl create namespace my-namespace","title":"Namespace"},{"location":"kubectl/imperative/#controladores","text":"","title":"Controladores"},{"location":"kubectl/imperative/#deployment","text":"kubectl create deployment myapp --image = nginx --port = 8080 --replicas = 3 ## Ejecutar con argumentos kubectl create deployment myapp --image = nginx --port = 8080 --replicas = 3 -- <arg1> <arg2> ... <argN>","title":"Deployment"},{"location":"kubectl/imperative/#statefulset","text":"Cuidado! Este recurso solo se puede crear a partir de un descriptor YAML: kubectl apply -f resource.yaml","title":"Statefulset"},{"location":"kubectl/imperative/#daemontool","text":"Cuidado! Este recurso solo se puede crear a partir de un descriptor YAML: kubectl apply -f resource.yaml","title":"DaemonTool"},{"location":"kubectl/imperative/#job","text":"kubectl create job test-job --image = nginx ## Ejecutar con argumentos kubectl create job test-job --image = nginx -- <arg1> <arg2> ... <argN> ## Crear desde un CronJob kubectl create job test-job --from = cronjob/my-cronjob","title":"Job"},{"location":"kubectl/imperative/#cronjob","text":"kubectl create cronjob my-job --image = busybox --schedule = \"*/1 * * * *\" --restart = Never ## Ejecutar con argumentos kubectl create cronjob my-job --image = busybox --schedule = \"*/1 * * * *\" -- <arg1> <arg2> ... <argN>","title":"CronJob"},{"location":"kubectl/imperative/#networking","text":"","title":"Networking"},{"location":"kubectl/imperative/#service","text":"## Service creado a partir de cualquier Replication Controller o Pod kubectl expose deployment my-deploy --port = <service-port> --protocol = TCP \\ --target-port = <pod-port> --name = my-service \\ --type =[ ClusterIP | NodePort | LoadBalancer | ExternalName ] ## ClusterIP Service kubectl create service clusterip my-svc --tcp = <service-port>:<pod-port> ## Headless Service kubectl create service clusterip my-svc --clusterip = \"None\" ## NodePort Service kubectl create service nodeport my-svc --tcp = <service-port>:<pod-port> ## LoadBalancer Service kubectl create service loadbalancer my-svc --tcp = <service-port>:<pod-port> ## ExternalName Service kubectl create service externalname my-svc --external-name bar.com --tcp = <service-port>:<pod-port>","title":"Service"},{"location":"kubectl/imperative/#almacenamiento","text":"","title":"Almacenamiento"},{"location":"kubectl/imperative/#persistent-volume-claim-pvc","text":"Cuidado! Este recurso solo se puede crear a partir de un descriptor YAML: kubectl apply -f resource.yaml","title":"Persistent Volume Claim (PVC)"},{"location":"kubectl/imperative/#configuracion","text":"","title":"Configuraci\u00f3n"},{"location":"kubectl/imperative/#configmap","text":"## Crear a partir de un fichero, sobrescribiendo el nombre del segundo fichero kubectl create configmap my-config --from-file = /path/file1.txt --from-file = key2 = /path/file2.txt ## Crear a partir de literales kubectl create configmap my-config --from-literal = key1 = config1 --from-literal = key2 = config2 ## Crear a partir de fichero .env kubectl create configmap my-config --from-env-file = path/file.env","title":"ConfigMap"},{"location":"kubectl/imperative/#secret","text":"## Crear a partir de un fichero, sobrescribiendo el nombre del segundo fichero kubectl create secret generic my-secret --from-file = /path/file1.txt --from-file = key2 = /path/file2.txt ## Crear a partir de literales kubectl create secret generic my-secret --from-literal = key1 = secret1 --from-literal = key2 = secret2 ## Crear a partir de fichero .env kubectl create secret generic my-secret --from-env-file = path/file.env ## Crear Docker Registry Secret kubectl create secret docker-registry my-secret --docker-server = DOCKER_REGISTRY_SERVER \\ --docker-username = DOCKER_USER --docker-password = DOCKER_PASSWORD --docker-email = DOCKER_EMAIL ## Crear TLS Secret kubectl create secret tls my-secret --cert = path/tls.cert --key = path/tls.key","title":"Secret"},{"location":"kubectl/imperative/#quota","text":"kubectl create quota my-quota --hard = \"cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10\" ## Aplicar restricci\u00f3n sobre un tipo de Pods kubectl create quota my-quota --hard = pods = 100 --scopes = BestEffort","title":"Quota"},{"location":"kubectl/imperative/#priorityclass","text":"kubectl create priorityclass high-priority --value = 1000 --preemption-policy = \"Never\" --global-default = false --description = \"high priority\"","title":"PriorityClass"},{"location":"kubectl/imperative/#pod-disruption-budget-pdb","text":"kubectl create poddisruptionbudget my-pdb --selector = app = rails --min-available = 1 ## Con valor porcentual kubectl create pdb my-pdb --selector = app = nginx --min-available = 50 %","title":"Pod Disruption Budget (PDB)"},{"location":"kubectl/imperative/#seguridad","text":"","title":"Seguridad"},{"location":"kubectl/imperative/#serviceaccount","text":"kubectl create serviceaccount my-service-account","title":"ServiceAccount"},{"location":"kubectl/imperative/#roleclusterrole","text":"## Role sobre recursos concretos kubectl create role pod-reader --verb = get,list,watch --resource = pods --resource-name = readablepod --resource-name = anotherpod ## ClusterRole sobre recursos concretos kubectl create clusterrole pod-reader --verb = get --resource = pods --resource-name = readablepod --resource-name = anotherpod ## ClusterRole para API endpoints kubectl create clusterrole \"foo\" --verb = get --non-resource-url = /logs/* ## ClusterRole agregado kubectl create clusterrole monitoring --aggregation-rule = \"rbac.example.com/aggregate-to-monitoring=true\"","title":"Role/ClusterRole"},{"location":"kubectl/imperative/#rolebindingclusterrolebinding","text":"## RoleBinding para dar permisos de admin en un Namespace a user1, user2, grupo1 y service1 kubectl create rolebinding admin --clusterrole = admin --user = user1 --user = user2 --group = group1 --serviceaccount = my-namespace:service1 ## RoleBinding para dar permisos de lectura a user1 kubectl create rolebinding reader --role = reader-role --user = user1 ## RoleBinding para dar permisos de admin-cluster a user1, user2, grupo1 y service1 kubectl create clusterrolebinding admin --clusterrole = admin --user = user1 --user = user2 --group = group1 --serviceaccount = my-namespace:service1 ## RoleBinding para dar permisos de lectura de todo el cluster a user1 kubectl create clusterrolebinding reader --clusterrole = reader-role --user = user1","title":"RoleBinding/ClusterRoleBinding"},{"location":"monitoring/debugging/","text":"Debugging Troubleshooting de aplicaciones Debugging Pods El primer paso para debugar una aplicaci\u00f3n, es ver el estado del Pod y los eventos acontecidos recientemente. Mediante el comando kubectl describe pod <pod-name> , se puede obtener informaci\u00f3n general del Pod: Configuraci\u00f3n de los contenedores Estado actual del Pod Eventos del Pod Nota Una forma de obtener informaci\u00f3n m\u00e1s detallada acerca de un Pod es mediante el comando kubectl get pod <pod-name> -o yaml . De esta forma se puede obtener en formato YAML toda la informaci\u00f3n que hay almacenada en Kubernetes sobre ese Pod. Informaci\u00f3n Posibles estados del Pod y motivos: Pending : significa que el Pod no puede ser emplazado en un nodo. Esto puede suceder porque no hay recursos o nodos suficientes, o de utilizar la propiedad hostPort , que no hay nodos con ese puerto disponible. Soluci\u00f3n : Eliminar algunos Pods, ajustar las requests de recursos de los Pods o a\u00f1adir nuevos nodos. Waiting : el Pod se ha desplegado en un nodo, pero no puede ser ejecutado. Normalmente la causa m\u00e1s com\u00fan de esto es que el Pod no haya podido hacer pull de la imagen Docker, correctamente. Soluci\u00f3n : Habr\u00eda que revisar que el nombre de la imagen sea la correcta, que la imagen exista en el repositorio o probar manualmente de realizar un docker pull <image> en el nodo para comprobar que la imagen pueda ser obtenida. Pod no arranca correctamente Si el Pod falla o no est\u00e1 en estado healthy , hay que analizar los logs de las aplicaciones para buscar m\u00e1s informaci\u00f3n. Para ello ejecutar: kubectl logs <pod-name> <container-name> ## Para ver los logs de un contenedor que haya fallado anteriormente kubectl logs --previous <pod-name> <container-name> Otra forma para debugar el Pod es accediendo a sus contenedores mediante una Shell, si estos disponen de debugging utilities que puedan ser utilizadas (p.ej. realizar un cat sobre un fichero concreto): kubectl exec -it cassandra -- sh Pod no tiene comportamiento esperado Si el problema es que el Pod no se ejecuta como se esperaba, es posible que haya alg\u00fan tipo de error en su descriptor de despliegue que haga que no se ejecute correctamente (p.ej. si hay alg\u00fan error tipogr\u00e1fico, esa propiedad es ignorada). Para validar si se trata de un error con el descriptor, habr\u00eda que eliminar el Pod y volver a ejecutarlo con la opci\u00f3n --validate : kubectl apply --validate -f mypod.yaml Si hubiera alg\u00fan tipo de problema, saltar\u00eda un mensaje de error informando de qu\u00e9 est\u00e1 mal. Otra comprobaci\u00f3n ser\u00eda obtener la descripci\u00f3n del Pod desplegado y compararla con la del descriptor para ver si existe alguna diferencia (sin tener en cuenta los campos que a\u00f1ade el servidor de la API. p.ej. startTime ). kubectl get pods/mypod -o yaml > mypod-on-apiserver.yaml Contenedores ef\u00edmeros (alpha) Cuando acceder via Shell a dentro de un contenedor no sea posible o suficiente (p.ej. porque la imagen que se est\u00e9 utilizando no disponga de debugging utilities ), otra alternativa son los contenedores ef\u00edmeros. Estos contenedores se levantar\u00e1n dentro del mismo Pod y compartir\u00e1n el mismo \"namespace de procesos\". De esta forma, el nuevo contenedor compartir\u00e1 el mismo arbol de procesos y sistema de ficheros que el contenedor de la aplicaci\u00f3n. El comando a ejecutar para levantar un contenedor ef\u00edmero: kubectl alpha debug -it ephemeral-demo --image = busybox --target = <app-container-name> Acceder logs de Init Container Para leer los logs de un Init Container ejecutar: kubectl logs <pod-name> -c <init-container-name> Si los contenedores ejecutan un script bash, se puede ejecutar set -x al inicio del script para que se graben en el logs los comandos que se van a ir ejecutando. Mensaje de terminaci\u00f3n Cuando un contenedor termina su ejecuci\u00f3n (ya sea porque ha terminado de ejecutar su proceso o por un error), vuelca un \"mensaje de terminaci\u00f3n\" sobre el fichero /dev/termination-log . Si el contenedor volcara el mensaje en otro fichero, se puede indicar en la definici\u00f3n del Pod con la propiedad spec.containers[].terminationMessagePath . Adicionalmente se puede a\u00f1adir la propiedad spec.containers[].terminationMessagePolicy: \"FallbackToLogsOnError\" , para que si el contenedor termina con error y el fichero del \"mensaje de terminaci\u00f3n\" est\u00e1 vac\u00edo, utilice el \u00faltimo bloque de logs del contenedor como \"mensaje de terminaci\u00f3n\". Debugging Replication Controllers Dado que los Replication Controllers solo se encargan de gestionar el n\u00famero de Pods desplegados, para debugarlos \u00fanicamente hay que debugar los Pods. Adicionalmente se pueden observar los eventos de los Replica Set para ver si hubiera alg\u00fan problema a la hora de crear nuevas instancias: kubectl describe rs <replica-set-name> Debugging Services Para empezar a debugar un Service hay que revisar que tenga el n\u00famero de Endpoints asociados correcto. Debe coincidir con el n\u00famero de Pods del Service . Para comprobarlo: kubectl get endpoints <service-name> Si el comando anterior no devolviera el mismo n\u00famero de direcciones IP que de Pods, habr\u00eda que revisar que se obtenga el listado correcto de Pods al buscar por las labels del Service : kubectl get pods --selector = label1 = value1,label2 = value2 Si el listado de Pods es el correcto, es posible que no se est\u00e9n exponiendo los puertos correctos. Habr\u00eda que comprobar que los containerPort de los Pod coinciden con los targetPort del Service. Si por el contrario, los Endpoints si tienen IP's, pero una vez se ha establecido conexi\u00f3n con el Service, la conexi\u00f3n se cae, es probable que el Pod no est\u00e9 funcionando correctamente. En ese caso habr\u00eda que revisar el contador de reinicios del Pod. Otra comprobaci\u00f3n ser\u00eda intentar acceder al Pod directamente a trav\u00e9s de su direcci\u00f3n IP. Si en ning\u00fan caso la aplicaci\u00f3n es accesible, es posible que la aplicaci\u00f3n no est\u00e9 exponiendo el mismo puerto que haya configurado en el Pod. Para comprobarlo ejecutar desde dentro el cluster: wget -gO- <pod-IP>:<port> El Service no resuelve por DNS Para comprobar que el dominio del Service resuelve correctamente, habr\u00eda que utilizar la funcionalidad nslookup desde un Pod en el mismo Namespace que la aplicaci\u00f3n. nslookup <service-name> Si fallara, podria ser que Pod y Service estubieran en Namespaces diferentes. Para ello comprobar: nslookup <service-name>.<service-namespace> Si a\u00fan as\u00ed fallara, habr\u00eda quer comprobar el FQDN del Service : nslookup <service-name>.<service-namespace>.svc.cluster.local Si resuelve correctamente, habr\u00eda que comprobar el fichero /etc/resolv.conf del Pod para ver que sea correcto: Debe contener una entrada nameserver con la IP del servicio DNS del cluster. Debe contener una entrada search con todos los sufijos del cluster: <service-name>.<service-namespace>.svc.cluster.local para encontrar los Service del mismo Namespace. svc.cluster.local para encontrar los Service de cualquier Namespace. cluster.local para encontrar dominios del cluster. Debe contener una entrada options con la propiedad ndots con un valor suficientemente alto como para que el servidor DNS considere los search paths . Si a\u00fan asi el comando fallara, s\u00f3lo quedar\u00eda comprobar el dominio de la API de Kubernetes, ya que este siempre deber\u00eda funcionar: nslookup kubernetes.default Si este comando falla, habria que comprobar que el kube-proxy funcione correctamente. El Service no funciona por IP Comprobar que el Service es accesible via IP. La direcci\u00f3n IP se puede obtener con el comando kubectl get svc <service-name> . Para comprobar que se puede acceder via IP ejecutar desde dentro del cluster: wget -qO- <service-IP>:<port> Si no funcionara, habr\u00eda que revisar que el Service est\u00e9 correctamente configurado con los puertos y protocolos correctos y que los Pods se est\u00e9n ejecutando correctamente. Comprobar que kube-proxy funciona correctamente Si los Endpoints y la configuraci\u00f3n de los Services es correcta, es probable que el problema est\u00e9 en la implementaci\u00f3n que se ejecute por debajo de los Services . En la mayoria de clusters Kubernetes ser\u00e1 kube-proxy . Primero habr\u00eda que comprobar que el proceso de kube-proxy se est\u00e9 ejecutando en el nodo. Para ello, habr\u00e1 que acceder via SSH y ejecutar el siguiente comando: ps auxw | grep kube-proxy Si el proceso se est\u00e1 ejecutando, habria que comprobar en los logs, si est\u00e1 teniendo dificultades para contactar al _master. Se puede revisar el fichero /var/log/kube-proxy.log . Si aparecen mensajes de error habr\u00eda que revisar la configuraci\u00f3n del nodo y revisar si el proceso de instalaci\u00f3n se ha realizado correctamente. Si la instalaci\u00f3n se ha realizado manualmente, deber\u00eda de haberse instalado el binario conntrack , por lo que habr\u00eda que revisar que se haya instalado correctamente. En funci\u00f3n del modo de ejecuci\u00f3n del kube-proxy ( iptables o ipvs ), habr\u00eda que revisar la tabla de redirecciones correspondiente para comprobar que las entradas son correctas: ## iptables iptables-save | grep <service-name> # -> Debe devolver una regla KUBE-SERVICES y una cadena KUBE-SVC-<hash> ## ipvs ipvsadm -ln # -> Debe devolver una entrada con la <service-IP>:<service-port> del Service y tantas entradas <pod-IP>:<pod-port> como Pods tenga el Service Cuando el kube-proxy opera en modo iptables , se puede dar el caso que un Pod no pueda accederse a si mismo a trav\u00e9s de la IP del Service. En ese caso habr\u00eda que revisar que el flag hairpin-mode de kube-proxy est\u00e9 correctamente configurado (ver Documentaci\u00f3n para m\u00e1s detalle). Debugging del cluster Se puede obtener el estado actual de los nodos con el comando kubectl get nodes . Mediante el comando kubectl describe node <node-name> , se puede obtener informaci\u00f3n general del nodo: Configuraci\u00f3n del nodo Estado actual del nodo Capacidad del nodo Recursos disponibles y en uso Eventos del nodo Nota Una forma de obtener informaci\u00f3n m\u00e1s detallada acerca de un nodo es mediante el comando kubectl get node <node-name> -o yaml . De esta forma se puede obtener en formato YAML toda la informaci\u00f3n que hay almacenada en Kubernetes sobre ese nodo. Si se desea obtener informaci\u00f3n m\u00e1s detallada del estado global del cluster se puede ejecutar el comando: kubectl cluster-info dump Advertencia Este comando devuelve TODA a informaci\u00f3n almacenada en el cluster (descriptores, logs, eventos...), por lo que hayq que realizar un proceso de b\u00fasqueda para encontrar la informaci\u00f3n que se desea. Para comprobar que todos los componentes del cluster operan correctamente hay que ir nodo por nodo observando los diferentes logs: Nodos master - API server: /var/log/kube-apiserver.log - Scheduler: /var/log/kube-scheduler.log - Controller: /var/log/kube-controller-manager.log Nodos worker - kubelet: /var/log/kubelet.log - kube-proxy: /var/log/kube-proxy.log","title":"Debugging"},{"location":"monitoring/debugging/#debugging","text":"","title":"Debugging"},{"location":"monitoring/debugging/#troubleshooting-de-aplicaciones","text":"","title":"Troubleshooting de aplicaciones"},{"location":"monitoring/debugging/#debugging-pods","text":"El primer paso para debugar una aplicaci\u00f3n, es ver el estado del Pod y los eventos acontecidos recientemente. Mediante el comando kubectl describe pod <pod-name> , se puede obtener informaci\u00f3n general del Pod: Configuraci\u00f3n de los contenedores Estado actual del Pod Eventos del Pod Nota Una forma de obtener informaci\u00f3n m\u00e1s detallada acerca de un Pod es mediante el comando kubectl get pod <pod-name> -o yaml . De esta forma se puede obtener en formato YAML toda la informaci\u00f3n que hay almacenada en Kubernetes sobre ese Pod. Informaci\u00f3n Posibles estados del Pod y motivos: Pending : significa que el Pod no puede ser emplazado en un nodo. Esto puede suceder porque no hay recursos o nodos suficientes, o de utilizar la propiedad hostPort , que no hay nodos con ese puerto disponible. Soluci\u00f3n : Eliminar algunos Pods, ajustar las requests de recursos de los Pods o a\u00f1adir nuevos nodos. Waiting : el Pod se ha desplegado en un nodo, pero no puede ser ejecutado. Normalmente la causa m\u00e1s com\u00fan de esto es que el Pod no haya podido hacer pull de la imagen Docker, correctamente. Soluci\u00f3n : Habr\u00eda que revisar que el nombre de la imagen sea la correcta, que la imagen exista en el repositorio o probar manualmente de realizar un docker pull <image> en el nodo para comprobar que la imagen pueda ser obtenida. Pod no arranca correctamente Si el Pod falla o no est\u00e1 en estado healthy , hay que analizar los logs de las aplicaciones para buscar m\u00e1s informaci\u00f3n. Para ello ejecutar: kubectl logs <pod-name> <container-name> ## Para ver los logs de un contenedor que haya fallado anteriormente kubectl logs --previous <pod-name> <container-name> Otra forma para debugar el Pod es accediendo a sus contenedores mediante una Shell, si estos disponen de debugging utilities que puedan ser utilizadas (p.ej. realizar un cat sobre un fichero concreto): kubectl exec -it cassandra -- sh Pod no tiene comportamiento esperado Si el problema es que el Pod no se ejecuta como se esperaba, es posible que haya alg\u00fan tipo de error en su descriptor de despliegue que haga que no se ejecute correctamente (p.ej. si hay alg\u00fan error tipogr\u00e1fico, esa propiedad es ignorada). Para validar si se trata de un error con el descriptor, habr\u00eda que eliminar el Pod y volver a ejecutarlo con la opci\u00f3n --validate : kubectl apply --validate -f mypod.yaml Si hubiera alg\u00fan tipo de problema, saltar\u00eda un mensaje de error informando de qu\u00e9 est\u00e1 mal. Otra comprobaci\u00f3n ser\u00eda obtener la descripci\u00f3n del Pod desplegado y compararla con la del descriptor para ver si existe alguna diferencia (sin tener en cuenta los campos que a\u00f1ade el servidor de la API. p.ej. startTime ). kubectl get pods/mypod -o yaml > mypod-on-apiserver.yaml","title":"Debugging Pods"},{"location":"monitoring/debugging/#contenedores-efimeros-alpha","text":"Cuando acceder via Shell a dentro de un contenedor no sea posible o suficiente (p.ej. porque la imagen que se est\u00e9 utilizando no disponga de debugging utilities ), otra alternativa son los contenedores ef\u00edmeros. Estos contenedores se levantar\u00e1n dentro del mismo Pod y compartir\u00e1n el mismo \"namespace de procesos\". De esta forma, el nuevo contenedor compartir\u00e1 el mismo arbol de procesos y sistema de ficheros que el contenedor de la aplicaci\u00f3n. El comando a ejecutar para levantar un contenedor ef\u00edmero: kubectl alpha debug -it ephemeral-demo --image = busybox --target = <app-container-name>","title":"Contenedores ef\u00edmeros (alpha)"},{"location":"monitoring/debugging/#acceder-logs-de-init-container","text":"Para leer los logs de un Init Container ejecutar: kubectl logs <pod-name> -c <init-container-name> Si los contenedores ejecutan un script bash, se puede ejecutar set -x al inicio del script para que se graben en el logs los comandos que se van a ir ejecutando.","title":"Acceder logs de Init Container"},{"location":"monitoring/debugging/#mensaje-de-terminacion","text":"Cuando un contenedor termina su ejecuci\u00f3n (ya sea porque ha terminado de ejecutar su proceso o por un error), vuelca un \"mensaje de terminaci\u00f3n\" sobre el fichero /dev/termination-log . Si el contenedor volcara el mensaje en otro fichero, se puede indicar en la definici\u00f3n del Pod con la propiedad spec.containers[].terminationMessagePath . Adicionalmente se puede a\u00f1adir la propiedad spec.containers[].terminationMessagePolicy: \"FallbackToLogsOnError\" , para que si el contenedor termina con error y el fichero del \"mensaje de terminaci\u00f3n\" est\u00e1 vac\u00edo, utilice el \u00faltimo bloque de logs del contenedor como \"mensaje de terminaci\u00f3n\".","title":"Mensaje de terminaci\u00f3n"},{"location":"monitoring/debugging/#debugging-replication-controllers","text":"Dado que los Replication Controllers solo se encargan de gestionar el n\u00famero de Pods desplegados, para debugarlos \u00fanicamente hay que debugar los Pods. Adicionalmente se pueden observar los eventos de los Replica Set para ver si hubiera alg\u00fan problema a la hora de crear nuevas instancias: kubectl describe rs <replica-set-name>","title":"Debugging Replication Controllers"},{"location":"monitoring/debugging/#debugging-services","text":"Para empezar a debugar un Service hay que revisar que tenga el n\u00famero de Endpoints asociados correcto. Debe coincidir con el n\u00famero de Pods del Service . Para comprobarlo: kubectl get endpoints <service-name> Si el comando anterior no devolviera el mismo n\u00famero de direcciones IP que de Pods, habr\u00eda que revisar que se obtenga el listado correcto de Pods al buscar por las labels del Service : kubectl get pods --selector = label1 = value1,label2 = value2 Si el listado de Pods es el correcto, es posible que no se est\u00e9n exponiendo los puertos correctos. Habr\u00eda que comprobar que los containerPort de los Pod coinciden con los targetPort del Service. Si por el contrario, los Endpoints si tienen IP's, pero una vez se ha establecido conexi\u00f3n con el Service, la conexi\u00f3n se cae, es probable que el Pod no est\u00e9 funcionando correctamente. En ese caso habr\u00eda que revisar el contador de reinicios del Pod. Otra comprobaci\u00f3n ser\u00eda intentar acceder al Pod directamente a trav\u00e9s de su direcci\u00f3n IP. Si en ning\u00fan caso la aplicaci\u00f3n es accesible, es posible que la aplicaci\u00f3n no est\u00e9 exponiendo el mismo puerto que haya configurado en el Pod. Para comprobarlo ejecutar desde dentro el cluster: wget -gO- <pod-IP>:<port> El Service no resuelve por DNS Para comprobar que el dominio del Service resuelve correctamente, habr\u00eda que utilizar la funcionalidad nslookup desde un Pod en el mismo Namespace que la aplicaci\u00f3n. nslookup <service-name> Si fallara, podria ser que Pod y Service estubieran en Namespaces diferentes. Para ello comprobar: nslookup <service-name>.<service-namespace> Si a\u00fan as\u00ed fallara, habr\u00eda quer comprobar el FQDN del Service : nslookup <service-name>.<service-namespace>.svc.cluster.local Si resuelve correctamente, habr\u00eda que comprobar el fichero /etc/resolv.conf del Pod para ver que sea correcto: Debe contener una entrada nameserver con la IP del servicio DNS del cluster. Debe contener una entrada search con todos los sufijos del cluster: <service-name>.<service-namespace>.svc.cluster.local para encontrar los Service del mismo Namespace. svc.cluster.local para encontrar los Service de cualquier Namespace. cluster.local para encontrar dominios del cluster. Debe contener una entrada options con la propiedad ndots con un valor suficientemente alto como para que el servidor DNS considere los search paths . Si a\u00fan asi el comando fallara, s\u00f3lo quedar\u00eda comprobar el dominio de la API de Kubernetes, ya que este siempre deber\u00eda funcionar: nslookup kubernetes.default Si este comando falla, habria que comprobar que el kube-proxy funcione correctamente. El Service no funciona por IP Comprobar que el Service es accesible via IP. La direcci\u00f3n IP se puede obtener con el comando kubectl get svc <service-name> . Para comprobar que se puede acceder via IP ejecutar desde dentro del cluster: wget -qO- <service-IP>:<port> Si no funcionara, habr\u00eda que revisar que el Service est\u00e9 correctamente configurado con los puertos y protocolos correctos y que los Pods se est\u00e9n ejecutando correctamente.","title":"Debugging Services"},{"location":"monitoring/debugging/#comprobar-que-kube-proxy-funciona-correctamente","text":"Si los Endpoints y la configuraci\u00f3n de los Services es correcta, es probable que el problema est\u00e9 en la implementaci\u00f3n que se ejecute por debajo de los Services . En la mayoria de clusters Kubernetes ser\u00e1 kube-proxy . Primero habr\u00eda que comprobar que el proceso de kube-proxy se est\u00e9 ejecutando en el nodo. Para ello, habr\u00e1 que acceder via SSH y ejecutar el siguiente comando: ps auxw | grep kube-proxy Si el proceso se est\u00e1 ejecutando, habria que comprobar en los logs, si est\u00e1 teniendo dificultades para contactar al _master. Se puede revisar el fichero /var/log/kube-proxy.log . Si aparecen mensajes de error habr\u00eda que revisar la configuraci\u00f3n del nodo y revisar si el proceso de instalaci\u00f3n se ha realizado correctamente. Si la instalaci\u00f3n se ha realizado manualmente, deber\u00eda de haberse instalado el binario conntrack , por lo que habr\u00eda que revisar que se haya instalado correctamente. En funci\u00f3n del modo de ejecuci\u00f3n del kube-proxy ( iptables o ipvs ), habr\u00eda que revisar la tabla de redirecciones correspondiente para comprobar que las entradas son correctas: ## iptables iptables-save | grep <service-name> # -> Debe devolver una regla KUBE-SERVICES y una cadena KUBE-SVC-<hash> ## ipvs ipvsadm -ln # -> Debe devolver una entrada con la <service-IP>:<service-port> del Service y tantas entradas <pod-IP>:<pod-port> como Pods tenga el Service Cuando el kube-proxy opera en modo iptables , se puede dar el caso que un Pod no pueda accederse a si mismo a trav\u00e9s de la IP del Service. En ese caso habr\u00eda que revisar que el flag hairpin-mode de kube-proxy est\u00e9 correctamente configurado (ver Documentaci\u00f3n para m\u00e1s detalle).","title":"Comprobar que kube-proxy funciona correctamente"},{"location":"monitoring/debugging/#debugging-del-cluster","text":"Se puede obtener el estado actual de los nodos con el comando kubectl get nodes . Mediante el comando kubectl describe node <node-name> , se puede obtener informaci\u00f3n general del nodo: Configuraci\u00f3n del nodo Estado actual del nodo Capacidad del nodo Recursos disponibles y en uso Eventos del nodo Nota Una forma de obtener informaci\u00f3n m\u00e1s detallada acerca de un nodo es mediante el comando kubectl get node <node-name> -o yaml . De esta forma se puede obtener en formato YAML toda la informaci\u00f3n que hay almacenada en Kubernetes sobre ese nodo. Si se desea obtener informaci\u00f3n m\u00e1s detallada del estado global del cluster se puede ejecutar el comando: kubectl cluster-info dump Advertencia Este comando devuelve TODA a informaci\u00f3n almacenada en el cluster (descriptores, logs, eventos...), por lo que hayq que realizar un proceso de b\u00fasqueda para encontrar la informaci\u00f3n que se desea. Para comprobar que todos los componentes del cluster operan correctamente hay que ir nodo por nodo observando los diferentes logs: Nodos master - API server: /var/log/kube-apiserver.log - Scheduler: /var/log/kube-scheduler.log - Controller: /var/log/kube-controller-manager.log Nodos worker - kubelet: /var/log/kubelet.log - kube-proxy: /var/log/kube-proxy.log","title":"Debugging del cluster"},{"location":"monitoring/monitoring/","text":"Monitoring Monitorizar con metrics-server Por defecto, los kubelet tienen instalado un agente cAdvisor para monitorizar las m\u00e9tricas del nodo en el que est\u00e1n. Estas m\u00e9tricas las expone a trav\u00e9s de su API, y se requiere de una aplicaci\u00f3n adicional para agrupar todas las m\u00e9tricas del cluster. Normalmente se provee metrics-server en el cluster para llevar a cabo esa tarea. Este aplicativo lo que hace es llamar a esos endpoint y almacenar los datos en memoria. Esto permite monitorizar el cluster en tiempo real, pero no realizar comparativas en un hist\u00f3rico. Para monitorizar las m\u00e9tricas a nivel de nodo, ejecutar: kubectl top nodes Para consultar las m\u00e9tricas a nivel de pods: kubectl top pods","title":"Monitoring"},{"location":"monitoring/monitoring/#monitoring","text":"","title":"Monitoring"},{"location":"monitoring/monitoring/#monitorizar-con-metrics-server","text":"Por defecto, los kubelet tienen instalado un agente cAdvisor para monitorizar las m\u00e9tricas del nodo en el que est\u00e1n. Estas m\u00e9tricas las expone a trav\u00e9s de su API, y se requiere de una aplicaci\u00f3n adicional para agrupar todas las m\u00e9tricas del cluster. Normalmente se provee metrics-server en el cluster para llevar a cabo esa tarea. Este aplicativo lo que hace es llamar a esos endpoint y almacenar los datos en memoria. Esto permite monitorizar el cluster en tiempo real, pero no realizar comparativas en un hist\u00f3rico. Para monitorizar las m\u00e9tricas a nivel de nodo, ejecutar: kubectl top nodes Para consultar las m\u00e9tricas a nivel de pods: kubectl top pods","title":"Monitorizar con metrics-server"},{"location":"network/dns/","text":"DNS Kubernetes por defecto ofrece una implementaci\u00f3n de DNS (CoreDNS), desplegando un Pod y un Service , y configurando el resto de Pods para que utilicen la IP de este Service para la resoluci\u00f3n de nombres DNS. El kubelet es el encargado de configurar los Pods, a\u00f1adiendo una entrada con la IP del Service del CoreDNS en el /etc/resolv.conf . Adicionalmente, a\u00f1ade varias entradas search para facilitar la resoluci\u00f3n de nombres de los Service que est\u00e9n dentro del mismo Namespace y cluster. De esta forma, un Pod que est\u00e9 en el mismo Namespace que un Service my-service , podr\u00e1 llamar a ese servicio simplemente con my-service , mientras que un Pod de otro Namespace, deber\u00e1 llamar al servicio por my-service.my-namespace . Tipos de registros DNS Registros para Service Los Service normales (no Headless) generan un registro en el DNS de tipo A (o AAAA para IPv6) con el dominio my-service.my-namespace.svc.cluster-domain.example y la Cluster IP del Service . Los Service Headless por su parte, generan tambi\u00e9n un registro DNS de tipo A/AAAA con el mismo dominio, pero resuelven a todo el conjunto de IP's de los Pods que forman parte del Service . Los clientes pueden consumir todo el grupo de Ip's o implementar una selecci\u00f3n de tipo Round-Robin. Tanto para los Service normales, como los Headless, cuando hay puertos con nombre, se genera un registro DNS de tipo SRV con la forma _my-port-name._my-port-protocol.my-service.my-namespace.svc.cluster-domain.example . Este tipo de registros resuelven para los Service normales, con el n\u00famero del puerto y el nombre del dominio del Service ( my-service.my-namespace.svc.cluster-domain.example ), y para los Service Headless devuelve m\u00faltiples respuestas con el n\u00famero del puerto y el dominio del Pod con la forma auto-generated-name.my-service.my-namespace.svc.cluster-domain.example . Registros para Pods Todos los Pods creados por un Deployment o DaemonSet generan un registro DNS de tipo A/AAAA del estilo pod-ip-address.deployment-name.my-namespace.svc.cluster-domain.example . Notas En la definici\u00f3n de los Pods, existen unos campos .spec.hostname y .spec.subdomain que permiten sobrescribir el nombre de dominio del Pod (por defecto se utiliza .metadata.name ). Si existe un Service Headless con el mismo nombre que .spec.subdomain , se generar\u00e1 un registro DNS de tipo A/AAAA para la IP de ese Pod. Por ejemplo: hostname : \"foo\" subdomain : \"bar\" Pod FQDN: foo.bar.my-namespace.svc.cluster-domain.example DNS Policies para Pods Kubernetes permite que se le configure una DNS policy diferente a cada uno de los Pods, mediante la stanza .spec.dnsPolicy : Default : El Pod hereda la configuraci\u00f3n de resoluci\u00f3n de nombres que tenga el nodo en el que corre el Pod. ClusterFirst : (Por defecto) Cualquier query DNS que no pertenezca al dominio del cluster Kubernetes, ser\u00e1 redireccionada al nameserver que se herede de la configuraci\u00f3n del nodo. De esta forma los administradores del cluster pueden a\u00f1adir subdominios u otros servidores DNS en la configuraci\u00f3n del nameserver por defecto. ClusterFirstWithHostNet : Este valor es necesario para los Pods que tengan a true el campo .spec.hostNetwork (es decir, los Pods que tengan acceso a la red del nodo en el que est\u00e1n alojados). None : El Pod ignora la configuraci\u00f3n DNS del cluster y s\u00f3lo utiliza la configuraci\u00f3n que se especifique en .spec.dnsConfig . Configuraci\u00f3n DNS de los Pods La stanza .spec.dnsConfig se puede utilizar con cualquier tipo de .spec.dnsPolicy . S\u00f3lo es obligat\u00f3ria cuando .spec.dnsPolicy es None . Estas son las propiedades que se pueden configurar: nameservers : listado de IP's de los servidores DNS que puede utilizar el Pod. Como m\u00e1ximo se pueden especificar 3 IP's. Solo es obligatorio especificar una IP en el caso de que .spec.dnsPolicy sea None . Salvo el caso anterior, el Pod podr\u00e1 utilizar estos nameservers m\u00e1s los que haya en la configuraci\u00f3n global del cluster. searches : listado de dominios search para la b\u00fasqueda de dominios de Pods. Como m\u00e1ximo se pueden a\u00f1adir 6 dominios search . Estos dominios se unir\u00e1n a los que ya haya configurados en la DNS policy. options : propiedades adicionales con name y opcionalmente value . Ejemplo apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 1.2.3.4 searches : - ns1.svc.cluster-domain.example - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0 /etc/hosts del Pod El fichero /etc/hosts de un Pod es gestionado por el kubelet . Si se quieren a\u00f1adir entradas adicionales en el fichero de los Containers de un Pod, se debe hacer mediante la stanza .spec.hostAliases , indicando la IP y los hostname asociados. Ejemplo apiVersion : v1 kind : Pod metadata : name : hostaliases-pod spec : restartPolicy : Never hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\" - ip : \"10.1.2.3\" hostnames : - \"foo.remote\" - \"bar.remote\" containers : - name : cat-hosts image : busybox command : - cat args : - \"/etc/hosts\"","title":"DNS"},{"location":"network/dns/#dns","text":"Kubernetes por defecto ofrece una implementaci\u00f3n de DNS (CoreDNS), desplegando un Pod y un Service , y configurando el resto de Pods para que utilicen la IP de este Service para la resoluci\u00f3n de nombres DNS. El kubelet es el encargado de configurar los Pods, a\u00f1adiendo una entrada con la IP del Service del CoreDNS en el /etc/resolv.conf . Adicionalmente, a\u00f1ade varias entradas search para facilitar la resoluci\u00f3n de nombres de los Service que est\u00e9n dentro del mismo Namespace y cluster. De esta forma, un Pod que est\u00e9 en el mismo Namespace que un Service my-service , podr\u00e1 llamar a ese servicio simplemente con my-service , mientras que un Pod de otro Namespace, deber\u00e1 llamar al servicio por my-service.my-namespace .","title":"DNS"},{"location":"network/dns/#tipos-de-registros-dns","text":"","title":"Tipos de registros DNS"},{"location":"network/dns/#registros-para-service","text":"Los Service normales (no Headless) generan un registro en el DNS de tipo A (o AAAA para IPv6) con el dominio my-service.my-namespace.svc.cluster-domain.example y la Cluster IP del Service . Los Service Headless por su parte, generan tambi\u00e9n un registro DNS de tipo A/AAAA con el mismo dominio, pero resuelven a todo el conjunto de IP's de los Pods que forman parte del Service . Los clientes pueden consumir todo el grupo de Ip's o implementar una selecci\u00f3n de tipo Round-Robin. Tanto para los Service normales, como los Headless, cuando hay puertos con nombre, se genera un registro DNS de tipo SRV con la forma _my-port-name._my-port-protocol.my-service.my-namespace.svc.cluster-domain.example . Este tipo de registros resuelven para los Service normales, con el n\u00famero del puerto y el nombre del dominio del Service ( my-service.my-namespace.svc.cluster-domain.example ), y para los Service Headless devuelve m\u00faltiples respuestas con el n\u00famero del puerto y el dominio del Pod con la forma auto-generated-name.my-service.my-namespace.svc.cluster-domain.example .","title":"Registros para Service"},{"location":"network/dns/#registros-para-pods","text":"Todos los Pods creados por un Deployment o DaemonSet generan un registro DNS de tipo A/AAAA del estilo pod-ip-address.deployment-name.my-namespace.svc.cluster-domain.example . Notas En la definici\u00f3n de los Pods, existen unos campos .spec.hostname y .spec.subdomain que permiten sobrescribir el nombre de dominio del Pod (por defecto se utiliza .metadata.name ). Si existe un Service Headless con el mismo nombre que .spec.subdomain , se generar\u00e1 un registro DNS de tipo A/AAAA para la IP de ese Pod. Por ejemplo: hostname : \"foo\" subdomain : \"bar\" Pod FQDN: foo.bar.my-namespace.svc.cluster-domain.example","title":"Registros para Pods"},{"location":"network/dns/#dns-policies-para-pods","text":"Kubernetes permite que se le configure una DNS policy diferente a cada uno de los Pods, mediante la stanza .spec.dnsPolicy : Default : El Pod hereda la configuraci\u00f3n de resoluci\u00f3n de nombres que tenga el nodo en el que corre el Pod. ClusterFirst : (Por defecto) Cualquier query DNS que no pertenezca al dominio del cluster Kubernetes, ser\u00e1 redireccionada al nameserver que se herede de la configuraci\u00f3n del nodo. De esta forma los administradores del cluster pueden a\u00f1adir subdominios u otros servidores DNS en la configuraci\u00f3n del nameserver por defecto. ClusterFirstWithHostNet : Este valor es necesario para los Pods que tengan a true el campo .spec.hostNetwork (es decir, los Pods que tengan acceso a la red del nodo en el que est\u00e1n alojados). None : El Pod ignora la configuraci\u00f3n DNS del cluster y s\u00f3lo utiliza la configuraci\u00f3n que se especifique en .spec.dnsConfig .","title":"DNS Policies para Pods"},{"location":"network/dns/#configuracion-dns-de-los-pods","text":"La stanza .spec.dnsConfig se puede utilizar con cualquier tipo de .spec.dnsPolicy . S\u00f3lo es obligat\u00f3ria cuando .spec.dnsPolicy es None . Estas son las propiedades que se pueden configurar: nameservers : listado de IP's de los servidores DNS que puede utilizar el Pod. Como m\u00e1ximo se pueden especificar 3 IP's. Solo es obligatorio especificar una IP en el caso de que .spec.dnsPolicy sea None . Salvo el caso anterior, el Pod podr\u00e1 utilizar estos nameservers m\u00e1s los que haya en la configuraci\u00f3n global del cluster. searches : listado de dominios search para la b\u00fasqueda de dominios de Pods. Como m\u00e1ximo se pueden a\u00f1adir 6 dominios search . Estos dominios se unir\u00e1n a los que ya haya configurados en la DNS policy. options : propiedades adicionales con name y opcionalmente value . Ejemplo apiVersion : v1 kind : Pod metadata : namespace : default name : dns-example spec : containers : - name : test image : nginx dnsPolicy : \"None\" dnsConfig : nameservers : - 1.2.3.4 searches : - ns1.svc.cluster-domain.example - my.dns.search.suffix options : - name : ndots value : \"2\" - name : edns0","title":"Configuraci\u00f3n DNS de los Pods"},{"location":"network/dns/#etchosts-del-pod","text":"El fichero /etc/hosts de un Pod es gestionado por el kubelet . Si se quieren a\u00f1adir entradas adicionales en el fichero de los Containers de un Pod, se debe hacer mediante la stanza .spec.hostAliases , indicando la IP y los hostname asociados. Ejemplo apiVersion : v1 kind : Pod metadata : name : hostaliases-pod spec : restartPolicy : Never hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\" - ip : \"10.1.2.3\" hostnames : - \"foo.remote\" - \"bar.remote\" containers : - name : cat-hosts image : busybox command : - cat args : - \"/etc/hosts\"","title":"/etc/hosts del Pod"},{"location":"network/endpoint/","text":"EndpointSlices Este recurso de la API de Kubernetes, es una versi\u00f3n mejorada de los Endpoints , sobretodo para clusters de gran tama\u00f1o. Los EndpontSlices contienen referencias a los endpoints de los Pods y sirven para mejorar el rendimiento de enrutamiento del kube-proxy . Para ello limitan a 100, el n\u00famero de endpoints a los que pueden referenciar, evitando as\u00ed el uso de un \u00fanico Endpoints por Service con centenares de endpoints, el cual es m\u00e1s complejo de gestionar y escalar. Nota El campo endpoints[].topology aporta informaci\u00f3n sobre el nodo al que pertenece el endpoint. Ejemplo apiVersion : discovery.k8s.io/v1beta1 kind : EndpointSlice metadata : name : example-abc labels : kubernetes.io/service-name : example addressType : IPv4 ports : - name : http protocol : TCP port : 80 endpoints : - addresses : - \"10.1.2.3\" conditions : ready : true hostname : pod-1 topology : kubernetes.io/hostname : node-1 topology.kubernetes.io/zone : us-west2-a","title":"EndpointSlices"},{"location":"network/endpoint/#endpointslices","text":"Este recurso de la API de Kubernetes, es una versi\u00f3n mejorada de los Endpoints , sobretodo para clusters de gran tama\u00f1o. Los EndpontSlices contienen referencias a los endpoints de los Pods y sirven para mejorar el rendimiento de enrutamiento del kube-proxy . Para ello limitan a 100, el n\u00famero de endpoints a los que pueden referenciar, evitando as\u00ed el uso de un \u00fanico Endpoints por Service con centenares de endpoints, el cual es m\u00e1s complejo de gestionar y escalar. Nota El campo endpoints[].topology aporta informaci\u00f3n sobre el nodo al que pertenece el endpoint. Ejemplo apiVersion : discovery.k8s.io/v1beta1 kind : EndpointSlice metadata : name : example-abc labels : kubernetes.io/service-name : example addressType : IPv4 ports : - name : http protocol : TCP port : 80 endpoints : - addresses : - \"10.1.2.3\" conditions : ready : true hostname : pod-1 topology : kubernetes.io/hostname : node-1 topology.kubernetes.io/zone : us-west2-a","title":"EndpointSlices"},{"location":"network/ingress/","text":"Ingress Este objeto de Kubernetes sirve para gestionar el acceso a los servicios desde fuera del cluster. Es una alternativa a los Service de tipo NodePort y LoadBalancer . Ofrece a los Service la posibilidad de ser accedidos via URL, balanceamiento de carga, terminaci\u00f3n SSL y virtual hosting basado en nombre (hospedar multiples domain names con una misma direcci\u00f3n IP). El Ingress requiere de dos componentes para operar: Ingress Controller : Componente encargado de realizar las operaciones que ofrece el Ingress. Este componente no viene por defecto en el cluster y debe de instalarse a parte (p.ej. Nginx Ingress Controller). Recurso Ingress : Objeto Kubernetes en el que se configuran las rutas HTTP o HTTPS que se exponen hacia fuera del cluster. Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : test-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - http : paths : - path : /testpath pathType : Prefix backend : serviceName : test servicePort : 80 Notas Mediante las anotaciones se pueden configurar las diferentes funcionalidades del Ingress Controller . En funci\u00f3n del Ingress Controller instalado las funcionalidades a configurar ser\u00e1n diferentes. En la stanza .spec de los recursos Ingress se detalla toda la informaci\u00f3n necesaria para configurar el balanceador de carga o el servidor proxy, as\u00ed como las reglas de redireccionamiento de las peticiones entrantes. Reglas Ingress Cada regla HTTP contiene la siguiente informaci\u00f3n: host : (Opcional) Indica que la regla s\u00f3lo se aplicar\u00e1 para las peticiones entrantes cuyo hostname destino sea este (p.ej. foo.bar.com ). Si no se especifica esta propiedad, se toma como valor * por lo que aplicar\u00e1 a todas las peticiones. http.paths : Listado de paths de las URL para los que aplicar\u00e1 esta regla. Cada path esta asociado a un serviceName y servicePort para indicar a qu\u00e9 Service redirigir\u00e1 el tr\u00e1fico. backend : Definici\u00f3n del Service al que se redirigir\u00e1 el tr\u00e1fico de un path. Contiene las propiedades serviceName y servicePort . Informaci\u00f3n Las peticiones que no cumplan los requisitos de las reglas del recurso Ingress ser\u00e1n redirigidos a un backend por defecto, configurable mediante anotaciones o en la propia configuraci\u00f3n del Ingress Controller . Tipos de path Existen 3 tipos de path que pueden configurarse en un recurso Ingress : ImplementationSpecific : (Por defecto) C\u00f3mo se hace match entre la definici\u00f3n del path y el path de la URL, se delega en la implementaci\u00f3n del Ingress Controler seleccionado con el Ingress Class . Exact : El path de la URL debe ser exactamente el mismo que el de la definici\u00f3n (case sensitive). Prefix : El path de la URL debe coincidir exactamente con un prefijo que empiece por / . Por ejemplo, una regla con path /foo/bar aplicar\u00eda para una URL con path /foo/bar/baz pero no con una que tenga /foo/barbaz . Informaci\u00f3n Si se producen varios match, se dar\u00e1 prioridad a la regla que haga el match m\u00e1s largo. Si a\u00fan asi hay empate entre varias reglas, se dar\u00e1 prioridad a las reglas Exact por encima de las Prefix . Ingress Controller El Ingress Controller suele estar formado por estos componentes: Deployment : gestionar\u00e1 el n\u00famero de replicas Service : de tipo NodePort o LoadBalancer . El loadbalancer del cloud provider redirigir\u00e1 las peticiones entrantes a este Service Configmap : contendr\u00e1 la configuraci\u00f3n del Controller ServiceAccount + Role/RoleBinding : permisos para que el Controller est\u00e9 a la escucha de nuevos recursos Ingress Ingress Class Se pueden instalar varias implementaciones diferentes de Ingress Controller en un mismo cluster. Para identificar a cada una de estas implementaciones se utilizan las IngressClass . Este objeto Kubernetes contiene el nombre del Ingress Controller que implementa esta clase, as\u00ed como configuraci\u00f3n adicional del Controller. Notas En los recursos Ingress se indica el nombre de qu\u00e9 IngressClass debe procesar esas reglas mediante la stanza .spec.ingressClassName . Se puede configurar que un IngressClass sea la clase por defecto con la anotaci\u00f3n ingressclass.kubernetes.io/is-default-class: true . De esta forma todos los recursos Ingress que no utilicen la propiedad ingressClassName , ser\u00e1n procesados por este IngressClass . Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : IngressClass metadata : name : external-lb spec : controller : example.com/ingress-controller parameters : apiGroup : k8s.example.com/v1alpha kind : IngressParameters name : external-lb Tipos de Ingress Single Service Ingress Pese a que Kubernetes ofrece m\u00e9todos alternativos a este, el Ingress permite exponer un \u00fanico servicio, especificando un \"backend por defecto\" (sin reglas). Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : test-ingress spec : backend : serviceName : testsvc servicePort : 80 Simple fanout Esta configuraci\u00f3n te permite exponer m\u00faltiples servicios a trav\u00e9s de una \u00fanica IP (manteniendo al m\u00ednimo el n\u00famero de load balancers necesarios), bas\u00e1ndose en la URI HTTP: foo.bar.com -> 178.91.123.132 -> / foo service1:4200 / bar service2:8080 Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : simple-fanout-example annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - host : foo.bar.com http : paths : - path : /foo backend : serviceName : service1 servicePort : 4200 - path : /bar backend : serviceName : service2 servicePort : 8080 Name based virtual hosting El Virtual Hosting basado en nombre permite exponer multiples servicios en una misma IP bas\u00e1ndose en el hostname de la petici\u00f3n: foo.bar.com --| |-> foo.bar.com service1:80 | 178.91.123.132 | bar.foo.com --| |-> bar.foo.com service2:80 Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : name-virtual-host-ingress spec : rules : - host : foo.bar.com http : paths : - backend : serviceName : service1 servicePort : 80 - host : bar.foo.com http : paths : - backend : serviceName : service2 servicePort : 80 TLS Se puede securizar el acceso al Ingress especificando un Secret que contenga un certificado TLS y su clave privada. El CN del certificado debe coincidir con el host del recurso Ingress . El Secret debe contener las keys tls.crt y tls.key : apiVersion : v1 kind : Secret metadata : name : testsecret-tls namespace : default type : kubernetes.io/tls data : tls.crt : base64 encoded cert tls.key : base64 encoded key Notas Actualmente el Ingress solo permite el puerto 443 como TLS y asume terminaci\u00f3n TLS. Es decir securiza \u00fanicamente la comunicaci\u00f3n entre el cliente y el load balancer. El Ingress permite exponer multiples hosts en el mismo puerto TLS. Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : tls-example-ingress spec : tls : - hosts : - sslexample.foo.com secretName : testsecret-tls rules : - host : sslexample.foo.com http : paths : - path : / backend : serviceName : service1 servicePort : 80","title":"Ingress"},{"location":"network/ingress/#ingress","text":"Este objeto de Kubernetes sirve para gestionar el acceso a los servicios desde fuera del cluster. Es una alternativa a los Service de tipo NodePort y LoadBalancer . Ofrece a los Service la posibilidad de ser accedidos via URL, balanceamiento de carga, terminaci\u00f3n SSL y virtual hosting basado en nombre (hospedar multiples domain names con una misma direcci\u00f3n IP). El Ingress requiere de dos componentes para operar: Ingress Controller : Componente encargado de realizar las operaciones que ofrece el Ingress. Este componente no viene por defecto en el cluster y debe de instalarse a parte (p.ej. Nginx Ingress Controller). Recurso Ingress : Objeto Kubernetes en el que se configuran las rutas HTTP o HTTPS que se exponen hacia fuera del cluster. Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : test-ingress annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - http : paths : - path : /testpath pathType : Prefix backend : serviceName : test servicePort : 80 Notas Mediante las anotaciones se pueden configurar las diferentes funcionalidades del Ingress Controller . En funci\u00f3n del Ingress Controller instalado las funcionalidades a configurar ser\u00e1n diferentes. En la stanza .spec de los recursos Ingress se detalla toda la informaci\u00f3n necesaria para configurar el balanceador de carga o el servidor proxy, as\u00ed como las reglas de redireccionamiento de las peticiones entrantes.","title":"Ingress"},{"location":"network/ingress/#reglas-ingress","text":"Cada regla HTTP contiene la siguiente informaci\u00f3n: host : (Opcional) Indica que la regla s\u00f3lo se aplicar\u00e1 para las peticiones entrantes cuyo hostname destino sea este (p.ej. foo.bar.com ). Si no se especifica esta propiedad, se toma como valor * por lo que aplicar\u00e1 a todas las peticiones. http.paths : Listado de paths de las URL para los que aplicar\u00e1 esta regla. Cada path esta asociado a un serviceName y servicePort para indicar a qu\u00e9 Service redirigir\u00e1 el tr\u00e1fico. backend : Definici\u00f3n del Service al que se redirigir\u00e1 el tr\u00e1fico de un path. Contiene las propiedades serviceName y servicePort . Informaci\u00f3n Las peticiones que no cumplan los requisitos de las reglas del recurso Ingress ser\u00e1n redirigidos a un backend por defecto, configurable mediante anotaciones o en la propia configuraci\u00f3n del Ingress Controller .","title":"Reglas Ingress"},{"location":"network/ingress/#tipos-de-path","text":"Existen 3 tipos de path que pueden configurarse en un recurso Ingress : ImplementationSpecific : (Por defecto) C\u00f3mo se hace match entre la definici\u00f3n del path y el path de la URL, se delega en la implementaci\u00f3n del Ingress Controler seleccionado con el Ingress Class . Exact : El path de la URL debe ser exactamente el mismo que el de la definici\u00f3n (case sensitive). Prefix : El path de la URL debe coincidir exactamente con un prefijo que empiece por / . Por ejemplo, una regla con path /foo/bar aplicar\u00eda para una URL con path /foo/bar/baz pero no con una que tenga /foo/barbaz . Informaci\u00f3n Si se producen varios match, se dar\u00e1 prioridad a la regla que haga el match m\u00e1s largo. Si a\u00fan asi hay empate entre varias reglas, se dar\u00e1 prioridad a las reglas Exact por encima de las Prefix .","title":"Tipos de path"},{"location":"network/ingress/#ingress-controller","text":"El Ingress Controller suele estar formado por estos componentes: Deployment : gestionar\u00e1 el n\u00famero de replicas Service : de tipo NodePort o LoadBalancer . El loadbalancer del cloud provider redirigir\u00e1 las peticiones entrantes a este Service Configmap : contendr\u00e1 la configuraci\u00f3n del Controller ServiceAccount + Role/RoleBinding : permisos para que el Controller est\u00e9 a la escucha de nuevos recursos Ingress","title":"Ingress Controller"},{"location":"network/ingress/#ingress-class","text":"Se pueden instalar varias implementaciones diferentes de Ingress Controller en un mismo cluster. Para identificar a cada una de estas implementaciones se utilizan las IngressClass . Este objeto Kubernetes contiene el nombre del Ingress Controller que implementa esta clase, as\u00ed como configuraci\u00f3n adicional del Controller. Notas En los recursos Ingress se indica el nombre de qu\u00e9 IngressClass debe procesar esas reglas mediante la stanza .spec.ingressClassName . Se puede configurar que un IngressClass sea la clase por defecto con la anotaci\u00f3n ingressclass.kubernetes.io/is-default-class: true . De esta forma todos los recursos Ingress que no utilicen la propiedad ingressClassName , ser\u00e1n procesados por este IngressClass . Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : IngressClass metadata : name : external-lb spec : controller : example.com/ingress-controller parameters : apiGroup : k8s.example.com/v1alpha kind : IngressParameters name : external-lb","title":"Ingress Class"},{"location":"network/ingress/#tipos-de-ingress","text":"","title":"Tipos de Ingress"},{"location":"network/ingress/#single-service-ingress","text":"Pese a que Kubernetes ofrece m\u00e9todos alternativos a este, el Ingress permite exponer un \u00fanico servicio, especificando un \"backend por defecto\" (sin reglas). Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : test-ingress spec : backend : serviceName : testsvc servicePort : 80","title":"Single Service Ingress"},{"location":"network/ingress/#simple-fanout","text":"Esta configuraci\u00f3n te permite exponer m\u00faltiples servicios a trav\u00e9s de una \u00fanica IP (manteniendo al m\u00ednimo el n\u00famero de load balancers necesarios), bas\u00e1ndose en la URI HTTP: foo.bar.com -> 178.91.123.132 -> / foo service1:4200 / bar service2:8080 Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : simple-fanout-example annotations : nginx.ingress.kubernetes.io/rewrite-target : / spec : rules : - host : foo.bar.com http : paths : - path : /foo backend : serviceName : service1 servicePort : 4200 - path : /bar backend : serviceName : service2 servicePort : 8080","title":"Simple fanout"},{"location":"network/ingress/#name-based-virtual-hosting","text":"El Virtual Hosting basado en nombre permite exponer multiples servicios en una misma IP bas\u00e1ndose en el hostname de la petici\u00f3n: foo.bar.com --| |-> foo.bar.com service1:80 | 178.91.123.132 | bar.foo.com --| |-> bar.foo.com service2:80 Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : name-virtual-host-ingress spec : rules : - host : foo.bar.com http : paths : - backend : serviceName : service1 servicePort : 80 - host : bar.foo.com http : paths : - backend : serviceName : service2 servicePort : 80","title":"Name based virtual hosting"},{"location":"network/ingress/#tls","text":"Se puede securizar el acceso al Ingress especificando un Secret que contenga un certificado TLS y su clave privada. El CN del certificado debe coincidir con el host del recurso Ingress . El Secret debe contener las keys tls.crt y tls.key : apiVersion : v1 kind : Secret metadata : name : testsecret-tls namespace : default type : kubernetes.io/tls data : tls.crt : base64 encoded cert tls.key : base64 encoded key Notas Actualmente el Ingress solo permite el puerto 443 como TLS y asume terminaci\u00f3n TLS. Es decir securiza \u00fanicamente la comunicaci\u00f3n entre el cliente y el load balancer. El Ingress permite exponer multiples hosts en el mismo puerto TLS. Ejemplo apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : tls-example-ingress spec : tls : - hosts : - sslexample.foo.com secretName : testsecret-tls rules : - host : sslexample.foo.com http : paths : - path : / backend : serviceName : service1 servicePort : 80","title":"TLS"},{"location":"network/kube-proxy/","text":"kube-proxy Este proxy se encarga de gestionar las reglas de red de los nodos, para permitir la conexi\u00f3n con los Pods, desde dentro y fuera del cluster. Se respalda en los Service para crear estas reglas. Estas reglas de enrutamiento permiten asignar una IP Virtual a los Pods, permitiendo de esta manera que dos Pods diferentes expongan su aplicaci\u00f3n por un mismo puerto, sin que colisionen entre s\u00ed. kube-proxy puede operar en tres modos: Userspace iptables IPVS Informaci\u00f3n En cada uno de estos modos se puede establecer que todas las conexiones de un mismo cliente sean respondidas por el mismo Pod, configurando al Service con la propiedad .spec.sessionAffinity: ClientIP . Se puede limitar la duraci\u00f3n en la que esta afinidad vaya a ser vigente con la stanza .spec.sessionAffinityConfig.clientIP.timeoutSeconds . Userspace kube-proxy se encarga de mantener las reglas iptables y de redirigir el tr\u00e1fico hacia los Pods Cuando se crea un Service se le asigna una IP virtual y es detectado por todas las instancias de kube-proxy . Las instancias de kube-proxy abren un puerto aleatorio en cada uno de sus nodos y establecen una redirecci\u00f3n iptables desde la IP Virtual del Service hacia el nuevo puerto (no se trata del puerto que expone el Service ). Cuando un cliente se conecte a la IP del Service y se alcance la regla iptables , redirigir\u00e1 los paquetes hacia el puerto asignado del kube-proxy . De esta manera el proxy es el encargado de elegir hacia qu\u00e9 Pod enviar el tr\u00e1fico. El Pod se elige con el algoritmo Round-Robin. Notas Si el primer Pod seleccionado no responde, kube-proxy lo reintentar\u00e1 con otro Pod. iptables kube-proxy se encarga \u00fanicamente de mantener las reglas iptables . Cuando se crea un Service se le asigna una IP virtual y es detectado por todas las instancias de kube-proxy . Las instancias de kube-proxy instalan un conjunto de reglas iptables que redirigen los paquetes con destino la IP Virtual del Service a un conjunto de reglas por Service , que a su vez est\u00e1n enlazadas con reglas por Endpoints , las cuales redirigen hacia cada uno de los Pods mediante NAT: IP Virtual -> Reglas Service -> Reglas Endpoint -> Pod Cuando un cliente se conecte a la IP del Service y se alcance la regla iptables , redirigir\u00e1 los paquetes hacia un Pod elegido de forma aleatoria o basado en una \"session affinity\". Notas A diferencia del modo userspace , este modo no depende de que los kube-proxy est\u00e9n operativos para que las IP Virtuales funcionen y la IP del cliente no se ve alterada. Este modo es mas seguro y genera menos sobrecarga del sistema, ya que el tr\u00e1fico se gestiona directamente en el espacio kernel y no requiere cambiar entre el userspace y el espacio kernel. Si el primer Pod seleccionado no responde, la conexi\u00f3n se da por fallida. Mediante los Readiness Probes de los Pods, se limita cu\u00e1ndo kube-proxy puede a\u00f1adir la regla iptables de un Pod. IPVS kube-proxy se encarga de llamar a la interfaz netlink para crear reglas IPVS y actualizarlas si hay cambios en los Service o Endpoints . Este modo esta basado en un filtrado similar al modo iptables , pero por debajo utiliza una estructura de datos de tabla hash que opera en el espacio kernel. Esto reduce la latencia y ofrece mejor rendimiento a la hora de actualizar las reglas. Se basa en la tecnolog\u00eda IPVS , un load balancer nativo del sistema Linux. Este modo ofrece m\u00e1s opciones para balancear el tr\u00e1fico entre los Pods: rr : Round-Robin lc : Least Connection (mantiene al m\u00ednimo las conexiones abiertas) dh : Destination Hashing sh : Source Hashing sed : Shortest Expected Delay nq : Never Queue Notas Este modo ofrece mayor ancho de banda que el resto de modos. Se recomienda este modo para para clusters muy grandes, con m\u00e1s de 10.000 Service .","title":"kube-proxy"},{"location":"network/kube-proxy/#kube-proxy","text":"Este proxy se encarga de gestionar las reglas de red de los nodos, para permitir la conexi\u00f3n con los Pods, desde dentro y fuera del cluster. Se respalda en los Service para crear estas reglas. Estas reglas de enrutamiento permiten asignar una IP Virtual a los Pods, permitiendo de esta manera que dos Pods diferentes expongan su aplicaci\u00f3n por un mismo puerto, sin que colisionen entre s\u00ed. kube-proxy puede operar en tres modos: Userspace iptables IPVS Informaci\u00f3n En cada uno de estos modos se puede establecer que todas las conexiones de un mismo cliente sean respondidas por el mismo Pod, configurando al Service con la propiedad .spec.sessionAffinity: ClientIP . Se puede limitar la duraci\u00f3n en la que esta afinidad vaya a ser vigente con la stanza .spec.sessionAffinityConfig.clientIP.timeoutSeconds .","title":"kube-proxy"},{"location":"network/kube-proxy/#userspace","text":"kube-proxy se encarga de mantener las reglas iptables y de redirigir el tr\u00e1fico hacia los Pods Cuando se crea un Service se le asigna una IP virtual y es detectado por todas las instancias de kube-proxy . Las instancias de kube-proxy abren un puerto aleatorio en cada uno de sus nodos y establecen una redirecci\u00f3n iptables desde la IP Virtual del Service hacia el nuevo puerto (no se trata del puerto que expone el Service ). Cuando un cliente se conecte a la IP del Service y se alcance la regla iptables , redirigir\u00e1 los paquetes hacia el puerto asignado del kube-proxy . De esta manera el proxy es el encargado de elegir hacia qu\u00e9 Pod enviar el tr\u00e1fico. El Pod se elige con el algoritmo Round-Robin. Notas Si el primer Pod seleccionado no responde, kube-proxy lo reintentar\u00e1 con otro Pod.","title":"Userspace"},{"location":"network/kube-proxy/#iptables","text":"kube-proxy se encarga \u00fanicamente de mantener las reglas iptables . Cuando se crea un Service se le asigna una IP virtual y es detectado por todas las instancias de kube-proxy . Las instancias de kube-proxy instalan un conjunto de reglas iptables que redirigen los paquetes con destino la IP Virtual del Service a un conjunto de reglas por Service , que a su vez est\u00e1n enlazadas con reglas por Endpoints , las cuales redirigen hacia cada uno de los Pods mediante NAT: IP Virtual -> Reglas Service -> Reglas Endpoint -> Pod Cuando un cliente se conecte a la IP del Service y se alcance la regla iptables , redirigir\u00e1 los paquetes hacia un Pod elegido de forma aleatoria o basado en una \"session affinity\". Notas A diferencia del modo userspace , este modo no depende de que los kube-proxy est\u00e9n operativos para que las IP Virtuales funcionen y la IP del cliente no se ve alterada. Este modo es mas seguro y genera menos sobrecarga del sistema, ya que el tr\u00e1fico se gestiona directamente en el espacio kernel y no requiere cambiar entre el userspace y el espacio kernel. Si el primer Pod seleccionado no responde, la conexi\u00f3n se da por fallida. Mediante los Readiness Probes de los Pods, se limita cu\u00e1ndo kube-proxy puede a\u00f1adir la regla iptables de un Pod.","title":"iptables"},{"location":"network/kube-proxy/#ipvs","text":"kube-proxy se encarga de llamar a la interfaz netlink para crear reglas IPVS y actualizarlas si hay cambios en los Service o Endpoints . Este modo esta basado en un filtrado similar al modo iptables , pero por debajo utiliza una estructura de datos de tabla hash que opera en el espacio kernel. Esto reduce la latencia y ofrece mejor rendimiento a la hora de actualizar las reglas. Se basa en la tecnolog\u00eda IPVS , un load balancer nativo del sistema Linux. Este modo ofrece m\u00e1s opciones para balancear el tr\u00e1fico entre los Pods: rr : Round-Robin lc : Least Connection (mantiene al m\u00ednimo las conexiones abiertas) dh : Destination Hashing sh : Source Hashing sed : Shortest Expected Delay nq : Never Queue Notas Este modo ofrece mayor ancho de banda que el resto de modos. Se recomienda este modo para para clusters muy grandes, con m\u00e1s de 10.000 Service .","title":"IPVS"},{"location":"network/policies/","text":"Network Policies Las NetworkPolicy permiten definir un conjunto de reglas de red para determinar qu\u00e9 conexiones se pueden establecer entre distintos grupos de Pods u otros endpoints de la red. Estos recursos de Kubernetes utilizan labels para determinar sobre qu\u00e9 Pods aplicar las reglas de red que determinar\u00e1n qu\u00e9 tr\u00e1fico se permite en los Pods seleccionados. Cuidado Para poder utilizar los NetworkPolicy es necesario que el network plugin que est\u00e9 utilizando Kubernetes implemente dicho recurso. De lo contrario, las reglas no tendr\u00e1n efecto. Notas Los Pods por defecto permiten todo tipo de tr\u00e1fico. Si un Pod es seleccionado por una NetworkPolicy se le aplicar\u00e1n las reglas que haya definidas. Si no tiene reglas, denegar\u00e1 todo el tr\u00e1fico entrante/saliente. Un Pod puede ser seleccionado por m\u00e1s de una NetworkPolicy . En ese caso, se aplicar\u00e1 la uni\u00f3n de todas las reglas definidas (no se producen conflictos). Las NetworkPolicy pueden definir reglas Ingress , Egress o ambas. Los tipos de reglas que se van a definir se indican en .spec.policyTypes . Ejemplo Este ejemplo permite las siguientes conexiones para los Pods del Namespace default que tengan la Label role=db : Tr\u00e1fico entrante : Desde cualquier Pod con IP dentro del rango 172.17.0.0-172.17.0.255 y 172.17.2.0-172.17.255.255 hacia el puerto 6379 Desde cualquier Pod que est\u00e9 en un Namespace etiquetado con project=myproject hacia el puerto 6379 Desde cualquier Pod del Namespace default etiquetado con role=frontend hacia el puerto 6379 Tr\u00e1fico saliente : Hacia el puerto 5978 de cualquier Pod dentro del rango de IP's 10.0.0.0-10.0.0.255 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : test-network-policy namespace : default spec : podSelector : matchLabels : role : db policyTypes : - Ingress - Egress ingress : - from : - ipBlock : cidr : 172.17.0.0/16 except : - 172.17.1.0/24 - namespaceSelector : matchLabels : project : myproject - podSelector : matchLabels : role : frontend ports : - protocol : TCP port : 6379 egress : - to : - ipBlock : cidr : 10.0.0.0/24 ports : - protocol : TCP port : 5978 Selectors Existen 4 tipos de Selectors que permiten definir hacia/desde qu\u00e9 conexiones se permite el tr\u00e1fico: podSelector : Permite seleccionar un conjunto de Pods con unas Labels concretas, que est\u00e9n en el mismo Namesapce que la NetworkPolicy . namespaceSelector : Permite seleccionar todos los Pods de un Namespace. namespaceSelector junto con podSelector : Permite seleccionar un conjunto de Pods con unas Labels concretas de un Namespace diferente. ipBlock : Permite seleccionar rangos IP. Se recomiendan IP's externas al cluster, ya que las IP de los Pods son ef\u00edmeras. Cuidado! Hay ocasiones en las que los mecanismos de enrutamiento de tr\u00e1fico que se han implementado en el cluster pueden reescribir las direcciones IP origen o destino de los paquetes. En estos casos hay que tener en cuenta qu\u00e9 direcciones IP hay que utilizar para filtrar las reglas. Ejemplos Ejemplo Esta NetworkPolicy aplica solo a las reglas Ingress de todos los Pods del Namespace. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-ingress spec : podSelector : {} policyTypes : - Ingress Ejemplo Esta NetworkPolicy aplica solo a las reglas Ingress de todos los Pods del Namespace. Aunque haya otras NetworkPolicy que limiten el tr\u00e1fico Ingress , esta habilita todo el tr\u00e1fico de este tipo. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-all-ingress spec : podSelector : {} ingress : - {} policyTypes : - Ingress Ejemplo Esta NetworkPolicy aplica solo a las reglas Egress de todos los Pods del Namespace. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-egress spec : podSelector : {} policyTypes : - Egress Ejemplo Esta NetworkPolicy aplica solo a las reglas Egress de todos los Pods del Namespace. Aunque haya otras NetworkPolicy que limiten el tr\u00e1fico Egress , esta habilita todo el tr\u00e1fico de este tipo. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-all-egress spec : podSelector : {} egress : - {} policyTypes : - Egress Ejemplo Esta NetworkPolicy deniega todo tipo de tr\u00e1fico de los Pods del Namespace. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-all spec : podSelector : {} policyTypes : - Ingress - Egress","title":"Network Policies"},{"location":"network/policies/#network-policies","text":"Las NetworkPolicy permiten definir un conjunto de reglas de red para determinar qu\u00e9 conexiones se pueden establecer entre distintos grupos de Pods u otros endpoints de la red. Estos recursos de Kubernetes utilizan labels para determinar sobre qu\u00e9 Pods aplicar las reglas de red que determinar\u00e1n qu\u00e9 tr\u00e1fico se permite en los Pods seleccionados. Cuidado Para poder utilizar los NetworkPolicy es necesario que el network plugin que est\u00e9 utilizando Kubernetes implemente dicho recurso. De lo contrario, las reglas no tendr\u00e1n efecto. Notas Los Pods por defecto permiten todo tipo de tr\u00e1fico. Si un Pod es seleccionado por una NetworkPolicy se le aplicar\u00e1n las reglas que haya definidas. Si no tiene reglas, denegar\u00e1 todo el tr\u00e1fico entrante/saliente. Un Pod puede ser seleccionado por m\u00e1s de una NetworkPolicy . En ese caso, se aplicar\u00e1 la uni\u00f3n de todas las reglas definidas (no se producen conflictos). Las NetworkPolicy pueden definir reglas Ingress , Egress o ambas. Los tipos de reglas que se van a definir se indican en .spec.policyTypes . Ejemplo Este ejemplo permite las siguientes conexiones para los Pods del Namespace default que tengan la Label role=db : Tr\u00e1fico entrante : Desde cualquier Pod con IP dentro del rango 172.17.0.0-172.17.0.255 y 172.17.2.0-172.17.255.255 hacia el puerto 6379 Desde cualquier Pod que est\u00e9 en un Namespace etiquetado con project=myproject hacia el puerto 6379 Desde cualquier Pod del Namespace default etiquetado con role=frontend hacia el puerto 6379 Tr\u00e1fico saliente : Hacia el puerto 5978 de cualquier Pod dentro del rango de IP's 10.0.0.0-10.0.0.255 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : test-network-policy namespace : default spec : podSelector : matchLabels : role : db policyTypes : - Ingress - Egress ingress : - from : - ipBlock : cidr : 172.17.0.0/16 except : - 172.17.1.0/24 - namespaceSelector : matchLabels : project : myproject - podSelector : matchLabels : role : frontend ports : - protocol : TCP port : 6379 egress : - to : - ipBlock : cidr : 10.0.0.0/24 ports : - protocol : TCP port : 5978","title":"Network Policies"},{"location":"network/policies/#selectors","text":"Existen 4 tipos de Selectors que permiten definir hacia/desde qu\u00e9 conexiones se permite el tr\u00e1fico: podSelector : Permite seleccionar un conjunto de Pods con unas Labels concretas, que est\u00e9n en el mismo Namesapce que la NetworkPolicy . namespaceSelector : Permite seleccionar todos los Pods de un Namespace. namespaceSelector junto con podSelector : Permite seleccionar un conjunto de Pods con unas Labels concretas de un Namespace diferente. ipBlock : Permite seleccionar rangos IP. Se recomiendan IP's externas al cluster, ya que las IP de los Pods son ef\u00edmeras. Cuidado! Hay ocasiones en las que los mecanismos de enrutamiento de tr\u00e1fico que se han implementado en el cluster pueden reescribir las direcciones IP origen o destino de los paquetes. En estos casos hay que tener en cuenta qu\u00e9 direcciones IP hay que utilizar para filtrar las reglas.","title":"Selectors"},{"location":"network/policies/#ejemplos","text":"Ejemplo Esta NetworkPolicy aplica solo a las reglas Ingress de todos los Pods del Namespace. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-ingress spec : podSelector : {} policyTypes : - Ingress Ejemplo Esta NetworkPolicy aplica solo a las reglas Ingress de todos los Pods del Namespace. Aunque haya otras NetworkPolicy que limiten el tr\u00e1fico Ingress , esta habilita todo el tr\u00e1fico de este tipo. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-all-ingress spec : podSelector : {} ingress : - {} policyTypes : - Ingress Ejemplo Esta NetworkPolicy aplica solo a las reglas Egress de todos los Pods del Namespace. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-egress spec : podSelector : {} policyTypes : - Egress Ejemplo Esta NetworkPolicy aplica solo a las reglas Egress de todos los Pods del Namespace. Aunque haya otras NetworkPolicy que limiten el tr\u00e1fico Egress , esta habilita todo el tr\u00e1fico de este tipo. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-all-egress spec : podSelector : {} egress : - {} policyTypes : - Egress Ejemplo Esta NetworkPolicy deniega todo tipo de tr\u00e1fico de los Pods del Namespace. apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-all spec : podSelector : {} policyTypes : - Ingress - Egress","title":"Ejemplos"},{"location":"network/service/","text":"Service Los Services permiten abstraer los conjuntos de Pods que ejecutan una aplicaci\u00f3n, para exponerlos como un \u00fanico servicio de red. Dada la naturaleza ef\u00edmera de los Pods, es complicado para los clientes y otros servicios, conectarse a ellos (las IP de los Pods no son siempre las mismas). Los Services solventan este problema, ofreciendo una \u00fanica direcci\u00f3n IP virtual y un registro DNS para acceder a un grupo de Pods. De esta forma, tambi\u00e9n ofrece la posibilidad de balancear la carga entre esos Pods y evita tener que acoplar un Service Discovery al c\u00f3digo fuente de las aplicaciones. Mediante un Selector, el Service identifica qu\u00e9 Pods debe exponer, pudiendo detectar cuando se levanta un nuevo Pod que deba ser expuesto. Cuando detecta un nuevo Pod, registra su IP en el objeto Endpoints del Service . Una vez el Service detecta un nuevo Pod con la misma Label que su Selector, Notas En la stanza ports se definen los puertos que va a exponer el Service . Se pueden indicar valores num\u00e9ricos o el nombre de los puertos definidos en el Pod. Esto \u00faltimo da flexibilidad al Pod para modificar por qu\u00e9 puerto expone un servicio. El campo port indica por qu\u00e9 puerto del Service se expone un servicio, mientras que el targetPort , es el puerto por el que la aplicaci\u00f3n se expone en el Pod. El nodePort es el puerto del nodo, en el que se expone un servicio, si el Service es de tipo NodePort . Si una aplicaci\u00f3n expone varios puertos en un Pod, se pueden configurar tambi\u00e9n en el Service , pero hay que darles nombre, para evitar ambig\u00fcedades. Se puede asignar manualmente la IP virtual con el campo .spec.clusterIP . Se se deja la stanza .spec.selector en blanco, se deber\u00e1 configurar el recurso Endpoints manualmente (p.ej. para redireccionar a servicios externos). Cualquier tipo de Service puede exponerse hacia fuera del cluster, si se a\u00f1ade el array .spec.externalIPs con alguna de las IPs externas del cluster (p.ej. la IP de uno de los nodos). Ejemplo Service: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 Endpoint con dos Pods: (Se genera autom\u00e1ticamente) apiVersion : v1 kind : Endpoints metadata : name : my-service subsets : - addresses : - ip : 192.0.2.42 - ip : 192.0.4.21 ports : - port : 9376 Tipos de Service Existen varias formas de exponer las aplicaciones en Kubernetes. Para ello hay varios tipos de Service que se indican con la stanza .spec.type : ClusterIP (Por defecto) NodePort LoadBalancer ExternalName Headless Service ClusterIP Este tipo de Service expone los Pods a trav\u00e9s de una IP interna, \u00fanicamente accesible desde dentro del cluster. NodePort Con este Service se exponen los Pods a trav\u00e9s de un puerto est\u00e1tico y de la IP de cada nodo del cluster. De esta forma los Pods son accesibles desde fuera del cluster con IP_nodo:NodePort . Notas Para configurar el puerto est\u00e1tico de los nodos en los que exponer el Service (en todos los nodos el mismo puerto), se debe indicar en el campo .spec.ports[*].nodePort . (Puertos v\u00e1lidos: 30000-32767) Estos Service son accesibles tanto desde fuera del cluster ( IP_nodo:NodePort ), como desde dentro ( .spec.clusterIP:spec.ports[*].port ). Ejemplo apiVersion : v1 kind : Service metadata : name : my-service spec : type : NodePort selector : app : MyApp ports : # By default and for convenience, the `targetPort` is set to the same value as the `port` field. - port : 80 targetPort : 80 # Optional field # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767) nodePort : 30007 LoadBalancer Este Service se puede utilizar en los proveedores Cloud para que aprovisione un Loadbalancer y as\u00ed exponer los Pods hacia el exterior a trav\u00e9s de una \u00fanica IP. En funci\u00f3n del proveedor Cloud, se pueden a\u00f1adir annotations adicionales, que configuren ciertas caracter\u00edsticas del Loadbalancer aprovisionado. Advertencia En este tipo de Service todos los puertos expuestos deben utilizar el mismo protocolo y solo puede ser TCP , UDP o SCTP . Ejemplo apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 clusterIP : 10.0.171.239 type : LoadBalancer status : loadBalancer : ingress : - ip : 192.0.2.127 ExternalName Este tipo de Service se utiliza para redireccionar a un nombre DNS de otro servicio. En el DNS en vez de generarse un registro A con la IP de un Pod, se genera un registro CNAME . Cuidado! Este tipo de Service puede dar problemas con las cabeceras HTTP que hagan referencia al \"hostname\" (p.ej. al comprobar un certificado SSL). Ejemplo apiVersion : v1 kind : Service metadata : name : my-service namespace : prod spec : type : ExternalName externalName : my.database.example.com Headless Service Estos Service se indican con .spec.type: None . Kubernetes no les asigna una ClusterIP, el kube-proxy no opera con ellos y no se realiza ning\u00fan tipo de balanceo de carga o proxy. El DNS se configura en funci\u00f3n de si se ha indicado Selector o no: Con Selector: Se genera el Endpoints con las IP de los Pods y se generan registros DNS para que apunten directamente a cada uno de los Pods. Sin Selector: No se genera el Endpoints . Informaci\u00f3n Cuando en alguno de los otros tipos de Service , no se especifica un Selector, el DNS genera los siguientes registros: Registro CNAME para los Service de tipo ExternalName . Registro A por cada Pod del Endpoints , si se encuentra un Endpoints con el mismo nombre que el Service . Este tipo de Service se puede utilizar como interfaz para otros mecanismo de Service Discovery. Tambi\u00e9n se suelen utilizar en los StatefulSet , por ejemplo para desplegar una base de datos con topolog\u00eda \"master-slave\". En este tipo de despliegues, las peticiones de escritura se suelen mandar \u00fanicamente a la instancia master . Con el resto de Services , no se podr\u00eda realizar esta operaci\u00f3n, porque las peticiones se balancear\u00edan entre todas las inst\u00e1ncias que expusiera el Service . Esta operaci\u00f3n tampoco se podr\u00eda realizar atacando directamente a la IP del Pod, ya que esta se genera de forma din\u00e1mica cada vez que se crea el Pod, ni tampoco su DNS, ya que en el dominio contiene la IP del Pod. Por ello se utilizan los Headless Service , para proporcionar un dominio independiente a cada uno de los Pods del estilo: <pod-name>.<headless-service-name>.<namespace>.svc.cluster.local Service Discovery Existen dos formas en las que se pueden obtener los datos de un Service : Variables de entorno Cuando se crea un Pod, el kubelet a\u00f1ade la IP y los puertos de los Services disponibles en ese momento, como variables de entorno dentro del Pod. Si se desea desactivar esta funcionalidad, se debe indicar en la definici\u00f3n del Pod con la propiedad .spec.enableServiceLinks a false . DNS El servidor DNS del cluster CoreDNS, consulta la API de Kubernetes en busca de nuevos Services y crea un nuevo registro DNS para cada uno de ellos, con la forma nombre_servicio.nombre_namespace y que resolver\u00e1 a la ClusterIP del Service . Este registro podr\u00e1 ser resuelto por todos los Pods del cluster. Los Pods del mismo Namespace, pueden resolver simplemente con nombre_servicio . CoreDNS tambi\u00e9n admite registros SRV, por lo que si un Service tiene varios puertos con un nombre asignado, se podr\u00eda obtener la IP y puerto del Service , con el registro _nombre_puerto._tcp.objects/pods/#variables-de-entorno . Service Topology Existe la posibilidad configurar la preferencia o limitar qu\u00e9 Endpoints reciben las peticiones de un Service , en funci\u00f3n del nodo en el que est\u00e9n desplegados. Mediante el campo .spec.topologyKeys , se puede indicar las Labels de los nodos con Endpoints que tienen preferencia. Esto es \u00fatil para limitar el tr\u00e1fico entre zonas, ya que la mayor\u00eda de proveedores Cloud cobran un extra por ese tr\u00e1fico, dando prioridad a los nodos m\u00e1s cercanos y evitando as\u00ed costes adicionales y reducir latencias. Notas Actualmente solo se permite utilizar las Labels \"kubernetes.io/hostname\" , \"topology.kubernetes.io/zone\" y topology.kubernetes.io/region\" . El valor \"*\" sirve para hacer un \"catch-all\" al final de las topologyKeys . Esta funcionalidad es incompatible con el uso de externalTrafficPolicy=Local . Ejemplos Limitar a Endpoints del nodo local: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : my-app ports : - protocol : TCP port : 80 targetPort : 9376 topologyKeys : - \"kubernetes.io/hostname\" Preferencia de nodos en el siguiente orden: nodo local, zona local, regi\u00f3n local: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : my-app ports : - protocol : TCP port : 80 targetPort : 9376 topologyKeys : - \"kubernetes.io/hostname\" - \"topology.kubernetes.io/zone\" - \"topology.kubernetes.io/region\" - \"*\"","title":"Service"},{"location":"network/service/#service","text":"Los Services permiten abstraer los conjuntos de Pods que ejecutan una aplicaci\u00f3n, para exponerlos como un \u00fanico servicio de red. Dada la naturaleza ef\u00edmera de los Pods, es complicado para los clientes y otros servicios, conectarse a ellos (las IP de los Pods no son siempre las mismas). Los Services solventan este problema, ofreciendo una \u00fanica direcci\u00f3n IP virtual y un registro DNS para acceder a un grupo de Pods. De esta forma, tambi\u00e9n ofrece la posibilidad de balancear la carga entre esos Pods y evita tener que acoplar un Service Discovery al c\u00f3digo fuente de las aplicaciones. Mediante un Selector, el Service identifica qu\u00e9 Pods debe exponer, pudiendo detectar cuando se levanta un nuevo Pod que deba ser expuesto. Cuando detecta un nuevo Pod, registra su IP en el objeto Endpoints del Service . Una vez el Service detecta un nuevo Pod con la misma Label que su Selector, Notas En la stanza ports se definen los puertos que va a exponer el Service . Se pueden indicar valores num\u00e9ricos o el nombre de los puertos definidos en el Pod. Esto \u00faltimo da flexibilidad al Pod para modificar por qu\u00e9 puerto expone un servicio. El campo port indica por qu\u00e9 puerto del Service se expone un servicio, mientras que el targetPort , es el puerto por el que la aplicaci\u00f3n se expone en el Pod. El nodePort es el puerto del nodo, en el que se expone un servicio, si el Service es de tipo NodePort . Si una aplicaci\u00f3n expone varios puertos en un Pod, se pueden configurar tambi\u00e9n en el Service , pero hay que darles nombre, para evitar ambig\u00fcedades. Se puede asignar manualmente la IP virtual con el campo .spec.clusterIP . Se se deja la stanza .spec.selector en blanco, se deber\u00e1 configurar el recurso Endpoints manualmente (p.ej. para redireccionar a servicios externos). Cualquier tipo de Service puede exponerse hacia fuera del cluster, si se a\u00f1ade el array .spec.externalIPs con alguna de las IPs externas del cluster (p.ej. la IP de uno de los nodos). Ejemplo Service: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 Endpoint con dos Pods: (Se genera autom\u00e1ticamente) apiVersion : v1 kind : Endpoints metadata : name : my-service subsets : - addresses : - ip : 192.0.2.42 - ip : 192.0.4.21 ports : - port : 9376","title":"Service"},{"location":"network/service/#tipos-de-service","text":"Existen varias formas de exponer las aplicaciones en Kubernetes. Para ello hay varios tipos de Service que se indican con la stanza .spec.type : ClusterIP (Por defecto) NodePort LoadBalancer ExternalName Headless Service","title":"Tipos de Service"},{"location":"network/service/#clusterip","text":"Este tipo de Service expone los Pods a trav\u00e9s de una IP interna, \u00fanicamente accesible desde dentro del cluster.","title":"ClusterIP"},{"location":"network/service/#nodeport","text":"Con este Service se exponen los Pods a trav\u00e9s de un puerto est\u00e1tico y de la IP de cada nodo del cluster. De esta forma los Pods son accesibles desde fuera del cluster con IP_nodo:NodePort . Notas Para configurar el puerto est\u00e1tico de los nodos en los que exponer el Service (en todos los nodos el mismo puerto), se debe indicar en el campo .spec.ports[*].nodePort . (Puertos v\u00e1lidos: 30000-32767) Estos Service son accesibles tanto desde fuera del cluster ( IP_nodo:NodePort ), como desde dentro ( .spec.clusterIP:spec.ports[*].port ). Ejemplo apiVersion : v1 kind : Service metadata : name : my-service spec : type : NodePort selector : app : MyApp ports : # By default and for convenience, the `targetPort` is set to the same value as the `port` field. - port : 80 targetPort : 80 # Optional field # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767) nodePort : 30007","title":"NodePort"},{"location":"network/service/#loadbalancer","text":"Este Service se puede utilizar en los proveedores Cloud para que aprovisione un Loadbalancer y as\u00ed exponer los Pods hacia el exterior a trav\u00e9s de una \u00fanica IP. En funci\u00f3n del proveedor Cloud, se pueden a\u00f1adir annotations adicionales, que configuren ciertas caracter\u00edsticas del Loadbalancer aprovisionado. Advertencia En este tipo de Service todos los puertos expuestos deben utilizar el mismo protocolo y solo puede ser TCP , UDP o SCTP . Ejemplo apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 clusterIP : 10.0.171.239 type : LoadBalancer status : loadBalancer : ingress : - ip : 192.0.2.127","title":"LoadBalancer"},{"location":"network/service/#externalname","text":"Este tipo de Service se utiliza para redireccionar a un nombre DNS de otro servicio. En el DNS en vez de generarse un registro A con la IP de un Pod, se genera un registro CNAME . Cuidado! Este tipo de Service puede dar problemas con las cabeceras HTTP que hagan referencia al \"hostname\" (p.ej. al comprobar un certificado SSL). Ejemplo apiVersion : v1 kind : Service metadata : name : my-service namespace : prod spec : type : ExternalName externalName : my.database.example.com","title":"ExternalName"},{"location":"network/service/#headless-service","text":"Estos Service se indican con .spec.type: None . Kubernetes no les asigna una ClusterIP, el kube-proxy no opera con ellos y no se realiza ning\u00fan tipo de balanceo de carga o proxy. El DNS se configura en funci\u00f3n de si se ha indicado Selector o no: Con Selector: Se genera el Endpoints con las IP de los Pods y se generan registros DNS para que apunten directamente a cada uno de los Pods. Sin Selector: No se genera el Endpoints . Informaci\u00f3n Cuando en alguno de los otros tipos de Service , no se especifica un Selector, el DNS genera los siguientes registros: Registro CNAME para los Service de tipo ExternalName . Registro A por cada Pod del Endpoints , si se encuentra un Endpoints con el mismo nombre que el Service . Este tipo de Service se puede utilizar como interfaz para otros mecanismo de Service Discovery. Tambi\u00e9n se suelen utilizar en los StatefulSet , por ejemplo para desplegar una base de datos con topolog\u00eda \"master-slave\". En este tipo de despliegues, las peticiones de escritura se suelen mandar \u00fanicamente a la instancia master . Con el resto de Services , no se podr\u00eda realizar esta operaci\u00f3n, porque las peticiones se balancear\u00edan entre todas las inst\u00e1ncias que expusiera el Service . Esta operaci\u00f3n tampoco se podr\u00eda realizar atacando directamente a la IP del Pod, ya que esta se genera de forma din\u00e1mica cada vez que se crea el Pod, ni tampoco su DNS, ya que en el dominio contiene la IP del Pod. Por ello se utilizan los Headless Service , para proporcionar un dominio independiente a cada uno de los Pods del estilo: <pod-name>.<headless-service-name>.<namespace>.svc.cluster.local","title":"Headless Service"},{"location":"network/service/#service-discovery","text":"Existen dos formas en las que se pueden obtener los datos de un Service :","title":"Service Discovery"},{"location":"network/service/#variables-de-entorno","text":"Cuando se crea un Pod, el kubelet a\u00f1ade la IP y los puertos de los Services disponibles en ese momento, como variables de entorno dentro del Pod. Si se desea desactivar esta funcionalidad, se debe indicar en la definici\u00f3n del Pod con la propiedad .spec.enableServiceLinks a false .","title":"Variables de entorno"},{"location":"network/service/#dns","text":"El servidor DNS del cluster CoreDNS, consulta la API de Kubernetes en busca de nuevos Services y crea un nuevo registro DNS para cada uno de ellos, con la forma nombre_servicio.nombre_namespace y que resolver\u00e1 a la ClusterIP del Service . Este registro podr\u00e1 ser resuelto por todos los Pods del cluster. Los Pods del mismo Namespace, pueden resolver simplemente con nombre_servicio . CoreDNS tambi\u00e9n admite registros SRV, por lo que si un Service tiene varios puertos con un nombre asignado, se podr\u00eda obtener la IP y puerto del Service , con el registro _nombre_puerto._tcp.objects/pods/#variables-de-entorno .","title":"DNS"},{"location":"network/service/#service-topology","text":"Existe la posibilidad configurar la preferencia o limitar qu\u00e9 Endpoints reciben las peticiones de un Service , en funci\u00f3n del nodo en el que est\u00e9n desplegados. Mediante el campo .spec.topologyKeys , se puede indicar las Labels de los nodos con Endpoints que tienen preferencia. Esto es \u00fatil para limitar el tr\u00e1fico entre zonas, ya que la mayor\u00eda de proveedores Cloud cobran un extra por ese tr\u00e1fico, dando prioridad a los nodos m\u00e1s cercanos y evitando as\u00ed costes adicionales y reducir latencias. Notas Actualmente solo se permite utilizar las Labels \"kubernetes.io/hostname\" , \"topology.kubernetes.io/zone\" y topology.kubernetes.io/region\" . El valor \"*\" sirve para hacer un \"catch-all\" al final de las topologyKeys . Esta funcionalidad es incompatible con el uso de externalTrafficPolicy=Local . Ejemplos Limitar a Endpoints del nodo local: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : my-app ports : - protocol : TCP port : 80 targetPort : 9376 topologyKeys : - \"kubernetes.io/hostname\" Preferencia de nodos en el siguiente orden: nodo local, zona local, regi\u00f3n local: apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app : my-app ports : - protocol : TCP port : 80 targetPort : 9376 topologyKeys : - \"kubernetes.io/hostname\" - \"topology.kubernetes.io/zone\" - \"topology.kubernetes.io/region\" - \"*\"","title":"Service Topology"},{"location":"objects/containers/","text":"Containers Esta tecnolog\u00eda permite empaquetar una aplicaci\u00f3n junto con todas sus dependencias Runtime, para desacoplar las aplicaciones de la infraestructura del host en el que se ejecutan y asegurar que la aplicaci\u00f3n se va a ejecutar de la misma forma en cualquier entorno. Nomenclatura Images Una Image es el binario que contiene la aplicaci\u00f3n y sus dependencias y permite levantar un Container. En la configuraci\u00f3n del Container se puede indicar la Image de distintas formas: nginx : Si s\u00f3lo se especifica un nombre se supone que la Image se va a buscar al Docker Registry oficial y se descargar\u00e1 la Image nginx/nginx:latest (siguiendo la nomenclatura user/image_name:tag ). bitnami/kubectl:1.18.5 : Igual que caso anterior. my-private.registry.com:8080/my-image:1.0.3 : En este caso la Image se va a buscar a un Registry privado. Runtime Class Este recurso permite configurar el Container Runtime que deber\u00e1 utilizar ciertos Pods (Docker, CRI-O...). Esto permite que ciertos workloads se ejecuten con un Container Runtime diferente al resto para aprovechar funcionalidades espec\u00edficas de esos Runtime. Para ello, los nodos d\u00f3nde se vaya a ejecutar ese workload deber\u00e1n tener aprovisionado el Container Runtime necesario. Por defecto el cluster supone que todos los nodos est\u00e1n configurados de forma homog\u00e9nea, por lo que si no todos tienen configurado el mismo Container Runtime, habr\u00e1 que modificar el Scheduler para que este tipo de Pods solo se desplieguen en los nodos indicados. Ejemplo de uso Recurso RuntimeClass: apiVersion : node.k8s.io/v1beta1 # RuntimeClass is defined in the node.k8s.io API group kind : RuntimeClass metadata : name : myclass # The name the RuntimeClass will be referenced by # RuntimeClass is a non-namespaced resource handler : myconfiguration # The name of the corresponding CRI configuration C\u00f3mo referenciar el RuntimeClass en un Pod: apiVersion : v1 kind : Pod metadata : name : mypod spec : runtimeClassName : myclass # ... Container Lifecycle Hooks Kubernetes proporciona una forma de ejecutar c\u00f3digo en los contenedores, a partir de eventos de su ciclo de vida. Hay dos hooks expuestos a los Containers: PostStart : Este hook se lanza cuando un Container es creado. No se garantiza que el hook se ejecuta antes que el ENTRYPOINT del Container. PreStop : Se ejecuta antes de que un Container se destruya, debido a una llamada a la API, un evento de fallo del Liveness Probe... Se ejecuta de forma s\u00edncrona, por lo que hasta que no se termine de ejecutar, no se llevar\u00e1 a cabo la destrucci\u00f3n del Container. Existen dos tipos de hook handler: Exec: Ejecuta un comando dentro del Container. HTTP: Lanza una petici\u00f3n HTTP contra un endpoint del Container. Pese a que el handler del hook PostStart y el ENTRYPOINT del Container se ejecuten de forma as\u00edncrona, el contenedor no pasar\u00e1 a running hasta que el handler termine de ejecutarse. Si el handler del PostStart o del PreStop fallan, el Container se destruye. Los handlers est\u00e1n pensados para ser ejecutados \"al menos una vez\", por lo que hay que controlar la posibilidad de que se ejecuten m\u00faltiples veces. (Rara vez se ejecutan m\u00e1s de una vez)","title":"Containers"},{"location":"objects/containers/#containers","text":"Esta tecnolog\u00eda permite empaquetar una aplicaci\u00f3n junto con todas sus dependencias Runtime, para desacoplar las aplicaciones de la infraestructura del host en el que se ejecutan y asegurar que la aplicaci\u00f3n se va a ejecutar de la misma forma en cualquier entorno.","title":"Containers"},{"location":"objects/containers/#nomenclatura-images","text":"Una Image es el binario que contiene la aplicaci\u00f3n y sus dependencias y permite levantar un Container. En la configuraci\u00f3n del Container se puede indicar la Image de distintas formas: nginx : Si s\u00f3lo se especifica un nombre se supone que la Image se va a buscar al Docker Registry oficial y se descargar\u00e1 la Image nginx/nginx:latest (siguiendo la nomenclatura user/image_name:tag ). bitnami/kubectl:1.18.5 : Igual que caso anterior. my-private.registry.com:8080/my-image:1.0.3 : En este caso la Image se va a buscar a un Registry privado.","title":"Nomenclatura Images"},{"location":"objects/containers/#runtime-class","text":"Este recurso permite configurar el Container Runtime que deber\u00e1 utilizar ciertos Pods (Docker, CRI-O...). Esto permite que ciertos workloads se ejecuten con un Container Runtime diferente al resto para aprovechar funcionalidades espec\u00edficas de esos Runtime. Para ello, los nodos d\u00f3nde se vaya a ejecutar ese workload deber\u00e1n tener aprovisionado el Container Runtime necesario. Por defecto el cluster supone que todos los nodos est\u00e1n configurados de forma homog\u00e9nea, por lo que si no todos tienen configurado el mismo Container Runtime, habr\u00e1 que modificar el Scheduler para que este tipo de Pods solo se desplieguen en los nodos indicados. Ejemplo de uso Recurso RuntimeClass: apiVersion : node.k8s.io/v1beta1 # RuntimeClass is defined in the node.k8s.io API group kind : RuntimeClass metadata : name : myclass # The name the RuntimeClass will be referenced by # RuntimeClass is a non-namespaced resource handler : myconfiguration # The name of the corresponding CRI configuration C\u00f3mo referenciar el RuntimeClass en un Pod: apiVersion : v1 kind : Pod metadata : name : mypod spec : runtimeClassName : myclass # ...","title":"Runtime Class"},{"location":"objects/containers/#container-lifecycle-hooks","text":"Kubernetes proporciona una forma de ejecutar c\u00f3digo en los contenedores, a partir de eventos de su ciclo de vida. Hay dos hooks expuestos a los Containers: PostStart : Este hook se lanza cuando un Container es creado. No se garantiza que el hook se ejecuta antes que el ENTRYPOINT del Container. PreStop : Se ejecuta antes de que un Container se destruya, debido a una llamada a la API, un evento de fallo del Liveness Probe... Se ejecuta de forma s\u00edncrona, por lo que hasta que no se termine de ejecutar, no se llevar\u00e1 a cabo la destrucci\u00f3n del Container. Existen dos tipos de hook handler: Exec: Ejecuta un comando dentro del Container. HTTP: Lanza una petici\u00f3n HTTP contra un endpoint del Container. Pese a que el handler del hook PostStart y el ENTRYPOINT del Container se ejecuten de forma as\u00edncrona, el contenedor no pasar\u00e1 a running hasta que el handler termine de ejecutarse. Si el handler del PostStart o del PreStop fallan, el Container se destruye. Los handlers est\u00e1n pensados para ser ejecutados \"al menos una vez\", por lo que hay que controlar la posibilidad de que se ejecuten m\u00faltiples veces. (Rara vez se ejecutan m\u00e1s de una vez)","title":"Container Lifecycle Hooks"},{"location":"objects/controllers/","text":"Controllers Kubernetes proporciona un conjunto de abstracciones a alto nivel, que permiten gestionar el despliegue y ejecuci\u00f3n de los Pods. Estos controladores permiten mantener el estado deseado de los Pods incluso ante reinicios o ca\u00eddas de servicio, aumentando el n\u00famero de replicas hasta llegar al n\u00famero deseado. Replica Set Este Controller es el encargado de mantener el n\u00famero deseado de replicas de los Pods. En su definici\u00f3n se encuentra la plantilla del Pod que se encargar\u00e1 de crear. Para identificar qu\u00e9 Pods debe gestionar, a\u00f1adir\u00e1 un Label en los Pods que cree y tendr\u00e1 configurado eun Selector para seleccionarlos. Advertencia No se trabaja directamente con Replica Set . Para gestionar el despliegue de Pods, se utilizar\u00e1n Deployments que a su vez crear\u00e1n un Replica Set . Cuidado! Las Labels que utilice el Replica Set para identificar a sus Pods, no pueden solaparse con Labels de otros Pods. De lo contrario, el Replica Set interpretar\u00e1 que debe gestionar esos Pods y pudiera llegar a eliminarlos. Deployment Este Controller se encarga de gestionar el despliegue de Pods. Contiene el n\u00famero de replicas y una plantilla con la configuraci\u00f3n del Pod deseado. Para gestionar el n\u00famero de replicas de un Pod, el Deployment crea un Replica Set . Cada vez que se modifique la definici\u00f3n del Pod, se crear\u00e1 un nuevo Replica Set y el anterior se actualizar\u00e1 para tener 0 replicas. Este mecanismo permite mantener Replica Set con configuraciones de Pod anteriores y as\u00ed realizar rollbacks. Para que el Deployment identifique que Replica Set y Pod debe gestionar, etiqueta con una Label todos los recursos que genera (Replica Set y Pods) para luego filtrar con un Selector. Advertencia Los Deployments solo admiten spec.template.spec.restartPolicy con valor Always Ejemplo apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 Estrategia y estado del despliegue Estrategia de despliegue Cada vez que se realiza un cambio en la definici\u00f3n del Pod, se dispara una redespliegue de los Pods. Esto se hace de forma controlada en funci\u00f3n de la estrategia de despliegue configurada. Esto se indica con la stanza .spec.strategy.type . Tipos de estrategia: Recreate : todos los Pods antiguos son destru\u00eddos antes de empezar a desplegar los nuevos. RollingUpdate : (Por defecto) mediante las propiedades .spec.strategy.rollingUpdate.maxUnavailable y .spec.strategy.rollingUpdate.maxSurge del Deployment, controla el ritmo en el que se crean los nuevos Pods y se destruyen los antiguos, levantando los nuevos a un ritmo de maxSurge y eliminando los viejos a un ritmo que permita mantener el maxUnavailable . maxSurge y maxUnavailable Ambas propiedades son opcionales y pueden indicarse con valores absolutos (p.ej. 3) o con valores porcentuales (p.ej. 25%) Estado del despliegue Para consultar el estado del despliegue: kubectl rollout status deployment.v1.apps/nginx-deployment Si se desean realizar varios cambios de forma program\u00e1tica y no se desea redesplegar por cada uno de los cambios: # Se pausan los despliegues kubectl rollout pause deployment.v1.apps/nginx-deployment # Se aplican los cambios kubectl set image deployment.v1.apps/nginx-deployment nginx = nginx:1.16.1 kubectl set resources deployment.v1.apps/nginx-deployment -c = nginx --limits = cpu = 200m,memory = 512Mi # Se ranudan los despliegues kubectl rollout resume deployment.v1.apps/nginx-deployment Rollback Para consultar los despliegues realizados previos al actual, se puede consultar con el siguiente comando de ejemplo (por defecto mantiene los \u00faltimos 10): kubectl rollout history deployment.v1.apps/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl apply --filename = https://k8s.io/examples/controllers/nginx-deployment.yaml --record = true 2 kubectl set image deployment.v1.apps/nginx-deployment nginx = nginx:1.16.1 --record = true 3 kubectl set image deployment.v1.apps/nginx-deployment nginx = nginx:1.161 --record = true CHANGE_CAUSE solo aparecer\u00e1 si en el momento de realizar el despliegue se especific\u00f3 la opci\u00f3n --record Para hacer un rollback a una versi\u00f3n anterior: kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision = 2 --to-revision es para redesplegar una versi\u00f3n espec\u00edfica. Si no se indica, se hace un rollback de la versi\u00f3n anterior a la actual Escalado El n\u00famero de replicas de un Pod se puede modificar de forma manual o de forma autom\u00e1tica. Manual kubectl scale deployment.v1.apps/nginx-deployment --replicas = 10 Autom\u00e1tica kubectl autoscale deployment.v1.apps/nginx-deployment --min = 10 --max = 15 --cpu-percent = 80 Este comando crea un Horizontal Pod Autoscaler StatefulSet Similares a los Deployment , este tipo de objeto permite crear Pods de forma ordenada y con entidad propia. Esto por ejemplo significa que el primer Pod que se crea siempre va a tener las mismas caracter\u00edsticas (p.ej. aunque se recree el Pod, se utilizar\u00e1 el mismo Volume que el anterior). El nombre de cada Pod va a ser siempre el mismo y estar\u00e1 formado por <statefulset_name>-N donde N es un n\u00famero cardinal que identifica la instancia del Pod (empezando desde 0). La definici\u00f3n de un StatefulSet es pr\u00e1cticamente la misma que un Deployment , solo que hay que a\u00f1adir la propiedad spec.serviceName con el nombre del Headless Service . Nota Hasta que la instancia anterior no est\u00e9 \"Running and Ready\", no se empezar\u00e1 a desplegar la siguiente instancia. Esto se puede desactivar con el campo .spec.podManagementPolicy que por defecto tiene valor OrderedReady . Para desactivarlo hay que indicar el valor Parallel . Para qu\u00e9 sirve? Los StatefulSets son \u00fatiles para las aplicaciones que: Requieran identificadores de red \u00fanicos y estables. Necesiten almacenamiento estable para la persistencia de datos. Requieran despliegue y escalado ordenado de las instancias. No implementen la posibilidad de ser desplegadas de forma distribuida (p.ej. base de datos SQL) Limitaciones Eliminar o escalar a menos el StatefulSet , no elimina los Volumes asociados a \u00e9ste. As\u00ed se garantiza la seguridad de los datos. Para poder conectarse con cada instancia particular, se debe crear un Headless Service adem\u00e1s del Service . Al utilizar la estrategia de despliegue por defecto ( OrderedReady ), puede que el despliegue quede en un estado roto si sucede alg\u00fan error. En ese caso hay que realizar un rollback y eliminar manualmente los nuevos Pods del despliegue fallido. Los Persistent Volume Claim (PVC) deben generarse de forma din\u00e1mica con la stanza spec.volumeClaimTemplates , de lo contrario, si se especifica un \u00fanico PVC , todas las instancias montar\u00e1n exactamente el mismo PVC . Ejemplo apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx # has to match .spec.template.metadata.labels serviceName : \"nginx\" replicas : 3 # by default is 1 template : metadata : labels : app : nginx # has to match .spec.selector.matchLabels spec : terminationGracePeriodSeconds : 10 containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"my-storage-class\" resources : requests : storage : 1Gi Adicionalmente debe crearse el Headless Service : apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None # Headless selector : app : nginx Estrategia de despliegue Al igual que en los Deployments , se puede configurar la forma en los que los StatefulSet realizar\u00e1n sus despliegues. En este caso mediante la stanza .spec.updateStrategy.type Los tipos de estrategia son: OnDelete : Esta estrategia desactiva la actualizaci\u00f3n autom\u00e1tica de los Pods. Para actualizarlos, hay que eliminar los Pods manualmente uno a uno, una vez se ha actualizado su definici\u00f3n en el StatefulSet . RollingUpdate : (Por defecto) Actualizar\u00e1 autom\u00e1ticamente los Pods, empezando por el que tenga el \u00edndice m\u00e1s alto, hasta al m\u00e1s bajo. Con esta estrategia se puede limitar cu\u00e1ntos Pods son actualizados (p.ej. para hacer un Canary Deployment). Para ello se utiliza la stanza .spec.updateStrategy.rollingUpdate.partition . Esta stanza tiene valor num\u00e9rico e indica a partir de qu\u00e9 instancia deben realizarse las actualizaciones. De esta forma las instancias que queden por debajo de ese valor no ser\u00e1n actualizadas (incluso si se recrean esos Pods). DaemonSet Este Controller es similar a los Deployments , pero su objetivo es intentar desplegar una copia del Pod en cada uno de los nodos del cluster. Si se elimina el DaemonSet , se eliminar\u00e1n todos los Pods que haya desplegado en los nodos. Para qu\u00e9 se suele utilizar? Desplegar un collector de logs en cada nodo Desplegar un collector de m\u00e9tricas para monitorizar cada uno de los nodos Nota Se puede limitar sobre qu\u00e9 nodos se desea desplegar mediante las propiedades .spec.template.spec.nodeSelector o .spec.template.spec.affinity Advertencia La stanza .spec.template.spec.restartPolicy solo admite valor Always Ejemplo apiVersion : apps/v1 kind : DaemonSet metadata : name : fluentd-elasticsearch namespace : kube-system labels : k8s-app : fluentd-logging spec : selector : matchLabels : name : fluentd-elasticsearch template : metadata : labels : name : fluentd-elasticsearch spec : tolerations : # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can't run pods - key : node-role.kubernetes.io/master effect : NoSchedule containers : - name : fluentd-elasticsearch image : quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources : limits : memory : 200Mi requests : cpu : 100m memory : 200Mi volumeMounts : - name : varlog mountPath : /var/log - name : varlibdockercontainers mountPath : /var/lib/docker/containers readOnly : true terminationGracePeriodSeconds : 30 volumes : - name : varlog hostPath : path : /var/log - name : varlibdockercontainers hostPath : path : /var/lib/docker/containers Estrategia de despliegue Al igual que los StatefulSets admite dos estrategias de despliegue: OnDelete y RollingUpdate . Job En ocasiones es necesario realizar una operaci\u00f3n finita dentro del cluster. Esto quiere decir, que se debe levantarse un Pod para realizar una operaci\u00f3n y cuando termine, \u00e9ste deber\u00e1 detenerse. Si el proceso del Pod debe ejecutarse varias veces (p.ej. procesar mensajes de una cola), se puede configurar la stanza spec.completions para indicar el n\u00famero de veces que debe de ejecutarse correctamente el Pod para dar por finalizado el Job (con cada ejecuci\u00f3n se crea un Pod nuevo). Si se especifica un spec.completions mayor a 1, se pueden paralelizar las ejecuciones con el campo spec.parallelism con el n\u00famero de Pods que se admiten ejecutar en paralelo. Para qu\u00e9 sirve? Una Job es \u00fatil para ejecutar procesos Batch. Existen varias formas de utilizar un Job : Jobs sin paralelismo: Solo levanta un Pod (a no ser que falle y el Pod se reinicie hasta completar satisfact\u00f3riamente). El Job se da por completado cuando el Pod termina satisfact\u00f3riamente. Ejemplo: Proceso de backup Jobs con paralelismo y un n\u00famero fijo de completados : Se van levantando Pods, hasta que se completen satisfact\u00f3riamente tantos Pods como .spec.completions . Se pueden levantar varios Pods a la vez, si se indica en .spec.parallelism . El n\u00famero m\u00e1ximo de Pods que pueden ejecutarse a la vez, es el valor m\u00e1s bajo entre .spec.parallelism y el valor que reste para llegar a .spec.completions . Ejemplo: Procesar n\u00famero finito de elementos de una cola Jobs con paralelismo y una cola de trabajo : Se levantan tantos Pods como .spec.parallelism . Cada Pod se encargar\u00e1 de ir obteniendo elementos de una cola de trabajo y cuando \u00e9sta se quede vac\u00eda, el Pod terminar\u00e1 satisfact\u00f3riamente. Una vez todos los Pods se han completado correctamente, el Job se d\u00e1 por completado. Ejemplo: Procesar todos los elementos de una cola Notas S\u00f3lo admite .spec.restartPolicy con valor Never o OnFailure (de lo contrario las ejecuciones no ser\u00edan finitas) Por defecto .spec.completions y .spec.parallelism tienen valor 1 Si no se especifica la stanza .spec.completions , se toma el valor que haya definido para .spec.parallelism Con spec.backoffLimit se indica el n\u00famero de reintentos que tiene una Job antes de darse por fallida Si se desea indicar el tiempo m\u00e1ximo que debe estar en ejecuci\u00f3n una Job (sin tener en cuenta los Pods), se puede indicar con .spec.activeDeadlineSeconds . Pasado ese tiempo la Job terminar\u00e1 como \"fallada\" junto a sus Pods, sin importar qu\u00e9 se estuviera ejecutando. Por defecto las Jobs completadas ( Completed o Failed ) y sus Pods, se mantienen en el sistema para poder revisar sus logs y eventos. Para limpiar autom\u00e1ticamente estas Jobs se puede indicar cu\u00e1nto tiempo pueden vivir las Jobs completadas con la stanza .spec.ttlSecondsAfterFinished Advertencia El campo restartPolicy \u00fanicamente aplica para los Pod, por lo que si la Job termina debido a que se ha excedido spec.backoffLimit o .spec.activeDeadlineSeconds , no habr\u00e1 ning\u00fan reintento m\u00e1s Ejemplo apiVersion : batch/v1 kind : Job metadata : name : job-wq-1 spec : completions : 8 parallelism : 2 template : metadata : name : job-wq-1 spec : containers : - name : c image : gcr.io/<project>/job-wq-1 env : - name : BROKER_URL value : amqp://guest:guest@rabbitmq-service:5672 - name : QUEUE value : job1 restartPolicy : OnFailure CronJob Las CronJob permiten ejecutar Jobs de forma peri\u00f3dica. La periodicidad se indica en sintaxis Cron (< minuto > < hora > < d\u00eda del mes > < mes > < d\u00eda de la semana >) con la stanza .spec.schedule . Advertencia Las CronJob no aseguran el n\u00famero de veces que puede ejecutarse una Job , por lo que sus procesos deber\u00e1n ser idempotentes Notas Un CronJob se considera fallido y detiene su ejecuci\u00f3n, cuando se ha llegado a omitir m\u00e1s de 100 la ejecuci\u00f3n de la Job , desde la \u00faltima ejecuci\u00f3n correcta. A cada horario de ejecuci\u00f3n, se le puede dar un margen para que el CronJob reintente la ejecuci\u00f3n del Job , con la stanza .spec.startingDeadlineSeconds . Si esta stanza se utiliza, el contador de reintentos de 100 pasa a contar las omisiones desde ahora - startingDeadlineSeconds . Se puede configurar la concurrencia que admiten las Cronjob , mediante la stanza `.spec.concurrencyPolicy: Allow : (Por defecto) Permite que un mismo CronJob pueda tener varias Jobs ejecut\u00e1ndose a la vez. Forbid : No se permiten ejecuciones simult\u00e1neas de Jobs . Si llega la hora de ejecutarse una nueva Job y la anterior a\u00fan esta ejecut\u00e1ndose, se omite la nueva. Replace : Si llega la hora de ejecutarse una nueva Job y la anterior a\u00fan esta ejecut\u00e1ndose, se ejecuta la nueva Job reemplazando la antigua. Para limpiar autom\u00e1ticamente las Jobs completadas, se puede indicar cu\u00e1ntas Jobs completadas se desea retener, usando los campos .spec.successfulJobsHistoryLimit (para Jobs completadas correctamente) y .spec.failedJobsHistoryLimit (para Jobs que hayan fallado). Por defecto, esos campos tienen valor 3 y 1 . Ejemplo apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure Garbage Collector Todos los recursos generados por un Deployment , ReplicaSet , Job , etc. llevan incrustado la stanza .metadata.ownerReferences que contiene informaci\u00f3n sobre a qu\u00e9 otro recurso pertenece (el recurso que lo haya generado). De esta manera, si se elimina el recurso \"padre\", los recursos que cuelgan de \u00e9l, se eliminar\u00e1n en cascada. Si por ejemplo solo se desea destruir el recurso \"padre\" y dejar el resto de objetos \"hu\u00e9rfanos\" se puede indicar con kubectl de la siguiente manera: kubectl delete replicaset my-repset --cascade = false API Si en vez de utilizar kubectl se desea eliminar un recurso con la API de Kubernetes, existen 3 formas. Borrado en cascada Foreground Cuando el kube-apiserver recibe la petici\u00f3n de borrado, empieza eliminando todos los recursos \"hijo\" que tengan la propiedad .metadata.ownerReference.blockOwnerDeletion=true . Una vez est\u00e1n todos eliminados, procede con el recurso \"padre\". kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\" Borrado en cascada Background Cuando el kube-apiserver recibe la petici\u00f3n de borrado, elimina directamente el recurso \"padre\" y a continuaci\u00f3n sus recursos \"hijo\". kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Background\"}' \\ -H \"Content-Type: application/json\" Borrado dejando recursos hu\u00e9rfanos S\u00f3lo se elimina el recurso \"padre\" y los recursos \"hijo\" se quedan hu\u00e9rfanos. kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' \\ -H \"Content-Type: application/json\"","title":"Controllers"},{"location":"objects/controllers/#controllers","text":"Kubernetes proporciona un conjunto de abstracciones a alto nivel, que permiten gestionar el despliegue y ejecuci\u00f3n de los Pods. Estos controladores permiten mantener el estado deseado de los Pods incluso ante reinicios o ca\u00eddas de servicio, aumentando el n\u00famero de replicas hasta llegar al n\u00famero deseado.","title":"Controllers"},{"location":"objects/controllers/#replica-set","text":"Este Controller es el encargado de mantener el n\u00famero deseado de replicas de los Pods. En su definici\u00f3n se encuentra la plantilla del Pod que se encargar\u00e1 de crear. Para identificar qu\u00e9 Pods debe gestionar, a\u00f1adir\u00e1 un Label en los Pods que cree y tendr\u00e1 configurado eun Selector para seleccionarlos. Advertencia No se trabaja directamente con Replica Set . Para gestionar el despliegue de Pods, se utilizar\u00e1n Deployments que a su vez crear\u00e1n un Replica Set . Cuidado! Las Labels que utilice el Replica Set para identificar a sus Pods, no pueden solaparse con Labels de otros Pods. De lo contrario, el Replica Set interpretar\u00e1 que debe gestionar esos Pods y pudiera llegar a eliminarlos.","title":"Replica Set"},{"location":"objects/controllers/#deployment","text":"Este Controller se encarga de gestionar el despliegue de Pods. Contiene el n\u00famero de replicas y una plantilla con la configuraci\u00f3n del Pod deseado. Para gestionar el n\u00famero de replicas de un Pod, el Deployment crea un Replica Set . Cada vez que se modifique la definici\u00f3n del Pod, se crear\u00e1 un nuevo Replica Set y el anterior se actualizar\u00e1 para tener 0 replicas. Este mecanismo permite mantener Replica Set con configuraciones de Pod anteriores y as\u00ed realizar rollbacks. Para que el Deployment identifique que Replica Set y Pod debe gestionar, etiqueta con una Label todos los recursos que genera (Replica Set y Pods) para luego filtrar con un Selector. Advertencia Los Deployments solo admiten spec.template.spec.restartPolicy con valor Always Ejemplo apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80","title":"Deployment"},{"location":"objects/controllers/#estrategia-y-estado-del-despliegue","text":"","title":"Estrategia y estado del despliegue"},{"location":"objects/controllers/#estrategia-de-despliegue","text":"Cada vez que se realiza un cambio en la definici\u00f3n del Pod, se dispara una redespliegue de los Pods. Esto se hace de forma controlada en funci\u00f3n de la estrategia de despliegue configurada. Esto se indica con la stanza .spec.strategy.type . Tipos de estrategia: Recreate : todos los Pods antiguos son destru\u00eddos antes de empezar a desplegar los nuevos. RollingUpdate : (Por defecto) mediante las propiedades .spec.strategy.rollingUpdate.maxUnavailable y .spec.strategy.rollingUpdate.maxSurge del Deployment, controla el ritmo en el que se crean los nuevos Pods y se destruyen los antiguos, levantando los nuevos a un ritmo de maxSurge y eliminando los viejos a un ritmo que permita mantener el maxUnavailable . maxSurge y maxUnavailable Ambas propiedades son opcionales y pueden indicarse con valores absolutos (p.ej. 3) o con valores porcentuales (p.ej. 25%)","title":"Estrategia de despliegue"},{"location":"objects/controllers/#estado-del-despliegue","text":"Para consultar el estado del despliegue: kubectl rollout status deployment.v1.apps/nginx-deployment Si se desean realizar varios cambios de forma program\u00e1tica y no se desea redesplegar por cada uno de los cambios: # Se pausan los despliegues kubectl rollout pause deployment.v1.apps/nginx-deployment # Se aplican los cambios kubectl set image deployment.v1.apps/nginx-deployment nginx = nginx:1.16.1 kubectl set resources deployment.v1.apps/nginx-deployment -c = nginx --limits = cpu = 200m,memory = 512Mi # Se ranudan los despliegues kubectl rollout resume deployment.v1.apps/nginx-deployment","title":"Estado del despliegue"},{"location":"objects/controllers/#rollback","text":"Para consultar los despliegues realizados previos al actual, se puede consultar con el siguiente comando de ejemplo (por defecto mantiene los \u00faltimos 10): kubectl rollout history deployment.v1.apps/nginx-deployment deployments \"nginx-deployment\" REVISION CHANGE-CAUSE 1 kubectl apply --filename = https://k8s.io/examples/controllers/nginx-deployment.yaml --record = true 2 kubectl set image deployment.v1.apps/nginx-deployment nginx = nginx:1.16.1 --record = true 3 kubectl set image deployment.v1.apps/nginx-deployment nginx = nginx:1.161 --record = true CHANGE_CAUSE solo aparecer\u00e1 si en el momento de realizar el despliegue se especific\u00f3 la opci\u00f3n --record Para hacer un rollback a una versi\u00f3n anterior: kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision = 2 --to-revision es para redesplegar una versi\u00f3n espec\u00edfica. Si no se indica, se hace un rollback de la versi\u00f3n anterior a la actual","title":"Rollback"},{"location":"objects/controllers/#escalado","text":"El n\u00famero de replicas de un Pod se puede modificar de forma manual o de forma autom\u00e1tica.","title":"Escalado"},{"location":"objects/controllers/#manual","text":"kubectl scale deployment.v1.apps/nginx-deployment --replicas = 10","title":"Manual"},{"location":"objects/controllers/#automatica","text":"kubectl autoscale deployment.v1.apps/nginx-deployment --min = 10 --max = 15 --cpu-percent = 80 Este comando crea un Horizontal Pod Autoscaler","title":"Autom\u00e1tica"},{"location":"objects/controllers/#statefulset","text":"Similares a los Deployment , este tipo de objeto permite crear Pods de forma ordenada y con entidad propia. Esto por ejemplo significa que el primer Pod que se crea siempre va a tener las mismas caracter\u00edsticas (p.ej. aunque se recree el Pod, se utilizar\u00e1 el mismo Volume que el anterior). El nombre de cada Pod va a ser siempre el mismo y estar\u00e1 formado por <statefulset_name>-N donde N es un n\u00famero cardinal que identifica la instancia del Pod (empezando desde 0). La definici\u00f3n de un StatefulSet es pr\u00e1cticamente la misma que un Deployment , solo que hay que a\u00f1adir la propiedad spec.serviceName con el nombre del Headless Service . Nota Hasta que la instancia anterior no est\u00e9 \"Running and Ready\", no se empezar\u00e1 a desplegar la siguiente instancia. Esto se puede desactivar con el campo .spec.podManagementPolicy que por defecto tiene valor OrderedReady . Para desactivarlo hay que indicar el valor Parallel . Para qu\u00e9 sirve? Los StatefulSets son \u00fatiles para las aplicaciones que: Requieran identificadores de red \u00fanicos y estables. Necesiten almacenamiento estable para la persistencia de datos. Requieran despliegue y escalado ordenado de las instancias. No implementen la posibilidad de ser desplegadas de forma distribuida (p.ej. base de datos SQL) Limitaciones Eliminar o escalar a menos el StatefulSet , no elimina los Volumes asociados a \u00e9ste. As\u00ed se garantiza la seguridad de los datos. Para poder conectarse con cada instancia particular, se debe crear un Headless Service adem\u00e1s del Service . Al utilizar la estrategia de despliegue por defecto ( OrderedReady ), puede que el despliegue quede en un estado roto si sucede alg\u00fan error. En ese caso hay que realizar un rollback y eliminar manualmente los nuevos Pods del despliegue fallido. Los Persistent Volume Claim (PVC) deben generarse de forma din\u00e1mica con la stanza spec.volumeClaimTemplates , de lo contrario, si se especifica un \u00fanico PVC , todas las instancias montar\u00e1n exactamente el mismo PVC . Ejemplo apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx # has to match .spec.template.metadata.labels serviceName : \"nginx\" replicas : 3 # by default is 1 template : metadata : labels : app : nginx # has to match .spec.selector.matchLabels spec : terminationGracePeriodSeconds : 10 containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"my-storage-class\" resources : requests : storage : 1Gi Adicionalmente debe crearse el Headless Service : apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None # Headless selector : app : nginx","title":"StatefulSet"},{"location":"objects/controllers/#estrategia-de-despliegue_1","text":"Al igual que en los Deployments , se puede configurar la forma en los que los StatefulSet realizar\u00e1n sus despliegues. En este caso mediante la stanza .spec.updateStrategy.type Los tipos de estrategia son: OnDelete : Esta estrategia desactiva la actualizaci\u00f3n autom\u00e1tica de los Pods. Para actualizarlos, hay que eliminar los Pods manualmente uno a uno, una vez se ha actualizado su definici\u00f3n en el StatefulSet . RollingUpdate : (Por defecto) Actualizar\u00e1 autom\u00e1ticamente los Pods, empezando por el que tenga el \u00edndice m\u00e1s alto, hasta al m\u00e1s bajo. Con esta estrategia se puede limitar cu\u00e1ntos Pods son actualizados (p.ej. para hacer un Canary Deployment). Para ello se utiliza la stanza .spec.updateStrategy.rollingUpdate.partition . Esta stanza tiene valor num\u00e9rico e indica a partir de qu\u00e9 instancia deben realizarse las actualizaciones. De esta forma las instancias que queden por debajo de ese valor no ser\u00e1n actualizadas (incluso si se recrean esos Pods).","title":"Estrategia de despliegue"},{"location":"objects/controllers/#daemonset","text":"Este Controller es similar a los Deployments , pero su objetivo es intentar desplegar una copia del Pod en cada uno de los nodos del cluster. Si se elimina el DaemonSet , se eliminar\u00e1n todos los Pods que haya desplegado en los nodos. Para qu\u00e9 se suele utilizar? Desplegar un collector de logs en cada nodo Desplegar un collector de m\u00e9tricas para monitorizar cada uno de los nodos Nota Se puede limitar sobre qu\u00e9 nodos se desea desplegar mediante las propiedades .spec.template.spec.nodeSelector o .spec.template.spec.affinity Advertencia La stanza .spec.template.spec.restartPolicy solo admite valor Always Ejemplo apiVersion : apps/v1 kind : DaemonSet metadata : name : fluentd-elasticsearch namespace : kube-system labels : k8s-app : fluentd-logging spec : selector : matchLabels : name : fluentd-elasticsearch template : metadata : labels : name : fluentd-elasticsearch spec : tolerations : # this toleration is to have the daemonset runnable on master nodes # remove it if your masters can't run pods - key : node-role.kubernetes.io/master effect : NoSchedule containers : - name : fluentd-elasticsearch image : quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources : limits : memory : 200Mi requests : cpu : 100m memory : 200Mi volumeMounts : - name : varlog mountPath : /var/log - name : varlibdockercontainers mountPath : /var/lib/docker/containers readOnly : true terminationGracePeriodSeconds : 30 volumes : - name : varlog hostPath : path : /var/log - name : varlibdockercontainers hostPath : path : /var/lib/docker/containers","title":"DaemonSet"},{"location":"objects/controllers/#estrategia-de-despliegue_2","text":"Al igual que los StatefulSets admite dos estrategias de despliegue: OnDelete y RollingUpdate .","title":"Estrategia de despliegue"},{"location":"objects/controllers/#job","text":"En ocasiones es necesario realizar una operaci\u00f3n finita dentro del cluster. Esto quiere decir, que se debe levantarse un Pod para realizar una operaci\u00f3n y cuando termine, \u00e9ste deber\u00e1 detenerse. Si el proceso del Pod debe ejecutarse varias veces (p.ej. procesar mensajes de una cola), se puede configurar la stanza spec.completions para indicar el n\u00famero de veces que debe de ejecutarse correctamente el Pod para dar por finalizado el Job (con cada ejecuci\u00f3n se crea un Pod nuevo). Si se especifica un spec.completions mayor a 1, se pueden paralelizar las ejecuciones con el campo spec.parallelism con el n\u00famero de Pods que se admiten ejecutar en paralelo. Para qu\u00e9 sirve? Una Job es \u00fatil para ejecutar procesos Batch. Existen varias formas de utilizar un Job : Jobs sin paralelismo: Solo levanta un Pod (a no ser que falle y el Pod se reinicie hasta completar satisfact\u00f3riamente). El Job se da por completado cuando el Pod termina satisfact\u00f3riamente. Ejemplo: Proceso de backup Jobs con paralelismo y un n\u00famero fijo de completados : Se van levantando Pods, hasta que se completen satisfact\u00f3riamente tantos Pods como .spec.completions . Se pueden levantar varios Pods a la vez, si se indica en .spec.parallelism . El n\u00famero m\u00e1ximo de Pods que pueden ejecutarse a la vez, es el valor m\u00e1s bajo entre .spec.parallelism y el valor que reste para llegar a .spec.completions . Ejemplo: Procesar n\u00famero finito de elementos de una cola Jobs con paralelismo y una cola de trabajo : Se levantan tantos Pods como .spec.parallelism . Cada Pod se encargar\u00e1 de ir obteniendo elementos de una cola de trabajo y cuando \u00e9sta se quede vac\u00eda, el Pod terminar\u00e1 satisfact\u00f3riamente. Una vez todos los Pods se han completado correctamente, el Job se d\u00e1 por completado. Ejemplo: Procesar todos los elementos de una cola Notas S\u00f3lo admite .spec.restartPolicy con valor Never o OnFailure (de lo contrario las ejecuciones no ser\u00edan finitas) Por defecto .spec.completions y .spec.parallelism tienen valor 1 Si no se especifica la stanza .spec.completions , se toma el valor que haya definido para .spec.parallelism Con spec.backoffLimit se indica el n\u00famero de reintentos que tiene una Job antes de darse por fallida Si se desea indicar el tiempo m\u00e1ximo que debe estar en ejecuci\u00f3n una Job (sin tener en cuenta los Pods), se puede indicar con .spec.activeDeadlineSeconds . Pasado ese tiempo la Job terminar\u00e1 como \"fallada\" junto a sus Pods, sin importar qu\u00e9 se estuviera ejecutando. Por defecto las Jobs completadas ( Completed o Failed ) y sus Pods, se mantienen en el sistema para poder revisar sus logs y eventos. Para limpiar autom\u00e1ticamente estas Jobs se puede indicar cu\u00e1nto tiempo pueden vivir las Jobs completadas con la stanza .spec.ttlSecondsAfterFinished Advertencia El campo restartPolicy \u00fanicamente aplica para los Pod, por lo que si la Job termina debido a que se ha excedido spec.backoffLimit o .spec.activeDeadlineSeconds , no habr\u00e1 ning\u00fan reintento m\u00e1s Ejemplo apiVersion : batch/v1 kind : Job metadata : name : job-wq-1 spec : completions : 8 parallelism : 2 template : metadata : name : job-wq-1 spec : containers : - name : c image : gcr.io/<project>/job-wq-1 env : - name : BROKER_URL value : amqp://guest:guest@rabbitmq-service:5672 - name : QUEUE value : job1 restartPolicy : OnFailure","title":"Job"},{"location":"objects/controllers/#cronjob","text":"Las CronJob permiten ejecutar Jobs de forma peri\u00f3dica. La periodicidad se indica en sintaxis Cron (< minuto > < hora > < d\u00eda del mes > < mes > < d\u00eda de la semana >) con la stanza .spec.schedule . Advertencia Las CronJob no aseguran el n\u00famero de veces que puede ejecutarse una Job , por lo que sus procesos deber\u00e1n ser idempotentes Notas Un CronJob se considera fallido y detiene su ejecuci\u00f3n, cuando se ha llegado a omitir m\u00e1s de 100 la ejecuci\u00f3n de la Job , desde la \u00faltima ejecuci\u00f3n correcta. A cada horario de ejecuci\u00f3n, se le puede dar un margen para que el CronJob reintente la ejecuci\u00f3n del Job , con la stanza .spec.startingDeadlineSeconds . Si esta stanza se utiliza, el contador de reintentos de 100 pasa a contar las omisiones desde ahora - startingDeadlineSeconds . Se puede configurar la concurrencia que admiten las Cronjob , mediante la stanza `.spec.concurrencyPolicy: Allow : (Por defecto) Permite que un mismo CronJob pueda tener varias Jobs ejecut\u00e1ndose a la vez. Forbid : No se permiten ejecuciones simult\u00e1neas de Jobs . Si llega la hora de ejecutarse una nueva Job y la anterior a\u00fan esta ejecut\u00e1ndose, se omite la nueva. Replace : Si llega la hora de ejecutarse una nueva Job y la anterior a\u00fan esta ejecut\u00e1ndose, se ejecuta la nueva Job reemplazando la antigua. Para limpiar autom\u00e1ticamente las Jobs completadas, se puede indicar cu\u00e1ntas Jobs completadas se desea retener, usando los campos .spec.successfulJobsHistoryLimit (para Jobs completadas correctamente) y .spec.failedJobsHistoryLimit (para Jobs que hayan fallado). Por defecto, esos campos tienen valor 3 y 1 . Ejemplo apiVersion : batch/v1beta1 kind : CronJob metadata : name : hello spec : schedule : \"*/1 * * * *\" jobTemplate : spec : template : spec : containers : - name : hello image : busybox args : - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy : OnFailure","title":"CronJob"},{"location":"objects/controllers/#garbage-collector","text":"Todos los recursos generados por un Deployment , ReplicaSet , Job , etc. llevan incrustado la stanza .metadata.ownerReferences que contiene informaci\u00f3n sobre a qu\u00e9 otro recurso pertenece (el recurso que lo haya generado). De esta manera, si se elimina el recurso \"padre\", los recursos que cuelgan de \u00e9l, se eliminar\u00e1n en cascada. Si por ejemplo solo se desea destruir el recurso \"padre\" y dejar el resto de objetos \"hu\u00e9rfanos\" se puede indicar con kubectl de la siguiente manera: kubectl delete replicaset my-repset --cascade = false","title":"Garbage Collector"},{"location":"objects/controllers/#api","text":"Si en vez de utilizar kubectl se desea eliminar un recurso con la API de Kubernetes, existen 3 formas.","title":"API"},{"location":"objects/controllers/#borrado-en-cascada-foreground","text":"Cuando el kube-apiserver recibe la petici\u00f3n de borrado, empieza eliminando todos los recursos \"hijo\" que tengan la propiedad .metadata.ownerReference.blockOwnerDeletion=true . Una vez est\u00e1n todos eliminados, procede con el recurso \"padre\". kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\ -H \"Content-Type: application/json\"","title":"Borrado en cascada Foreground"},{"location":"objects/controllers/#borrado-en-cascada-background","text":"Cuando el kube-apiserver recibe la petici\u00f3n de borrado, elimina directamente el recurso \"padre\" y a continuaci\u00f3n sus recursos \"hijo\". kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Background\"}' \\ -H \"Content-Type: application/json\"","title":"Borrado en cascada Background"},{"location":"objects/controllers/#borrado-dejando-recursos-huerfanos","text":"S\u00f3lo se elimina el recurso \"padre\" y los recursos \"hijo\" se quedan hu\u00e9rfanos. kubectl proxy --port = 8080 curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \\ -d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' \\ -H \"Content-Type: application/json\"","title":"Borrado dejando recursos hu\u00e9rfanos"},{"location":"objects/objects/","text":"Gesti\u00f3n de objetos Kubernetes Para poder operar con los objetos del cluster de Kubernetes, se dispone de un conjunto de funcionalidades que permiten identificar a cada uno de los objetos, asi como organizarlos en agrupaciones diferentes. Estas funcionalidades son las siguientes: Names UID Namespace Labels & Selectors Annotations Field Selector Names Cada objeto del cluster debe tener un nombre por el cual pueda identificarse. De esta manera se pueden identificar diferentes objetos de un mismo tipo. Los nombres deben ser \u00fanicos por tipo de objeto. Es decir, en un mismo Namespace s\u00f3lo puede existir un \u00fanico Pod con nombre my-pod , pero a su vez si podr\u00eda existir un Deployment llamado my-pod . Dado que estos nombres son utilizados frecuentemente para ser registrados en los servidores DNS, deben ser nombres DNS v\u00e1lidos. UID Este identificador es \u00fanico por cada objeto generado en el cluster, permitiendo identificarlos del resto, incluso de las diferentes instancias de un mismo objeto que haya habido en el tiempo. Namespace Los Namespaces permiten aislar entornos de trabajo para diferentes equipos o workloads. Los Namespaces no puedes ser anidados entre si. La mayor\u00eda de workloads deben pertenecer a un Namespace, pero hay algunos objetos de Kubernetes que no, como por ejemplo los Nodes o los PersistentVolumes. Kubernetes trae por defecto 4 Namespaces: default: Namespace por defecto d\u00f3nde se generan los objetos Kubernetes a los que no se les indique Namespace. kube-node-lease: Namespace d\u00f3nde se despliegan los objeto Lease (objeto que mejora el rendimiento de los \"heartbeats\" de los nodos de Kubernetes cuando el cluster escala). kube-public: Este Namespace es accesible por todos los usuarios (incluso an\u00f3nimos). Se puede utilizar para desplegar elementos que deban ser visibles de forma p\u00fablica a trav\u00e9s de todo el cluster. kube-system: Namespace reservado para albergar los elementos de sistema del cluster Kubernetes. Labels & Selectors Las Labels son un conjunto de parejas clave-valor que permiten asignar a objetos Kubernetes, atributos relevantes para los usuarios, permitiendo que luego se puedan seleccionar subconjuntos de objetos en base a esos atributos. Para seleccionar subconjuntos de objetos filtrando por sus Labels, se utilizan los Selectors. Cuando se especifican m\u00faltiples Labels en un Selector, solo se devolver\u00e1n los objetos que contengan todas esas Labels. Ejemplo con kubeclt En los comandos kubectl se pueden indicar Selector de m\u00faltiples formas: # Equality-based requirement kubeclt get pods -l 'env=prod,tier=frontend' # Set-based requirement kubectl get pods -l 'env in (production, qa),tier in (frontend)' Labels recomendadas Este es un conjunto de Labels que se recomienda a\u00f1adir a las aplicaciones que se desplieguen (las que apliquen), para facilitar posteriormente su identificaci\u00f3n: Clave Descripci\u00f3n Ejemplo app.kubernetes.io/name Nombre de la aplicaci\u00f3n mongodb app.kubernetes.io/instance Nombre \u00fanico de la instancia mongo-db-xdsf app.kubernetes.io/version Versi\u00f3n de la aplicaci\u00f3n 3.2.0 app.kubernetes.io/component Componente dentro de la arquitectura database app.kubernetes.io/part-of Nombre de la aplicaci\u00f3n que forma parte (si es un componente) graylog app.kubernetes.io/managed-by Herramienta utilizada para gestionar la aplicaci\u00f3n helm Annotations Metadatos que se a\u00f1aden a los objetos para que sean utilizados por otras herramientas o librer\u00edas. Son parejas clave-valor al igual que las Labels, pero estas no son utilizadas para identificar y seleccionar objetos. Field Selector Estos Selector permiten filtrar objetos Kubernetes por el valor de uno de sus campos. Ejemplo con kubeclt # Ejemplo 1 kubeclt get deployments,services -A --field-selector metadata.namespace! = default # Ejemplo 2 kubectl get pods --field-selector = status.phase! = Running,spec.restartPolicy = Always","title":"Gesti\u00f3n de objetos Kubernetes"},{"location":"objects/objects/#gestion-de-objetos-kubernetes","text":"Para poder operar con los objetos del cluster de Kubernetes, se dispone de un conjunto de funcionalidades que permiten identificar a cada uno de los objetos, asi como organizarlos en agrupaciones diferentes. Estas funcionalidades son las siguientes: Names UID Namespace Labels & Selectors Annotations Field Selector","title":"Gesti\u00f3n de objetos Kubernetes"},{"location":"objects/objects/#names","text":"Cada objeto del cluster debe tener un nombre por el cual pueda identificarse. De esta manera se pueden identificar diferentes objetos de un mismo tipo. Los nombres deben ser \u00fanicos por tipo de objeto. Es decir, en un mismo Namespace s\u00f3lo puede existir un \u00fanico Pod con nombre my-pod , pero a su vez si podr\u00eda existir un Deployment llamado my-pod . Dado que estos nombres son utilizados frecuentemente para ser registrados en los servidores DNS, deben ser nombres DNS v\u00e1lidos.","title":"Names"},{"location":"objects/objects/#uid","text":"Este identificador es \u00fanico por cada objeto generado en el cluster, permitiendo identificarlos del resto, incluso de las diferentes instancias de un mismo objeto que haya habido en el tiempo.","title":"UID"},{"location":"objects/objects/#namespace","text":"Los Namespaces permiten aislar entornos de trabajo para diferentes equipos o workloads. Los Namespaces no puedes ser anidados entre si. La mayor\u00eda de workloads deben pertenecer a un Namespace, pero hay algunos objetos de Kubernetes que no, como por ejemplo los Nodes o los PersistentVolumes. Kubernetes trae por defecto 4 Namespaces: default: Namespace por defecto d\u00f3nde se generan los objetos Kubernetes a los que no se les indique Namespace. kube-node-lease: Namespace d\u00f3nde se despliegan los objeto Lease (objeto que mejora el rendimiento de los \"heartbeats\" de los nodos de Kubernetes cuando el cluster escala). kube-public: Este Namespace es accesible por todos los usuarios (incluso an\u00f3nimos). Se puede utilizar para desplegar elementos que deban ser visibles de forma p\u00fablica a trav\u00e9s de todo el cluster. kube-system: Namespace reservado para albergar los elementos de sistema del cluster Kubernetes.","title":"Namespace"},{"location":"objects/objects/#labels-selectors","text":"Las Labels son un conjunto de parejas clave-valor que permiten asignar a objetos Kubernetes, atributos relevantes para los usuarios, permitiendo que luego se puedan seleccionar subconjuntos de objetos en base a esos atributos. Para seleccionar subconjuntos de objetos filtrando por sus Labels, se utilizan los Selectors. Cuando se especifican m\u00faltiples Labels en un Selector, solo se devolver\u00e1n los objetos que contengan todas esas Labels. Ejemplo con kubeclt En los comandos kubectl se pueden indicar Selector de m\u00faltiples formas: # Equality-based requirement kubeclt get pods -l 'env=prod,tier=frontend' # Set-based requirement kubectl get pods -l 'env in (production, qa),tier in (frontend)'","title":"Labels &amp; Selectors"},{"location":"objects/objects/#labels-recomendadas","text":"Este es un conjunto de Labels que se recomienda a\u00f1adir a las aplicaciones que se desplieguen (las que apliquen), para facilitar posteriormente su identificaci\u00f3n: Clave Descripci\u00f3n Ejemplo app.kubernetes.io/name Nombre de la aplicaci\u00f3n mongodb app.kubernetes.io/instance Nombre \u00fanico de la instancia mongo-db-xdsf app.kubernetes.io/version Versi\u00f3n de la aplicaci\u00f3n 3.2.0 app.kubernetes.io/component Componente dentro de la arquitectura database app.kubernetes.io/part-of Nombre de la aplicaci\u00f3n que forma parte (si es un componente) graylog app.kubernetes.io/managed-by Herramienta utilizada para gestionar la aplicaci\u00f3n helm","title":"Labels recomendadas"},{"location":"objects/objects/#annotations","text":"Metadatos que se a\u00f1aden a los objetos para que sean utilizados por otras herramientas o librer\u00edas. Son parejas clave-valor al igual que las Labels, pero estas no son utilizadas para identificar y seleccionar objetos.","title":"Annotations"},{"location":"objects/objects/#field-selector","text":"Estos Selector permiten filtrar objetos Kubernetes por el valor de uno de sus campos. Ejemplo con kubeclt # Ejemplo 1 kubeclt get deployments,services -A --field-selector metadata.namespace! = default # Ejemplo 2 kubectl get pods --field-selector = status.phase! = Running,spec.restartPolicy = Always","title":"Field Selector"},{"location":"objects/pods/","text":"Pods La unidad b\u00e1sica de despliegue en Kubernetes son los Pods . Los Pods normalmente contienen un \u00fanico Container, pero existen situaciones en las que se requiere desplegar un Pod con m\u00faltiples Containers, por ejemplo para externalizar los logs del Container aun concentrador de logs. Todos los contenedores de un mismo Pod son levantados en un mismo nodo y comparten recursos, se pueden comunicar entre ellos de forma local y se pueden coordinar en la forma en que se destruyen. Los contenedores dentro de un Pod pueden ser de dos tipos: Init containers: Se ejecutan antes que los app containers y suelen ser procesos finitos. App containers: Se empiezan a ejecutar despu\u00e9s de que hayan terminado los init containers y contienen la aplicaci\u00f3n principal del Pod. Networking Cada Pod recibe una direcci\u00f3n IP \u00fanica. Esta IP es compartida por todos los Containers del Pod. Esto implica que cada contenedor deber\u00e1 exponerse (si deben exponerse varios contenedores) con un puerto diferente. Los Containers de un mismo Pod, pueden comunicarse entre si mediante localhost . Storage Los Containers de un mismo Pod, pueden compartir datos mediante Volumes. Los Volumes, depende de c\u00f3mo se configuren, permiten persistir esos datos y asegurar que seguir\u00e1n estando disponibles ante un reinicio del Pod. Pod phase Dentro del campo status de un Pod (campo creado en tiempo de ejecuci\u00f3n), existe el campo phase que identifica en que fase est\u00e1 un Pod, dentro de su ciclo de vida. Estos son los valores posibles de phase : Valor Descripci\u00f3n Pending El Pod ha sido aceptado por Kubernetes, pero falta por descargar alguna Container Image. Esta fase, incluye el proceso de scheduling del Pod y el de descarga de las im\u00e1genes de los Containers en el nodo. Si no encuentra un nodo disponible, se queda en este estado a la espera Running El Pod ya ha sido desplegado en un nodo y todos los Containers han sido creados. Esta fase, incluye el proceso de inicio o reinicio de un Container, as\u00ed como cuando existe al menos un Container en ejecuci\u00f3n Succeeded Todos los Containers del Pod han terminado correctamente y no van a ser reiniciados Failed Todos los Containers del Pod han terminado, pero alguno de ellos con error. Esto puede ser porque un Container haya terminado con un c\u00f3digo de retorno diferente de zero, o que el sistema haya terminado el proceso Unknown El estado del Pod no se puede obtener (debido a un problema de comunicaci\u00f3n con el nodo o el Pod) Pod Conditions En el campo status tambi\u00e9n existen un conjunto de condiciones que pueden ayudar en las tareas de troubleshooting. Puede verse el estado de estas condiciones con kubectl describe pod . Listado de condiciones: PodScheduled : True cuando el Pod ha sido registrado en un nodo. Initialized : True cuando todos los init containers del Pod se han iniciado correctamente. ContainersReady : True cuando todos los Container del Pod est\u00e1n listos. Ready : True cuando el Pod esta disponible para recibir peticiones y ya puede ser a\u00f1adido en los balanceadores de carga de los Services. kubectl describe pod # ... Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True # ... Container Probes Los kubelet realizan peri\u00f3dicamente sondeos sobre los Containers para evaluar su correcto funcionamiento. Para ello ejecuta un Handler que expone el mismo Container. Existen 3 tipos de Handlers: ExecAction: Ejecuta un comando dentro del Container y se considera satisfactorio si devuelve un c\u00f3digo de estado 0 . readinessProbe : exec : command : - cat - /app/is_ready TCPSocketAction: Realiza un comprobaci\u00f3n TCP contra la direcci\u00f3n IP del Container, a un puerto espec\u00edfico. El resultado es satisfactorio si el puerto est\u00e1 abierto. readinessProbe : tcpSocket : port : 3306 HTTPGetAction: Realiza una petici\u00f3n HTTP GET contra la IP del Container, a un puerto y ruta espec\u00edficas. El resultado es satisfactorio si el c\u00f3digo de respuesta est\u00e1 entre 200 (incluido) y 400 (excluido). readinessProbe : httpGet : path : /api/ready port : 8080 Los resultados posibles son: Success: resultado satisfactorio. Failure: el Container no ha pasado el diagn\u00f3stico. Unknown: el diagn\u00f3stico ha fallado. Existen 3 tipos de sondeos para los que el kubelet puede tomar distintas acciones: Liveness Probe: Indica que el Container se est\u00e1 ejecutando correctamente. Si esta sonda falla, el kubelet destruye el Container y puede que sea reiniciado en funci\u00f3n de su \"pol\u00edtica de reinicio\". Si no se especifica esta sonda, se considera que el Container se ejecuta correctamente si est\u00e1 en estado Running . \u00bfCu\u00e1ndo se recomienda utilizar Liveness Probe? Cuando el proceso que se ejecuta en el Container no sea capaz de terminar debido a un error o un mal funcionamiento, ya que por defecto el kubelet considerar\u00e1 que el Container estar\u00e1 healthy , sin importar si la aplicaci\u00f3n tiene errores en su c\u00f3digo o configuraci\u00f3n Readiness Probe: Indica que el Container est\u00e1 listo para recibir peticiones. Si esta sonda falla, el Endpoints Controller, elimina la IP del Pod de todos los Endpoints en los que est\u00e9 registrado. Si no se especifica esta sonda, se concede un delay inicial, en el que se considera como si la sonda hubiera fallado. Pasado ese periodo inicial, la sonda pasa a tener resultado satisfactorio y la aplicaci\u00f3n empieza a recibir tr\u00e1fico. \u00bfCu\u00e1ndo se recomienda utilizar Readiness Probe? Si debido a la l\u00f3gica de negocio de la aplicaci\u00f3n, cuando se levanta el contenedor, a\u00fan no est\u00e1 preparada para recibir peticiones, esta sonda permite indicar cu\u00e1ndo el Container est\u00e1 preparado para empezar a recibir tr\u00e1fico (p.ej. comprobando el endpoint /health , si la aplicaci\u00f3n lo provee). Muy \u00fatil para los Containers que realicen procesos al inicio despu\u00e9s de levantarse (p.ej. cargar datos, preparar entorno...). Tambi\u00e9n es muy \u00fatil cuando se despliegan m\u00faltiples instancias, ya que hasta que estas no est\u00e9n listas, no empezar\u00e1n a recibir tr\u00e1fico. Startup Probe: Indica cu\u00e1ndo la aplicaci\u00f3n del Container se ha iniciado. Cuando se especifica esta sonda, no se empezar\u00e1n a evaluar el resto de sondas, hasta que esta d\u00e9 un resultado favorable. Si esta sonda falla, el kubelet destruye el Container y puede que sea reiniciado en funci\u00f3n de su \"pol\u00edtica de reinicio\". Si no se especifica esta sonda, se considera por defecto que el resultado es satisfactorio. \u00bfCu\u00e1ndo se recomienda utilizar Startup Probe? Cuando la aplicaci\u00f3n tarda mucho en iniciar la primera vez, esta sonda permite evaluar la misma condici\u00f3n que la Liveness Probe , pero con un periodo de gracia superior. De esta forma el periodo de gracia del Liveness Probe , no se ve afectado por la carga inicial de la aplicaci\u00f3n. Ejemplo Ejemplo de Pod con Readiness Probe : apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : my-app image : my-image ports : - containerPort : 8080 readinessProbe : httpGet : path : /api/ready port : 8080 initDelaySeconds : 10 ## Delay inicial periodSeconds : 5 ## Cada cuando evaluar la sonda failureThreshold : 8 ## N\u00famero m\u00e1ximo de reintentos fallidos Pod & Container Status El estado del Pod depende los estados de los Containers que contiene. Container Status Una vez un Pod es registrado en un nodo, el kubelet empieza a crear los Contenedores con el Container Runtime especificado. Los posibles estados del Conatiner son: Waiting : Estado por defecto, cuando no est\u00e1 en Running o Terminated . Durante este estado, se descargan las im\u00e1genes, se montan los Volumes... Al ejecutar kubectl describe pod <pod_name> , este estado suele ir acompa\u00f1ado de un mensaje que aporta m\u00e1s informaci\u00f3n del estado: #... State: Waiting Reason: ErrImagePull #... Running : Indica que el Container se est\u00e1 ejecutando sin problemas. Indica tambi\u00e9n que todos los postStart hooks se han ejecutado correctamente. Este estado va acompa\u00f1ado de la fecha en la que entr\u00f3 en este estado: #... State: Running Started: Wed, 30 Jan 2019 16 :46:38 +0530 #... Terminated : Indica que el Container ha completado su ejecuci\u00f3n y se ha detenido. Puede que el Container haya completado su ejecuci\u00f3n de forma satisfactoria o con error. Antes de pasar a este estado se van a ejecutar todos los preStop hooks. Este estado va acompa\u00f1ado de la raz\u00f3n por la que se ha completado la ejecuci\u00f3n, el c\u00f3digo de retorno, el tiempo de inicio del Container y el de su finalizaci\u00f3n: #... State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 30 Jan 2019 11 :45:26 +0530 Finished: Wed, 30 Jan 2019 11 :45:26 +0530 #... A\u00f1adir condiciones al Pod Status (Pod Readiness) Existe la posibilidad de a\u00f1adir condiciones adicionales, para evaluar el estado del Pod. Para ello se utiliza la propiedad readinessGates , d\u00f3nde se indica el nombre de qu\u00e9 condiciones adicionales se esperan recibir en el campo status.conditions y que deben tener status: \"True\" . Las condiciones deben ser a\u00f1adidas en el campo status.conditions mediante una acci\u00f3n PATCH contra la API de Kubernetes. Esto debe ser llevado a cabo por una aplicaci\u00f3n o opeator, que utilice una librer\u00eda cliente de Kubernetes . El Pod no pasar\u00e1 a estar listo, hasta que todos los Containers del Pod est\u00e9n listos y todas las condiciones indicadas en el readinessGates eval\u00faen a True . Si todos los Containers est\u00e1n disponibles, pero alguna de las condiciones adicionales no, el Pod solo tendr\u00e1 la condici\u00f3n ContainersReady a True . kind: Pod #... spec: readinessGates: - conditionType: \"www.example.com/feature-1\" status: conditions: - type: Ready # a built in PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018 -01-01T00:00:00Z - type: \"www.example.com/feature-1\" # an extra PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018 -01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true #... Restart Policy Dentro de la definici\u00f3n de un Pod se puede especificar la pol\u00edtica de reinicio que van a tener todos los Containers que contiene, pudiendo tomar los valores Always (por defecto), OnFailure y Never . Cada vez que el kubelet reinicia un Container, se produce un back-off delay que aumenta de forma exponencial (10s, 20s, 40s...) hasta un m\u00e1ximo de 5 minutos. Una vez el Container lleva 10 minutos funcionando correctamente sin reinicios, el contador del delay vuelve a 0. Init Containers Estos contenedores son \u00fatiles para ejecutar utilidades o scripts de configuraci\u00f3n que no est\u00e9n presentes en la Image de la aplicaci\u00f3n principal. Este listado de contenedores se ejecuta de forma secuencial en el orden en el que se hayan definido en el Pod (hasta que no termine de ejecutarse el primero, no empezar\u00e1 a ejecutarse el segundo). No empezar\u00e1n a ejecutarse los app containers hasta que el \u00faltimo init container haya completado su ejecuci\u00f3n. Al igual que los app containers , el estado de estos contenedores puede consultarse en el campo status.initContainerStatuses ( status.containerStatuses para los app containers ) del Pod. Dado que la ejecuci\u00f3n de los init containers puede darse varias veces (debido a un reinicio, a un reintento de ejecuci\u00f3n...), el proceso a ejecutar deber\u00eda ser idempotente. Los init containers no soportan los lifecycle hooks, livenessProbe , readinessProbe o startupProbe . Un cambio en la Image de un init container , reiniciar\u00e1 todos los Containers, pero un cambio en la Image de un app container , solo reiniciar\u00e1 ese mismo Container. Utilidades de los Init Containers Las Image de estos contenedores pueden contener utilidades que vayan a ser necesarias para la configuraci\u00f3n de la aplicaci\u00f3n y con ello simplificar el contenido de las Images de las aplicaciones principales. Esto ahorra tener que generar una nueva Image que a parte de la aplicaci\u00f3n principal contenga herramientas \"innecesarias\" y a\u00f1ade m\u00e1s seguridad a las Image de los app containers reduciendo los posibles vectores de ataque. Ofrece un mecanismo para bloquear la ejecuci\u00f3n de los app containers de un Pod, hasta que no se cumpla cierta condici\u00f3n. De esta forma se puede hacer que ciertos Pods \"dependan\" de otros. Pueden utilizar datos sensibles para sus ejecuciones (p.ej. mediante Secrets), sin tener que expon\u00e9rselos a los app containers . Ejemplo init container Este Pod depende de que dos Service ( myservice y mydb ) est\u00e9n disponibles, para poder arrancar: apiVersion : v1 kind : Pod metadata : name : myapp-pod labels : app : myapp spec : containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox:1.28 command : [ 'sh' , '-c' , \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\" ] - name : init-mydb image : busybox:1.28 command : [ 'sh' , '-c' , \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\" ] Pod Preset Este es un objeto Kubernetes que permite modificar la especificaci\u00f3n de un Pod en tiempo de creaci\u00f3n, permitiendo a\u00f1adir elementos tales como variables de entorno, Volumes o VolumeMounts (aplica tanto a los contenedores de aplicaciones como a los initContainer ). De esta manera el desarrollador del manifest del Pod no debe de conocer esas configuraciones. Los PodPreset tienen un Selector, para aplicar los cambios s\u00f3lo en los Pods que tengan ciertas Labels. Nota Se pueden excluir Pods para los que no se quiera aplicar el PodPreset , a\u00f1adiendo en el Pod la anotaci\u00f3n: podpreset.admission.kubernetes.io/exclude: \"true\" . PodPreset apiVersion : settings.k8s.io/v1alpha1 kind : PodPreset metadata : name : allow-database spec : selector : matchLabels : role : frontend env : - name : DB_PORT value : \"6379\" volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} Pod apiVersion : v1 kind : Pod metadata : name : website labels : app : website role : frontend spec : containers : - name : website image : nginx ports : - containerPort : 80 Pod Topology Spread Constraints Existe una propiedad spec.topologySpreadConstraints en los Pod, que permite que los Pods se desplieguen de forma balanceada entre las distintas topolog\u00edas de nodos. Una topolog\u00eda de nodos se considera a un grupo de nodos que tiene una Label en com\u00fan con el mismo valor. Por ejemplo si hubiera 4 nodos, la mitad tuviera una Label zone=zoneA y la otra mitad zone=zoneB , se podr\u00eda decir que existen 2 topolog\u00edas de nodos (tomando como referencia \u00fanicamente esta Label). +---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ Partiendo de esta misma topolog\u00eda y que hubieran desplegados varios Pods de la siguiente manera, con Label foo=bar : +---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ | P | P | P | | +-------+-------+-------+-------+ Si se intentara desplegar el siguiente Pod: kind : Pod apiVersion : v1 metadata : name : mypod labels : foo : bar spec : topologySpreadConstraints : - maxSkew : 1 # N\u00famero m\u00e1ximo de Pods que pueden diferir las topolog\u00edas topologyKey : zone # Label key de referencia para identificar topolog\u00edas whenUnsatisfiable : DoNotSchedule # o ScheduleAnyway para no ser restrictivo labelSelector : # Pods que se utilizar\u00e1n para evaluar la restricci\u00f3n matchLabels : foo : bar containers : - name : pause image : k8s.gcr.io/pause:3.1 S\u00f3lo se podr\u00eda desplegar en los nodos de la zoneB ya que de lo contrario se estar\u00eda incumpliendo la condici\u00f3n maxSkew , pudiendo quedar de la siguiente manera: +---------------+---------------+ +---------------+---------------+ | zoneA | zoneB | | zoneA | zoneB | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | OR | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | P | P | P | P | | P | P | P P | | +-------+-------+-------+-------+ +-------+-------+-------+-------+ Estas restricciones se pueden combinar con los PodAffinity y PodAntiAffinity para agrupar Pods dentro de una misma topolog\u00eda o limitar a 1 Pod por topolog\u00eda. Pueden definirse multiples restricciones de topolog\u00eda, utilizando diferentes topologyKey . Para que el Pod se despliegue, se deben cumplir todas las restricciones definidas. Advertencia Hay que tener cuidado con el valor maxSkew que se defina en cada restricci\u00f3n, pues se pueden dar conflictos y los Pods no se desplegar\u00e1n. Por ejemplo: Escenario inicial: +---------------+-------+ | zoneA | zoneB | +-------+-------+-------+ | node1 | node2 | node3 | +-------+-------+-------+ | P P | P | P P | +-------+-------+-------+ Pod a desplegar: kind : Pod apiVersion : v1 metadata : name : mypod labels : foo : bar spec : topologySpreadConstraints : - maxSkew : 1 topologyKey : zone whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar - maxSkew : 1 topologyKey : node whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar containers : - name : pause image : k8s.gcr.io/pause:3.1 Resultado: El Pod no despliega porque si despliega en la zoneB incumple el maxSkew de la segunda restricci\u00f3n, y si despliega en la zoneA incumple el de la primera. Cuidado! Cuando por ejemplo un Deployment reduce las instancias de Pod, pueden darse casos de distribuci\u00f3n de Pods desbalanceados (incumpliendo el maxSkew ). Pod Disruption Budget Para evitar que una afectaci\u00f3n voluntaria (p.ej. actualizar un Deployment, forzando el redespliegue de sus Pods; eliminar manualmente un Pod; drenar un nodo...) desestabilice un despliegue en alta disponibilidad, se pueden definir PodDisruptionBudget , que limitan el n\u00famero de Pods que se pueden tumbar de forma voluntaria, para asegurar el servicio a los usuarios. Cuidado! Los PodDisruptionBudgets no pueden evitar afectaciones involuntarias (ca\u00edda de nodos, cortes de red...). Los PodDisruptionBufget pueden contener las siguientes propiedades: spec.selector : Label selector para elegit sobre qu\u00e9 Pods aplicar las restricciones. spec.maxUnavailable : Se puede indicar el n\u00famero m\u00e1ximo de Pods, o el porcentaje de Pods m\u00e1ximo, que pueden estar ca\u00eddos. spec.minAvailable : Se puede indicar el n\u00famero m\u00ednimo de Pods, o el porcentaje de Pods m\u00ednimo, que deben estar levantados. Informaci\u00f3n Si se especifica spec.maxUnavailable o spec.minAvailable en forma porcentual y el n\u00famero de Pods correspondiente no es un entero exacto, se toma el valor de redondear al alza (p.ej. el 50% de 7 Pods, ser\u00edan 4 Pods). Ejemplo apiVersion : policy/v1beta1 kind : PodDisruptionBudget metadata : name : zk-pdb spec : maxUnavailable : 1 selector : matchLabels : app : zookeeper Downward API Esta API permite a un Container obtener informaci\u00f3n de s\u00ed mismo, exponiendo campos del Pod y del Container. Existen dos formas de exponer estos campos: Variables de entorno Volume files Variables de entorno Se pueden crear variables de entorno en la definici\u00f3n de un Pod, que tengan el valor de ciertos campos del Pod o del Container. Para que una variable de entorno tenga el valor de un campo del Pod hay que utilizar la stanza fieldRef.fieldPath , mientras que para referenciar un campo del Container se debe usar resourceFieldRef . Informaci\u00f3n Existen algunas variables de entorno que Kubernetes crea dentro del Container por defecto. Algunas de ellas son: HOSTNAME: Nombre del Pod. NODE_NAME: Nombre del nodo. POD_NAMESPACE: Nombre del Namespace. MY_SERVICE_HOST: IP del Service. (Para cada Service del Namespace) MY_SERVICE_PORT: Puerto del Service. (Para cada Service del Namespace) Advertencia Las variables de entorno que hacen referencia a los Service del Namespace, se crean en tiempo de creaci\u00f3n del Pod, por lo que si se a\u00f1aden nuevos Service con posterioridad a la creaci\u00f3n del Pod, no estar\u00e1n disponibles como variables de entorno. Ejemplo variables de entorno apiVersion : v1 kind : Pod metadata : name : dapi-envars spec : containers : - name : test-container image : k8s.gcr.io/busybox:1.24 command : [ \"sh\" , \"-c\" ] args : - while true; do echo -en '\\n'; printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT; printenv MY_CPU_REQUEST MY_CPU_LIMIT; sleep 10; done; resources : requests : memory : \"32Mi\" cpu : \"125m\" limits : memory : \"64Mi\" cpu : \"250m\" env : # Campos del Pod - name : MY_POD_IP valueFrom : fieldRef : fieldPath : status.podIP - name : MY_POD_SERVICE_ACCOUNT valueFrom : fieldRef : fieldPath : spec.serviceAccountName # Campos del Container - name : MY_CPU_REQUEST valueFrom : resourceFieldRef : containerName : test-container resource : requests.cpu - name : MY_CPU_LIMIT valueFrom : resourceFieldRef : containerName : test-container resource : limits.cpu restartPolicy : Never Volume files Otra forma de que el Container disponga de esa informaci\u00f3n es volc\u00e1ndola en ficheros y montarlos en el Container con un Volume. Ejemplo con Volumes apiVersion : v1 kind : Pod metadata : name : kubernetes-downwardapi-volume-example labels : zone : us-est-coast cluster : test-cluster1 rack : rack-22 spec : containers : - name : client-container image : k8s.gcr.io/busybox command : [ \"sh\" , \"-c\" ] args : - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/cpu_limit ]]; then echo -en '\\n'; cat /etc/podinfo/cpu_limit; fi; sleep 5; done; resources : requests : memory : \"32Mi\" cpu : \"125m\" limits : memory : \"64Mi\" cpu : \"250m\" volumeMounts : - name : podinfo mountPath : /etc/podinfo volumes : - name : podinfo downwardAPI : items : # Campo del Pod - path : \"labels\" fieldRef : fieldPath : metadata.labels # Campo del Container - path : \"cpu_limit\" resourceFieldRef : containerName : client-container resource : limits.cpu divisor : 1m Container Registry privado Normalmente para acceder a Container Registries privados se requiere autenticarse contra ellos. Para que los Pods pueda hacer pull de las im\u00e1genes de un Registry privado, hay que crear primero un Secret con las credenciales: kubectl create secret docker-registry regcred --docker-server = <your-registry-server> --docker-username = <your-name> --docker-password = <your-pword> --docker-email = <your-email> A continuaci\u00f3n hacer referencia a este Secret desde la definici\u00f3n del Pod con la stanza imagePullSecrets : apiVersion : v1 kind : Pod metadata : name : private-reg spec : containers : - name : private-reg-container image : my-private-registry/my-image imagePullSecrets : - name : regcred Pods multi-container En ocasiones existe la necesidad de desplegar junto a las aplicaciones, elementos adicionales que los complementen. Por ejemplo, un agente de monitorizaci\u00f3n o un extractor de logs. Estos componentes no interesa acoplarlos al c\u00f3digo de las aplicaciones, dado que tienen responsabilidades diferentes. En estos casos, se despliegan como nuevos contenedores, dentro del mismo Pod que la aplicaci\u00f3n. Existen diferentes patrones de dise\u00f1o (aunque todos ellos se implementen de la misma forma): Sidecar: A\u00f1adir un contenedor dentro del mismo Pod que la aplicaci\u00f3n. Ejemplo: agente de extracci\u00f3n de logs, encargado de enviar logs de la aplicaci\u00f3n al servidor de logs Adapter: Igual que el patr\u00f3n Sidecar , pero en este caso los contenedores se encargan de transformar la informaci\u00f3n que se env\u00eda y rec\u00edbe. Ejemplo: siguiendo con el ejemplo anterior, si las aplicaciones generan logs en distintos formatos, a\u00f1adir un contenedor adicional, encargado de normalizar los logs, antes de enviarlos al servidor de logs. Ambassador: Igual que el patr\u00f3n Sidecar , pero en este caso el contenedor adicional, pretende abstraer a la aplicaci\u00f3n del entorno en el que se est\u00e1 ejecutando. Ejemplo: en el supuesto de existir varias bases de datos (DEV, PRE, PRO...), la aplicaci\u00f3n apuntar\u00eda al contenedor Ambassador (al estar en el mismo Pod, via localhost ), y este ser\u00eda el encargado de apuntar a la base de datos correcta.","title":"Pods"},{"location":"objects/pods/#pods","text":"La unidad b\u00e1sica de despliegue en Kubernetes son los Pods . Los Pods normalmente contienen un \u00fanico Container, pero existen situaciones en las que se requiere desplegar un Pod con m\u00faltiples Containers, por ejemplo para externalizar los logs del Container aun concentrador de logs. Todos los contenedores de un mismo Pod son levantados en un mismo nodo y comparten recursos, se pueden comunicar entre ellos de forma local y se pueden coordinar en la forma en que se destruyen. Los contenedores dentro de un Pod pueden ser de dos tipos: Init containers: Se ejecutan antes que los app containers y suelen ser procesos finitos. App containers: Se empiezan a ejecutar despu\u00e9s de que hayan terminado los init containers y contienen la aplicaci\u00f3n principal del Pod.","title":"Pods"},{"location":"objects/pods/#networking","text":"Cada Pod recibe una direcci\u00f3n IP \u00fanica. Esta IP es compartida por todos los Containers del Pod. Esto implica que cada contenedor deber\u00e1 exponerse (si deben exponerse varios contenedores) con un puerto diferente. Los Containers de un mismo Pod, pueden comunicarse entre si mediante localhost .","title":"Networking"},{"location":"objects/pods/#storage","text":"Los Containers de un mismo Pod, pueden compartir datos mediante Volumes. Los Volumes, depende de c\u00f3mo se configuren, permiten persistir esos datos y asegurar que seguir\u00e1n estando disponibles ante un reinicio del Pod.","title":"Storage"},{"location":"objects/pods/#pod-phase","text":"Dentro del campo status de un Pod (campo creado en tiempo de ejecuci\u00f3n), existe el campo phase que identifica en que fase est\u00e1 un Pod, dentro de su ciclo de vida. Estos son los valores posibles de phase : Valor Descripci\u00f3n Pending El Pod ha sido aceptado por Kubernetes, pero falta por descargar alguna Container Image. Esta fase, incluye el proceso de scheduling del Pod y el de descarga de las im\u00e1genes de los Containers en el nodo. Si no encuentra un nodo disponible, se queda en este estado a la espera Running El Pod ya ha sido desplegado en un nodo y todos los Containers han sido creados. Esta fase, incluye el proceso de inicio o reinicio de un Container, as\u00ed como cuando existe al menos un Container en ejecuci\u00f3n Succeeded Todos los Containers del Pod han terminado correctamente y no van a ser reiniciados Failed Todos los Containers del Pod han terminado, pero alguno de ellos con error. Esto puede ser porque un Container haya terminado con un c\u00f3digo de retorno diferente de zero, o que el sistema haya terminado el proceso Unknown El estado del Pod no se puede obtener (debido a un problema de comunicaci\u00f3n con el nodo o el Pod)","title":"Pod phase"},{"location":"objects/pods/#pod-conditions","text":"En el campo status tambi\u00e9n existen un conjunto de condiciones que pueden ayudar en las tareas de troubleshooting. Puede verse el estado de estas condiciones con kubectl describe pod . Listado de condiciones: PodScheduled : True cuando el Pod ha sido registrado en un nodo. Initialized : True cuando todos los init containers del Pod se han iniciado correctamente. ContainersReady : True cuando todos los Container del Pod est\u00e1n listos. Ready : True cuando el Pod esta disponible para recibir peticiones y ya puede ser a\u00f1adido en los balanceadores de carga de los Services. kubectl describe pod # ... Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True # ...","title":"Pod Conditions"},{"location":"objects/pods/#container-probes","text":"Los kubelet realizan peri\u00f3dicamente sondeos sobre los Containers para evaluar su correcto funcionamiento. Para ello ejecuta un Handler que expone el mismo Container. Existen 3 tipos de Handlers: ExecAction: Ejecuta un comando dentro del Container y se considera satisfactorio si devuelve un c\u00f3digo de estado 0 . readinessProbe : exec : command : - cat - /app/is_ready TCPSocketAction: Realiza un comprobaci\u00f3n TCP contra la direcci\u00f3n IP del Container, a un puerto espec\u00edfico. El resultado es satisfactorio si el puerto est\u00e1 abierto. readinessProbe : tcpSocket : port : 3306 HTTPGetAction: Realiza una petici\u00f3n HTTP GET contra la IP del Container, a un puerto y ruta espec\u00edficas. El resultado es satisfactorio si el c\u00f3digo de respuesta est\u00e1 entre 200 (incluido) y 400 (excluido). readinessProbe : httpGet : path : /api/ready port : 8080 Los resultados posibles son: Success: resultado satisfactorio. Failure: el Container no ha pasado el diagn\u00f3stico. Unknown: el diagn\u00f3stico ha fallado. Existen 3 tipos de sondeos para los que el kubelet puede tomar distintas acciones: Liveness Probe: Indica que el Container se est\u00e1 ejecutando correctamente. Si esta sonda falla, el kubelet destruye el Container y puede que sea reiniciado en funci\u00f3n de su \"pol\u00edtica de reinicio\". Si no se especifica esta sonda, se considera que el Container se ejecuta correctamente si est\u00e1 en estado Running . \u00bfCu\u00e1ndo se recomienda utilizar Liveness Probe? Cuando el proceso que se ejecuta en el Container no sea capaz de terminar debido a un error o un mal funcionamiento, ya que por defecto el kubelet considerar\u00e1 que el Container estar\u00e1 healthy , sin importar si la aplicaci\u00f3n tiene errores en su c\u00f3digo o configuraci\u00f3n Readiness Probe: Indica que el Container est\u00e1 listo para recibir peticiones. Si esta sonda falla, el Endpoints Controller, elimina la IP del Pod de todos los Endpoints en los que est\u00e9 registrado. Si no se especifica esta sonda, se concede un delay inicial, en el que se considera como si la sonda hubiera fallado. Pasado ese periodo inicial, la sonda pasa a tener resultado satisfactorio y la aplicaci\u00f3n empieza a recibir tr\u00e1fico. \u00bfCu\u00e1ndo se recomienda utilizar Readiness Probe? Si debido a la l\u00f3gica de negocio de la aplicaci\u00f3n, cuando se levanta el contenedor, a\u00fan no est\u00e1 preparada para recibir peticiones, esta sonda permite indicar cu\u00e1ndo el Container est\u00e1 preparado para empezar a recibir tr\u00e1fico (p.ej. comprobando el endpoint /health , si la aplicaci\u00f3n lo provee). Muy \u00fatil para los Containers que realicen procesos al inicio despu\u00e9s de levantarse (p.ej. cargar datos, preparar entorno...). Tambi\u00e9n es muy \u00fatil cuando se despliegan m\u00faltiples instancias, ya que hasta que estas no est\u00e9n listas, no empezar\u00e1n a recibir tr\u00e1fico. Startup Probe: Indica cu\u00e1ndo la aplicaci\u00f3n del Container se ha iniciado. Cuando se especifica esta sonda, no se empezar\u00e1n a evaluar el resto de sondas, hasta que esta d\u00e9 un resultado favorable. Si esta sonda falla, el kubelet destruye el Container y puede que sea reiniciado en funci\u00f3n de su \"pol\u00edtica de reinicio\". Si no se especifica esta sonda, se considera por defecto que el resultado es satisfactorio. \u00bfCu\u00e1ndo se recomienda utilizar Startup Probe? Cuando la aplicaci\u00f3n tarda mucho en iniciar la primera vez, esta sonda permite evaluar la misma condici\u00f3n que la Liveness Probe , pero con un periodo de gracia superior. De esta forma el periodo de gracia del Liveness Probe , no se ve afectado por la carga inicial de la aplicaci\u00f3n. Ejemplo Ejemplo de Pod con Readiness Probe : apiVersion : v1 kind : Pod metadata : name : my-pod spec : containers : - name : my-app image : my-image ports : - containerPort : 8080 readinessProbe : httpGet : path : /api/ready port : 8080 initDelaySeconds : 10 ## Delay inicial periodSeconds : 5 ## Cada cuando evaluar la sonda failureThreshold : 8 ## N\u00famero m\u00e1ximo de reintentos fallidos","title":"Container Probes"},{"location":"objects/pods/#pod-container-status","text":"El estado del Pod depende los estados de los Containers que contiene.","title":"Pod &amp; Container Status"},{"location":"objects/pods/#container-status","text":"Una vez un Pod es registrado en un nodo, el kubelet empieza a crear los Contenedores con el Container Runtime especificado. Los posibles estados del Conatiner son: Waiting : Estado por defecto, cuando no est\u00e1 en Running o Terminated . Durante este estado, se descargan las im\u00e1genes, se montan los Volumes... Al ejecutar kubectl describe pod <pod_name> , este estado suele ir acompa\u00f1ado de un mensaje que aporta m\u00e1s informaci\u00f3n del estado: #... State: Waiting Reason: ErrImagePull #... Running : Indica que el Container se est\u00e1 ejecutando sin problemas. Indica tambi\u00e9n que todos los postStart hooks se han ejecutado correctamente. Este estado va acompa\u00f1ado de la fecha en la que entr\u00f3 en este estado: #... State: Running Started: Wed, 30 Jan 2019 16 :46:38 +0530 #... Terminated : Indica que el Container ha completado su ejecuci\u00f3n y se ha detenido. Puede que el Container haya completado su ejecuci\u00f3n de forma satisfactoria o con error. Antes de pasar a este estado se van a ejecutar todos los preStop hooks. Este estado va acompa\u00f1ado de la raz\u00f3n por la que se ha completado la ejecuci\u00f3n, el c\u00f3digo de retorno, el tiempo de inicio del Container y el de su finalizaci\u00f3n: #... State: Terminated Reason: Completed Exit Code: 0 Started: Wed, 30 Jan 2019 11 :45:26 +0530 Finished: Wed, 30 Jan 2019 11 :45:26 +0530 #...","title":"Container Status"},{"location":"objects/pods/#anadir-condiciones-al-pod-status-pod-readiness","text":"Existe la posibilidad de a\u00f1adir condiciones adicionales, para evaluar el estado del Pod. Para ello se utiliza la propiedad readinessGates , d\u00f3nde se indica el nombre de qu\u00e9 condiciones adicionales se esperan recibir en el campo status.conditions y que deben tener status: \"True\" . Las condiciones deben ser a\u00f1adidas en el campo status.conditions mediante una acci\u00f3n PATCH contra la API de Kubernetes. Esto debe ser llevado a cabo por una aplicaci\u00f3n o opeator, que utilice una librer\u00eda cliente de Kubernetes . El Pod no pasar\u00e1 a estar listo, hasta que todos los Containers del Pod est\u00e9n listos y todas las condiciones indicadas en el readinessGates eval\u00faen a True . Si todos los Containers est\u00e1n disponibles, pero alguna de las condiciones adicionales no, el Pod solo tendr\u00e1 la condici\u00f3n ContainersReady a True . kind: Pod #... spec: readinessGates: - conditionType: \"www.example.com/feature-1\" status: conditions: - type: Ready # a built in PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018 -01-01T00:00:00Z - type: \"www.example.com/feature-1\" # an extra PodCondition status: \"False\" lastProbeTime: null lastTransitionTime: 2018 -01-01T00:00:00Z containerStatuses: - containerID: docker://abcd... ready: true #...","title":"A\u00f1adir condiciones al Pod Status (Pod Readiness)"},{"location":"objects/pods/#restart-policy","text":"Dentro de la definici\u00f3n de un Pod se puede especificar la pol\u00edtica de reinicio que van a tener todos los Containers que contiene, pudiendo tomar los valores Always (por defecto), OnFailure y Never . Cada vez que el kubelet reinicia un Container, se produce un back-off delay que aumenta de forma exponencial (10s, 20s, 40s...) hasta un m\u00e1ximo de 5 minutos. Una vez el Container lleva 10 minutos funcionando correctamente sin reinicios, el contador del delay vuelve a 0.","title":"Restart Policy"},{"location":"objects/pods/#init-containers","text":"Estos contenedores son \u00fatiles para ejecutar utilidades o scripts de configuraci\u00f3n que no est\u00e9n presentes en la Image de la aplicaci\u00f3n principal. Este listado de contenedores se ejecuta de forma secuencial en el orden en el que se hayan definido en el Pod (hasta que no termine de ejecutarse el primero, no empezar\u00e1 a ejecutarse el segundo). No empezar\u00e1n a ejecutarse los app containers hasta que el \u00faltimo init container haya completado su ejecuci\u00f3n. Al igual que los app containers , el estado de estos contenedores puede consultarse en el campo status.initContainerStatuses ( status.containerStatuses para los app containers ) del Pod. Dado que la ejecuci\u00f3n de los init containers puede darse varias veces (debido a un reinicio, a un reintento de ejecuci\u00f3n...), el proceso a ejecutar deber\u00eda ser idempotente. Los init containers no soportan los lifecycle hooks, livenessProbe , readinessProbe o startupProbe . Un cambio en la Image de un init container , reiniciar\u00e1 todos los Containers, pero un cambio en la Image de un app container , solo reiniciar\u00e1 ese mismo Container. Utilidades de los Init Containers Las Image de estos contenedores pueden contener utilidades que vayan a ser necesarias para la configuraci\u00f3n de la aplicaci\u00f3n y con ello simplificar el contenido de las Images de las aplicaciones principales. Esto ahorra tener que generar una nueva Image que a parte de la aplicaci\u00f3n principal contenga herramientas \"innecesarias\" y a\u00f1ade m\u00e1s seguridad a las Image de los app containers reduciendo los posibles vectores de ataque. Ofrece un mecanismo para bloquear la ejecuci\u00f3n de los app containers de un Pod, hasta que no se cumpla cierta condici\u00f3n. De esta forma se puede hacer que ciertos Pods \"dependan\" de otros. Pueden utilizar datos sensibles para sus ejecuciones (p.ej. mediante Secrets), sin tener que expon\u00e9rselos a los app containers . Ejemplo init container Este Pod depende de que dos Service ( myservice y mydb ) est\u00e9n disponibles, para poder arrancar: apiVersion : v1 kind : Pod metadata : name : myapp-pod labels : app : myapp spec : containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox:1.28 command : [ 'sh' , '-c' , \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\" ] - name : init-mydb image : busybox:1.28 command : [ 'sh' , '-c' , \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\" ]","title":"Init Containers"},{"location":"objects/pods/#pod-preset","text":"Este es un objeto Kubernetes que permite modificar la especificaci\u00f3n de un Pod en tiempo de creaci\u00f3n, permitiendo a\u00f1adir elementos tales como variables de entorno, Volumes o VolumeMounts (aplica tanto a los contenedores de aplicaciones como a los initContainer ). De esta manera el desarrollador del manifest del Pod no debe de conocer esas configuraciones. Los PodPreset tienen un Selector, para aplicar los cambios s\u00f3lo en los Pods que tengan ciertas Labels. Nota Se pueden excluir Pods para los que no se quiera aplicar el PodPreset , a\u00f1adiendo en el Pod la anotaci\u00f3n: podpreset.admission.kubernetes.io/exclude: \"true\" . PodPreset apiVersion : settings.k8s.io/v1alpha1 kind : PodPreset metadata : name : allow-database spec : selector : matchLabels : role : frontend env : - name : DB_PORT value : \"6379\" volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} Pod apiVersion : v1 kind : Pod metadata : name : website labels : app : website role : frontend spec : containers : - name : website image : nginx ports : - containerPort : 80","title":"Pod Preset"},{"location":"objects/pods/#pod-topology-spread-constraints","text":"Existe una propiedad spec.topologySpreadConstraints en los Pod, que permite que los Pods se desplieguen de forma balanceada entre las distintas topolog\u00edas de nodos. Una topolog\u00eda de nodos se considera a un grupo de nodos que tiene una Label en com\u00fan con el mismo valor. Por ejemplo si hubiera 4 nodos, la mitad tuviera una Label zone=zoneA y la otra mitad zone=zoneB , se podr\u00eda decir que existen 2 topolog\u00edas de nodos (tomando como referencia \u00fanicamente esta Label). +---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ Partiendo de esta misma topolog\u00eda y que hubieran desplegados varios Pods de la siguiente manera, con Label foo=bar : +---------------+---------------+ | zoneA | zoneB | +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ | P | P | P | | +-------+-------+-------+-------+ Si se intentara desplegar el siguiente Pod: kind : Pod apiVersion : v1 metadata : name : mypod labels : foo : bar spec : topologySpreadConstraints : - maxSkew : 1 # N\u00famero m\u00e1ximo de Pods que pueden diferir las topolog\u00edas topologyKey : zone # Label key de referencia para identificar topolog\u00edas whenUnsatisfiable : DoNotSchedule # o ScheduleAnyway para no ser restrictivo labelSelector : # Pods que se utilizar\u00e1n para evaluar la restricci\u00f3n matchLabels : foo : bar containers : - name : pause image : k8s.gcr.io/pause:3.1 S\u00f3lo se podr\u00eda desplegar en los nodos de la zoneB ya que de lo contrario se estar\u00eda incumpliendo la condici\u00f3n maxSkew , pudiendo quedar de la siguiente manera: +---------------+---------------+ +---------------+---------------+ | zoneA | zoneB | | zoneA | zoneB | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | node1 | node2 | node3 | node4 | OR | node1 | node2 | node3 | node4 | +-------+-------+-------+-------+ +-------+-------+-------+-------+ | P | P | P | P | | P | P | P P | | +-------+-------+-------+-------+ +-------+-------+-------+-------+ Estas restricciones se pueden combinar con los PodAffinity y PodAntiAffinity para agrupar Pods dentro de una misma topolog\u00eda o limitar a 1 Pod por topolog\u00eda. Pueden definirse multiples restricciones de topolog\u00eda, utilizando diferentes topologyKey . Para que el Pod se despliegue, se deben cumplir todas las restricciones definidas. Advertencia Hay que tener cuidado con el valor maxSkew que se defina en cada restricci\u00f3n, pues se pueden dar conflictos y los Pods no se desplegar\u00e1n. Por ejemplo: Escenario inicial: +---------------+-------+ | zoneA | zoneB | +-------+-------+-------+ | node1 | node2 | node3 | +-------+-------+-------+ | P P | P | P P | +-------+-------+-------+ Pod a desplegar: kind : Pod apiVersion : v1 metadata : name : mypod labels : foo : bar spec : topologySpreadConstraints : - maxSkew : 1 topologyKey : zone whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar - maxSkew : 1 topologyKey : node whenUnsatisfiable : DoNotSchedule labelSelector : matchLabels : foo : bar containers : - name : pause image : k8s.gcr.io/pause:3.1 Resultado: El Pod no despliega porque si despliega en la zoneB incumple el maxSkew de la segunda restricci\u00f3n, y si despliega en la zoneA incumple el de la primera. Cuidado! Cuando por ejemplo un Deployment reduce las instancias de Pod, pueden darse casos de distribuci\u00f3n de Pods desbalanceados (incumpliendo el maxSkew ).","title":"Pod Topology Spread Constraints"},{"location":"objects/pods/#pod-disruption-budget","text":"Para evitar que una afectaci\u00f3n voluntaria (p.ej. actualizar un Deployment, forzando el redespliegue de sus Pods; eliminar manualmente un Pod; drenar un nodo...) desestabilice un despliegue en alta disponibilidad, se pueden definir PodDisruptionBudget , que limitan el n\u00famero de Pods que se pueden tumbar de forma voluntaria, para asegurar el servicio a los usuarios. Cuidado! Los PodDisruptionBudgets no pueden evitar afectaciones involuntarias (ca\u00edda de nodos, cortes de red...). Los PodDisruptionBufget pueden contener las siguientes propiedades: spec.selector : Label selector para elegit sobre qu\u00e9 Pods aplicar las restricciones. spec.maxUnavailable : Se puede indicar el n\u00famero m\u00e1ximo de Pods, o el porcentaje de Pods m\u00e1ximo, que pueden estar ca\u00eddos. spec.minAvailable : Se puede indicar el n\u00famero m\u00ednimo de Pods, o el porcentaje de Pods m\u00ednimo, que deben estar levantados. Informaci\u00f3n Si se especifica spec.maxUnavailable o spec.minAvailable en forma porcentual y el n\u00famero de Pods correspondiente no es un entero exacto, se toma el valor de redondear al alza (p.ej. el 50% de 7 Pods, ser\u00edan 4 Pods). Ejemplo apiVersion : policy/v1beta1 kind : PodDisruptionBudget metadata : name : zk-pdb spec : maxUnavailable : 1 selector : matchLabels : app : zookeeper","title":"Pod Disruption Budget"},{"location":"objects/pods/#downward-api","text":"Esta API permite a un Container obtener informaci\u00f3n de s\u00ed mismo, exponiendo campos del Pod y del Container. Existen dos formas de exponer estos campos: Variables de entorno Volume files","title":"Downward API"},{"location":"objects/pods/#variables-de-entorno","text":"Se pueden crear variables de entorno en la definici\u00f3n de un Pod, que tengan el valor de ciertos campos del Pod o del Container. Para que una variable de entorno tenga el valor de un campo del Pod hay que utilizar la stanza fieldRef.fieldPath , mientras que para referenciar un campo del Container se debe usar resourceFieldRef . Informaci\u00f3n Existen algunas variables de entorno que Kubernetes crea dentro del Container por defecto. Algunas de ellas son: HOSTNAME: Nombre del Pod. NODE_NAME: Nombre del nodo. POD_NAMESPACE: Nombre del Namespace. MY_SERVICE_HOST: IP del Service. (Para cada Service del Namespace) MY_SERVICE_PORT: Puerto del Service. (Para cada Service del Namespace) Advertencia Las variables de entorno que hacen referencia a los Service del Namespace, se crean en tiempo de creaci\u00f3n del Pod, por lo que si se a\u00f1aden nuevos Service con posterioridad a la creaci\u00f3n del Pod, no estar\u00e1n disponibles como variables de entorno. Ejemplo variables de entorno apiVersion : v1 kind : Pod metadata : name : dapi-envars spec : containers : - name : test-container image : k8s.gcr.io/busybox:1.24 command : [ \"sh\" , \"-c\" ] args : - while true; do echo -en '\\n'; printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT; printenv MY_CPU_REQUEST MY_CPU_LIMIT; sleep 10; done; resources : requests : memory : \"32Mi\" cpu : \"125m\" limits : memory : \"64Mi\" cpu : \"250m\" env : # Campos del Pod - name : MY_POD_IP valueFrom : fieldRef : fieldPath : status.podIP - name : MY_POD_SERVICE_ACCOUNT valueFrom : fieldRef : fieldPath : spec.serviceAccountName # Campos del Container - name : MY_CPU_REQUEST valueFrom : resourceFieldRef : containerName : test-container resource : requests.cpu - name : MY_CPU_LIMIT valueFrom : resourceFieldRef : containerName : test-container resource : limits.cpu restartPolicy : Never","title":"Variables de entorno"},{"location":"objects/pods/#volume-files","text":"Otra forma de que el Container disponga de esa informaci\u00f3n es volc\u00e1ndola en ficheros y montarlos en el Container con un Volume. Ejemplo con Volumes apiVersion : v1 kind : Pod metadata : name : kubernetes-downwardapi-volume-example labels : zone : us-est-coast cluster : test-cluster1 rack : rack-22 spec : containers : - name : client-container image : k8s.gcr.io/busybox command : [ \"sh\" , \"-c\" ] args : - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/cpu_limit ]]; then echo -en '\\n'; cat /etc/podinfo/cpu_limit; fi; sleep 5; done; resources : requests : memory : \"32Mi\" cpu : \"125m\" limits : memory : \"64Mi\" cpu : \"250m\" volumeMounts : - name : podinfo mountPath : /etc/podinfo volumes : - name : podinfo downwardAPI : items : # Campo del Pod - path : \"labels\" fieldRef : fieldPath : metadata.labels # Campo del Container - path : \"cpu_limit\" resourceFieldRef : containerName : client-container resource : limits.cpu divisor : 1m","title":"Volume files"},{"location":"objects/pods/#container-registry-privado","text":"Normalmente para acceder a Container Registries privados se requiere autenticarse contra ellos. Para que los Pods pueda hacer pull de las im\u00e1genes de un Registry privado, hay que crear primero un Secret con las credenciales: kubectl create secret docker-registry regcred --docker-server = <your-registry-server> --docker-username = <your-name> --docker-password = <your-pword> --docker-email = <your-email> A continuaci\u00f3n hacer referencia a este Secret desde la definici\u00f3n del Pod con la stanza imagePullSecrets : apiVersion : v1 kind : Pod metadata : name : private-reg spec : containers : - name : private-reg-container image : my-private-registry/my-image imagePullSecrets : - name : regcred","title":"Container Registry privado"},{"location":"objects/pods/#pods-multi-container","text":"En ocasiones existe la necesidad de desplegar junto a las aplicaciones, elementos adicionales que los complementen. Por ejemplo, un agente de monitorizaci\u00f3n o un extractor de logs. Estos componentes no interesa acoplarlos al c\u00f3digo de las aplicaciones, dado que tienen responsabilidades diferentes. En estos casos, se despliegan como nuevos contenedores, dentro del mismo Pod que la aplicaci\u00f3n. Existen diferentes patrones de dise\u00f1o (aunque todos ellos se implementen de la misma forma): Sidecar: A\u00f1adir un contenedor dentro del mismo Pod que la aplicaci\u00f3n. Ejemplo: agente de extracci\u00f3n de logs, encargado de enviar logs de la aplicaci\u00f3n al servidor de logs Adapter: Igual que el patr\u00f3n Sidecar , pero en este caso los contenedores se encargan de transformar la informaci\u00f3n que se env\u00eda y rec\u00edbe. Ejemplo: siguiendo con el ejemplo anterior, si las aplicaciones generan logs en distintos formatos, a\u00f1adir un contenedor adicional, encargado de normalizar los logs, antes de enviarlos al servidor de logs. Ambassador: Igual que el patr\u00f3n Sidecar , pero en este caso el contenedor adicional, pretende abstraer a la aplicaci\u00f3n del entorno en el que se est\u00e1 ejecutando. Ejemplo: en el supuesto de existir varias bases de datos (DEV, PRE, PRO...), la aplicaci\u00f3n apuntar\u00eda al contenedor Ambassador (al estar en el mismo Pod, via localhost ), y este ser\u00eda el encargado de apuntar a la base de datos correcta.","title":"Pods multi-container"},{"location":"other/json-path/","text":"JSON Path El elemento root de un JSON se referencia con $ . Mediante la expresi\u00f3n ?(<criteria>) se puede filtrar los elementos de una lista, en funci\u00f3n de un criterio. Por ejemplo, en funci\u00f3n del valor de los elementos de la lista (representados con @ ), o de alguna de sus propiedades. Para obtener un rango de elementos de una lista se puede hacer con la expresi\u00f3n $[inicio:fin] . El primer elemento devuelto ser\u00e1 el de la posici\u00f3n inicio y el \u00faltimo el de la posici\u00f3n anterior a fin . Tambien se puede obtener un rango de valores alternos con la expresi\u00f3n $[inicio:fin:salto] . Por ejemplo, para obtener \u00fanicamente los valores de las posiciones pares. Ejemplo1 { \"car\" : { \"wheels\" : [ { \"model\" : \"1\" , \"location\" : \"spain\" } ] } } Obtener modelo de rueda: $.car.wheels[0].model Resultado: \"1\" Obtener modelo de rueda con \"location\" igual a \"spain\": $.car.wheels[?(@.location==\"spain\")].model Resultado: \"1\" Ejemplo2 [ 12 , 20 , 40 , 50 ] Obtener n\u00fameros mayores a 30: $[?(@>30)] o $[?(@ in [40,50])] o $[?(@ nin [12,20])] Resultado: [40, 50] Obtener el \u00faltimo elemento de la lista: $[-1:0] Resultado: [50] Obtener el pen\u00faltimo elemento de la lista: $[-2:-1] Resultado: [40] Obtener los tres primeros elementos: $[0:3] ( Ojo!: Devuelve los elementos 0, 1 y 2) Resultado: [12, 20, 40] Ejemplo3 [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] Obtener valores con posici\u00f3n par: $[0:8:2] Resultado: [0, 2, 4, 6] kubectl y JSON Path Con kubectl no hace falta hacer referencia al elemento root con $ . Por ejemplo para obtener un listado de los nodos y su CPU: kubectl get nodes -o jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}' Otra forma de mostrar la informaci\u00f3n ser\u00eda con: kubectl get nodes -o custom-columns = NODE:.metadata.name,CPU:.status.capacity.cpu --sort-by = .status.capacity.cpu","title":"JSON Path"},{"location":"other/json-path/#json-path","text":"El elemento root de un JSON se referencia con $ . Mediante la expresi\u00f3n ?(<criteria>) se puede filtrar los elementos de una lista, en funci\u00f3n de un criterio. Por ejemplo, en funci\u00f3n del valor de los elementos de la lista (representados con @ ), o de alguna de sus propiedades. Para obtener un rango de elementos de una lista se puede hacer con la expresi\u00f3n $[inicio:fin] . El primer elemento devuelto ser\u00e1 el de la posici\u00f3n inicio y el \u00faltimo el de la posici\u00f3n anterior a fin . Tambien se puede obtener un rango de valores alternos con la expresi\u00f3n $[inicio:fin:salto] . Por ejemplo, para obtener \u00fanicamente los valores de las posiciones pares. Ejemplo1 { \"car\" : { \"wheels\" : [ { \"model\" : \"1\" , \"location\" : \"spain\" } ] } } Obtener modelo de rueda: $.car.wheels[0].model Resultado: \"1\" Obtener modelo de rueda con \"location\" igual a \"spain\": $.car.wheels[?(@.location==\"spain\")].model Resultado: \"1\" Ejemplo2 [ 12 , 20 , 40 , 50 ] Obtener n\u00fameros mayores a 30: $[?(@>30)] o $[?(@ in [40,50])] o $[?(@ nin [12,20])] Resultado: [40, 50] Obtener el \u00faltimo elemento de la lista: $[-1:0] Resultado: [50] Obtener el pen\u00faltimo elemento de la lista: $[-2:-1] Resultado: [40] Obtener los tres primeros elementos: $[0:3] ( Ojo!: Devuelve los elementos 0, 1 y 2) Resultado: [12, 20, 40] Ejemplo3 [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] Obtener valores con posici\u00f3n par: $[0:8:2] Resultado: [0, 2, 4, 6]","title":"JSON Path"},{"location":"other/json-path/#kubectl-y-json-path","text":"Con kubectl no hace falta hacer referencia al elemento root con $ . Por ejemplo para obtener un listado de los nodos y su CPU: kubectl get nodes -o jsonpath = '{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"}{end}' Otra forma de mostrar la informaci\u00f3n ser\u00eda con: kubectl get nodes -o custom-columns = NODE:.metadata.name,CPU:.status.capacity.cpu --sort-by = .status.capacity.cpu","title":"kubectl y JSON Path"},{"location":"policies/limit-ranges/","text":"Limit Ranges Por defecto, los contenedores se ejecutan dentro del cluster de Kubernetes sin ning\u00fan tipo de restricci\u00f3n en el consumo de recursos. Mediante el uso del API object LimitRange , los administradores pueden aplicar las siguientes restricciones: Regular el uso m\u00ednimo y m\u00e1ximo de recursos por Pod o Container en un Namespace. Regular el almacenamiento m\u00ednimo y m\u00e1ximo que pueden solicitar los PersistentVolumeClaim de un Namespace. Configurar request/limit por defecto para los contenedores de un Namespace. Notas El Admission Controller LimitRanger se encarga de inyectar en tiempo de creaci\u00f3n, request/limit por defecto, en los contenedores que no los lleven configurados. Cualquier petici\u00f3n para crear/actualizar un recurso, que vulnere las restricciones definidas en un LimitRange , ser\u00e1 denegada con un \" 403 Forbidden \". Las validaciones del LimitRange se realizan en la fase de \"Pod Admission\", por lo que no se aplica sobre los Pods que ya se est\u00e9n ejecutando. Sucede lo mismo cuando se modifica un LimitRange : no se aplican cambios en los Pods ya creados. Recursos por defecto Se pueden utilizar los LimitRange para modificar en tiempo de creaci\u00f3n, la definici\u00f3n de los contenedores, para inyectar requests/limits por defecto. Asignar valores por defecto para los limit de memoria, permite que un Pod pueda ser desplegado en un Namespace con un ResourceQuota definido. Ejemplo LimitRange que configura un request de 0.5 CPU y un limit de 1 CPU para los contenedores del Namespace \"my-test\" que no los tengan configurados: apiVersion : v1 kind : LimitRange metadata : name : cpu-limit-range namespace : my-test spec : limits : - default : cpu : 1 defaultRequest : cpu : 0.5 type : Container Si se intenta crear un Pod en un Namespace con este LimitRange de ejemplo, se pueden dar estos casos (similar para memoria RAM): Pod sin requests/limits : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 0.5 CPU y un limit de 1 CPU. Pod con limit de 2 CPU y sin request : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 2 CPU y un limit de 2 CPU. Pod con request de 0.2 CPU y sin limit : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 0.2 CPU y un limit de 1 CPU. CPU/Memoria Min y Max Se puede limitar qu\u00e9 Pods pueden ser admitidos en un Namespace, en funci\u00f3n de los requests/limits que tengan definidos. Por ejemplo no permitir desplegar Pods que no lleguen a un m\u00ednimo de recursos o que no superen un m\u00e1ximo. Ejemplo LimitRange que solo admite contenedores que consuman m\u00ednimo 500Mi de RAM y no superen el uso de 1Gi de RAM: apiVersion : v1 kind : LimitRange metadata : name : mem-min-max-demo-lr spec : limits : - max : memory : 1Gi min : memory : 500Mi type : Container El LimitRange del ejemplo, al no especificar \"valores por defecto\" para los contenedores que no definan requests/limits , toma por defecto el valor m\u00e1ximo del limit . Es decir, seria lo mismo que crear el siguiente LimitRange : apiVersion : v1 kind : LimitRange metadata : name : mem-min-max-demo-lr namespace : my-test spec : limits : - default : memory : 1Gi defaultRequest : memory : 1Gi max : memory : 1Gi min : memory : 500Mi type : Container Nota Especificar valores m\u00e1ximos de recursos es \u00fatil para evitar desplegar Pods que requieran m\u00e1s recursos de los que proporcionan los nodos. Si se intenta crear un Pod en un Namespace con este LimitRange de ejemplo, se pueden dar estos casos (similar para CPU): Pod sin requests/limits : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 1Gi RAM y un limit de 1 Gi RAM. Pod con request de 500Mi y limit de 2Gi : No se permitir\u00e1 el despliegue dado que sobrepasa el m\u00e1ximo permitido en el Namespace. Pod con request de 200Mi y limit de 1Gi : No se permitir\u00e1 el despliegue dado que no llega al m\u00ednimo requerido en el Namespace. Almacenamiento Min y Max Al igual que con los recursos CPU y memoria RAM para los contenedores, se puede limitar qu\u00e9 PersistentVolumeClaim se admiten en un Namespace, en funci\u00f3n del almacenamiento que se solicite. Esto permite a los administradores controlar por ejemplo el m\u00e1ximo de almacenamiento que puede solicitar un \u00fanico Claim. Ejemplo apiVersion : v1 kind : LimitRange metadata : name : storagelimits spec : limits : - type : PersistentVolumeClaim max : storage : 2Gi min : storage : 1Gi","title":"Limit Ranges"},{"location":"policies/limit-ranges/#limit-ranges","text":"Por defecto, los contenedores se ejecutan dentro del cluster de Kubernetes sin ning\u00fan tipo de restricci\u00f3n en el consumo de recursos. Mediante el uso del API object LimitRange , los administradores pueden aplicar las siguientes restricciones: Regular el uso m\u00ednimo y m\u00e1ximo de recursos por Pod o Container en un Namespace. Regular el almacenamiento m\u00ednimo y m\u00e1ximo que pueden solicitar los PersistentVolumeClaim de un Namespace. Configurar request/limit por defecto para los contenedores de un Namespace. Notas El Admission Controller LimitRanger se encarga de inyectar en tiempo de creaci\u00f3n, request/limit por defecto, en los contenedores que no los lleven configurados. Cualquier petici\u00f3n para crear/actualizar un recurso, que vulnere las restricciones definidas en un LimitRange , ser\u00e1 denegada con un \" 403 Forbidden \". Las validaciones del LimitRange se realizan en la fase de \"Pod Admission\", por lo que no se aplica sobre los Pods que ya se est\u00e9n ejecutando. Sucede lo mismo cuando se modifica un LimitRange : no se aplican cambios en los Pods ya creados.","title":"Limit Ranges"},{"location":"policies/limit-ranges/#recursos-por-defecto","text":"Se pueden utilizar los LimitRange para modificar en tiempo de creaci\u00f3n, la definici\u00f3n de los contenedores, para inyectar requests/limits por defecto. Asignar valores por defecto para los limit de memoria, permite que un Pod pueda ser desplegado en un Namespace con un ResourceQuota definido. Ejemplo LimitRange que configura un request de 0.5 CPU y un limit de 1 CPU para los contenedores del Namespace \"my-test\" que no los tengan configurados: apiVersion : v1 kind : LimitRange metadata : name : cpu-limit-range namespace : my-test spec : limits : - default : cpu : 1 defaultRequest : cpu : 0.5 type : Container Si se intenta crear un Pod en un Namespace con este LimitRange de ejemplo, se pueden dar estos casos (similar para memoria RAM): Pod sin requests/limits : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 0.5 CPU y un limit de 1 CPU. Pod con limit de 2 CPU y sin request : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 2 CPU y un limit de 2 CPU. Pod con request de 0.2 CPU y sin limit : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 0.2 CPU y un limit de 1 CPU.","title":"Recursos por defecto"},{"location":"policies/limit-ranges/#cpumemoria-min-y-max","text":"Se puede limitar qu\u00e9 Pods pueden ser admitidos en un Namespace, en funci\u00f3n de los requests/limits que tengan definidos. Por ejemplo no permitir desplegar Pods que no lleguen a un m\u00ednimo de recursos o que no superen un m\u00e1ximo. Ejemplo LimitRange que solo admite contenedores que consuman m\u00ednimo 500Mi de RAM y no superen el uso de 1Gi de RAM: apiVersion : v1 kind : LimitRange metadata : name : mem-min-max-demo-lr spec : limits : - max : memory : 1Gi min : memory : 500Mi type : Container El LimitRange del ejemplo, al no especificar \"valores por defecto\" para los contenedores que no definan requests/limits , toma por defecto el valor m\u00e1ximo del limit . Es decir, seria lo mismo que crear el siguiente LimitRange : apiVersion : v1 kind : LimitRange metadata : name : mem-min-max-demo-lr namespace : my-test spec : limits : - default : memory : 1Gi defaultRequest : memory : 1Gi max : memory : 1Gi min : memory : 500Mi type : Container Nota Especificar valores m\u00e1ximos de recursos es \u00fatil para evitar desplegar Pods que requieran m\u00e1s recursos de los que proporcionan los nodos. Si se intenta crear un Pod en un Namespace con este LimitRange de ejemplo, se pueden dar estos casos (similar para CPU): Pod sin requests/limits : Se crear\u00e1 un Pod cuyos contenedores tendr\u00e1n un request de 1Gi RAM y un limit de 1 Gi RAM. Pod con request de 500Mi y limit de 2Gi : No se permitir\u00e1 el despliegue dado que sobrepasa el m\u00e1ximo permitido en el Namespace. Pod con request de 200Mi y limit de 1Gi : No se permitir\u00e1 el despliegue dado que no llega al m\u00ednimo requerido en el Namespace.","title":"CPU/Memoria Min y Max"},{"location":"policies/limit-ranges/#almacenamiento-min-y-max","text":"Al igual que con los recursos CPU y memoria RAM para los contenedores, se puede limitar qu\u00e9 PersistentVolumeClaim se admiten en un Namespace, en funci\u00f3n del almacenamiento que se solicite. Esto permite a los administradores controlar por ejemplo el m\u00e1ximo de almacenamiento que puede solicitar un \u00fanico Claim. Ejemplo apiVersion : v1 kind : LimitRange metadata : name : storagelimits spec : limits : - type : PersistentVolumeClaim max : storage : 2Gi min : storage : 1Gi","title":"Almacenamiento Min y Max"},{"location":"policies/resource-quota/","text":"Resource Quota Cuando en un mismo cluster trabajan diferentes equipos, cada uno de ellos en un Namespace diferente, los administradores necesitan limitar los recursos asignados a cada espacio de trabajo. Para ello, los administradores utilizan ResourceQuota , los cuales permiten limitar el uso de recursos agregado (suma de todos los consumos) de un Namespace, as\u00ed como el n\u00famero m\u00e1ximo de API objects permitidos de un tipo concreto. Notas Un Namespace con un ResourceQuota que especifique los requests totales permitidos, no permitir\u00e1 desplegar Pods sin requests definidos. Lo mismo sucede cuando un ResourceQuota define limits totales. Los LimitRange ayudan a evitar que se denieguen despliegues por no tener limits/requests definidos. Cualquier petici\u00f3n de creaci\u00f3n/actualizaci\u00f3n de un API object que implique vulnerar las restricciones del ResourceQuota , ser\u00e1 denegada. Quota para recursos de computaci\u00f3n En el siguiente ejemplo se limita el total de requests y limits permitidas para la CPU y la RAM: Ejemplo apiVersion : v1 kind : ResourceQuota metadata : name : mem-cpu-demo spec : hard : requests.cpu : \"1\" requests.memory : 1Gi limits.cpu : \"2\" limits.memory : 2Gi En este Namespace, no se permitir\u00e1n Pods que no tengan requests o limits definidos (a no ser que se los inyecte un LimitRange ), y por ejemplo, el total de CPU requests no puede superar 1 CPU. Quota para recursos de almacenamiento Se pueden limitar los siguientes recursos de almacenamiento en un Namespace: Stanza Descripci\u00f3n requests.storage Almacenamiento total permitido, sumando todas las peticiones de almacenamiento de los PersistentVolumeClaim persistentvolumeclaims N\u00famero total de PersistentVolumeClaim que pueden existir en un Namespace <storage-class-name>.storageclass.storage.k8s.io/requests.storage Almacenamiento total permitido, sumando todas las peticiones de almacenamiento de los PersistentVolumeClaim de una StorageClass concreta <storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims N\u00famero total de PersistentVolumeClaim de una StorageClass concreta que pueden existir en un Namespace requests.ephemeral-storage Total de requests de almacenamiento ef\u00edmero permitido en el Namespace limits.ephemeral-storage Total de limits de almacenamiento ef\u00edmero permitido en el Namespace Ejemplo apiVersion : v1 kind : ResourceQuota metadata : name : storagequota spec : hard : persistentvolumeclaims : \"5\" requests.storage : \"5Gi\" Quota para n\u00famero de API objects Para limitar el n\u00famero de Api objects de un tipo concreto, se puede utilizar la stanza count/<resource>.<api-group> . Esto sirve tanto para API objects nativos de Kubernetes, como para Custom Resource Definitions . Por ejemplo: count/cronjobs.batch Para los siguientes recursos se puede utilizar tambi\u00e9n una stanza simplificada: Stanza simplificada Stanza normal configmaps count/configmaps secrets count/secrets persistentvolumeclaims count/persistentvolumeclaims pods count/pods replicationcontrollers count/replicationcontrollers resourcequotas count/resourcequotas services count/services services.loadbalancers services.nodeports Quota Scopes Se puede seleccionar qu\u00e9 Pods se deben tener en cuenta para evaluar el ResourceQuota . Para filtrar los Pods, se utiliza el array spec.scopes . Los valores que puede tomar la propiedad son: Scope Descripci\u00f3n Terminating S\u00f3lo tiene en cuenta los Pods donde spec.activeDeadlineSeconds >= 0 NotTerminating S\u00f3lo tiene en cuenta los Pods donde spec.activeDeadlineSeconds sea nil BestEffort S\u00f3lo tiene en cuenta los Pods que tengan un QoS \"Best-Effort\" NotBestEffort S\u00f3lo tiene en cuenta los Pods que no tengan un QoS \"Best-Effort\" Notas Con el scope BestEffort s\u00f3lo se puede restringir el recurso pods . Con el resto de scopes, s\u00f3lo se pueden restringir los recursos pods , cpu , requests.cpu , limits.cpu , memory , requests.memory , limits.memory . Ejemplo En el Namespace de este ejemplo s\u00f3lo pueden existir 100 Pods con QoS \"Best-Effort\": apiVersion : v1 kind : ResourceQuota metadata : name : best-effort spec : hard : pods : \"100\" scopes : - BestEffort ResourceQuota por PriorityClass Se puede limitar tambi\u00e9n el consumo m\u00e1ximo que puede tener un grupo de Pods en funci\u00f3n de su PriorityClass . Ejemplo En este ejemplo los Pods con la stanza spec.priorityClassName: high , no podr\u00e1n consumir en total m\u00e1s de 1000 CPU y 200Gi RAM. Adem\u00e1s no podr\u00e1n ser m\u00e1s de 10 Pods. apiVersion : v1 kind : ResourceQuota metadata : name : pods-high spec : hard : cpu : \"1000\" memory : 200Gi pods : \"10\" scopeSelector : matchExpressions : - operator : In scopeName : PriorityClass values : [ \"high\" ] LimitRange vs ResourceQuota A la hora de establecer los consumos m\u00e1ximos de CPU y RAM, los LimitRange permiten definir el m\u00ednimo y m\u00e1ximo permitidos por contenedor de forma individual. Por otra parte, los ResourceQuota permiten definir el m\u00ednimo y m\u00e1ximo de recursos que se pueden consumir en un Namespace, teniendo en cuenta el consumo total de todos los contenedores.","title":"Resource Quotas"},{"location":"policies/resource-quota/#resource-quota","text":"Cuando en un mismo cluster trabajan diferentes equipos, cada uno de ellos en un Namespace diferente, los administradores necesitan limitar los recursos asignados a cada espacio de trabajo. Para ello, los administradores utilizan ResourceQuota , los cuales permiten limitar el uso de recursos agregado (suma de todos los consumos) de un Namespace, as\u00ed como el n\u00famero m\u00e1ximo de API objects permitidos de un tipo concreto. Notas Un Namespace con un ResourceQuota que especifique los requests totales permitidos, no permitir\u00e1 desplegar Pods sin requests definidos. Lo mismo sucede cuando un ResourceQuota define limits totales. Los LimitRange ayudan a evitar que se denieguen despliegues por no tener limits/requests definidos. Cualquier petici\u00f3n de creaci\u00f3n/actualizaci\u00f3n de un API object que implique vulnerar las restricciones del ResourceQuota , ser\u00e1 denegada.","title":"Resource Quota"},{"location":"policies/resource-quota/#quota-para-recursos-de-computacion","text":"En el siguiente ejemplo se limita el total de requests y limits permitidas para la CPU y la RAM: Ejemplo apiVersion : v1 kind : ResourceQuota metadata : name : mem-cpu-demo spec : hard : requests.cpu : \"1\" requests.memory : 1Gi limits.cpu : \"2\" limits.memory : 2Gi En este Namespace, no se permitir\u00e1n Pods que no tengan requests o limits definidos (a no ser que se los inyecte un LimitRange ), y por ejemplo, el total de CPU requests no puede superar 1 CPU.","title":"Quota para recursos de computaci\u00f3n"},{"location":"policies/resource-quota/#quota-para-recursos-de-almacenamiento","text":"Se pueden limitar los siguientes recursos de almacenamiento en un Namespace: Stanza Descripci\u00f3n requests.storage Almacenamiento total permitido, sumando todas las peticiones de almacenamiento de los PersistentVolumeClaim persistentvolumeclaims N\u00famero total de PersistentVolumeClaim que pueden existir en un Namespace <storage-class-name>.storageclass.storage.k8s.io/requests.storage Almacenamiento total permitido, sumando todas las peticiones de almacenamiento de los PersistentVolumeClaim de una StorageClass concreta <storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims N\u00famero total de PersistentVolumeClaim de una StorageClass concreta que pueden existir en un Namespace requests.ephemeral-storage Total de requests de almacenamiento ef\u00edmero permitido en el Namespace limits.ephemeral-storage Total de limits de almacenamiento ef\u00edmero permitido en el Namespace Ejemplo apiVersion : v1 kind : ResourceQuota metadata : name : storagequota spec : hard : persistentvolumeclaims : \"5\" requests.storage : \"5Gi\"","title":"Quota para recursos de almacenamiento"},{"location":"policies/resource-quota/#quota-para-numero-de-api-objects","text":"Para limitar el n\u00famero de Api objects de un tipo concreto, se puede utilizar la stanza count/<resource>.<api-group> . Esto sirve tanto para API objects nativos de Kubernetes, como para Custom Resource Definitions . Por ejemplo: count/cronjobs.batch Para los siguientes recursos se puede utilizar tambi\u00e9n una stanza simplificada: Stanza simplificada Stanza normal configmaps count/configmaps secrets count/secrets persistentvolumeclaims count/persistentvolumeclaims pods count/pods replicationcontrollers count/replicationcontrollers resourcequotas count/resourcequotas services count/services services.loadbalancers services.nodeports","title":"Quota para n\u00famero de API objects"},{"location":"policies/resource-quota/#quota-scopes","text":"Se puede seleccionar qu\u00e9 Pods se deben tener en cuenta para evaluar el ResourceQuota . Para filtrar los Pods, se utiliza el array spec.scopes . Los valores que puede tomar la propiedad son: Scope Descripci\u00f3n Terminating S\u00f3lo tiene en cuenta los Pods donde spec.activeDeadlineSeconds >= 0 NotTerminating S\u00f3lo tiene en cuenta los Pods donde spec.activeDeadlineSeconds sea nil BestEffort S\u00f3lo tiene en cuenta los Pods que tengan un QoS \"Best-Effort\" NotBestEffort S\u00f3lo tiene en cuenta los Pods que no tengan un QoS \"Best-Effort\" Notas Con el scope BestEffort s\u00f3lo se puede restringir el recurso pods . Con el resto de scopes, s\u00f3lo se pueden restringir los recursos pods , cpu , requests.cpu , limits.cpu , memory , requests.memory , limits.memory . Ejemplo En el Namespace de este ejemplo s\u00f3lo pueden existir 100 Pods con QoS \"Best-Effort\": apiVersion : v1 kind : ResourceQuota metadata : name : best-effort spec : hard : pods : \"100\" scopes : - BestEffort","title":"Quota Scopes"},{"location":"policies/resource-quota/#resourcequota-por-priorityclass","text":"Se puede limitar tambi\u00e9n el consumo m\u00e1ximo que puede tener un grupo de Pods en funci\u00f3n de su PriorityClass . Ejemplo En este ejemplo los Pods con la stanza spec.priorityClassName: high , no podr\u00e1n consumir en total m\u00e1s de 1000 CPU y 200Gi RAM. Adem\u00e1s no podr\u00e1n ser m\u00e1s de 10 Pods. apiVersion : v1 kind : ResourceQuota metadata : name : pods-high spec : hard : cpu : \"1000\" memory : 200Gi pods : \"10\" scopeSelector : matchExpressions : - operator : In scopeName : PriorityClass values : [ \"high\" ]","title":"ResourceQuota por PriorityClass"},{"location":"policies/resource-quota/#limitrange-vs-resourcequota","text":"A la hora de establecer los consumos m\u00e1ximos de CPU y RAM, los LimitRange permiten definir el m\u00ednimo y m\u00e1ximo permitidos por contenedor de forma individual. Por otra parte, los ResourceQuota permiten definir el m\u00ednimo y m\u00e1ximo de recursos que se pueden consumir en un Namespace, teniendo en cuenta el consumo total de todos los contenedores.","title":"LimitRange vs ResourceQuota"},{"location":"policies/security-policies/","text":"Pod Security Policies Este API object permite definir un conjunto de reglas o condiciones que los Pods deben cumplir para ser admitidos en el sistema. S\u00f3lo los usuarios o los ServiceAccount pueden hacer uso de los PodSecurityPolicy . Para ello, hay que asociarle un rol con el verbo use sobre el nombre de la PodSecurityPolicy . Por ejemplo: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : <role name> rules : - apiGroups : [ 'policy' ] resources : [ 'podsecuritypolicies' ] verbs : [ 'use' ] resourceNames : - <list of policies to authorize> Para poder utilizar los PodSecurityPolicies hay que activar el correspondiente \"Admission Controller\". Al hacerlo hay que tener cuidado, ya que hasta que no se autorice el ServiceAccount por defecto (o el que se requiera) para utilizar alguna pol\u00edtica, no se podr\u00e1n crear Pods en el Namespace. En consecuencia, se recomienda que antes de activar el \"Admission Controller\", se hayan creado algunas PodSecurityPolicies y se hayan autorizado los ServiceAccounts de los Namespace para utilizarlas, o de lo contrario no se podr\u00e1n desplegar Pods en el cluster. Nota Para estandarizar el uso de Security Policies entre diferentes clusters, hay definidos 3 tipos de policy b\u00e1sicos: Privileged (el m\u00e1s permisivo): destinado a ser utilizado por los usuarios administradores. Baseline : destinado para uso general. Restricted (el m\u00e1s restrictivo): destinado a ser utilizado por los Pods/usuarios m\u00e1s expuestos. Cuidado! Actualmente este recurso est\u00e1 cada vez m\u00e1s en desuso, en favor de soluciones de terceros (p.ej OPA Gatekeeper ). Ejemplo Este es el ejemplo de la PodSecurityPolicy m\u00e1s permisiva que se puede crear: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : privileged annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny' Security Context vs Pod Security Policy El Security Context son un conjunto de especificaciones de seguridad que se aplican en tiempo de ejecuci\u00f3n sobre los Pods y sus contenedores. Forman parte de la configuraci\u00f3n de los contenedores del Pod. Por otra parte las Pod Security Policies son un mecanismo del Control Plane de Kubernetes, que se encarga de asegurarse de forma centralizada, que solo se puedan desplegar en el cluster Pods que configuren Security Contexts concretos.","title":"Pod Security Policies"},{"location":"policies/security-policies/#pod-security-policies","text":"Este API object permite definir un conjunto de reglas o condiciones que los Pods deben cumplir para ser admitidos en el sistema. S\u00f3lo los usuarios o los ServiceAccount pueden hacer uso de los PodSecurityPolicy . Para ello, hay que asociarle un rol con el verbo use sobre el nombre de la PodSecurityPolicy . Por ejemplo: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : <role name> rules : - apiGroups : [ 'policy' ] resources : [ 'podsecuritypolicies' ] verbs : [ 'use' ] resourceNames : - <list of policies to authorize> Para poder utilizar los PodSecurityPolicies hay que activar el correspondiente \"Admission Controller\". Al hacerlo hay que tener cuidado, ya que hasta que no se autorice el ServiceAccount por defecto (o el que se requiera) para utilizar alguna pol\u00edtica, no se podr\u00e1n crear Pods en el Namespace. En consecuencia, se recomienda que antes de activar el \"Admission Controller\", se hayan creado algunas PodSecurityPolicies y se hayan autorizado los ServiceAccounts de los Namespace para utilizarlas, o de lo contrario no se podr\u00e1n desplegar Pods en el cluster. Nota Para estandarizar el uso de Security Policies entre diferentes clusters, hay definidos 3 tipos de policy b\u00e1sicos: Privileged (el m\u00e1s permisivo): destinado a ser utilizado por los usuarios administradores. Baseline : destinado para uso general. Restricted (el m\u00e1s restrictivo): destinado a ser utilizado por los Pods/usuarios m\u00e1s expuestos. Cuidado! Actualmente este recurso est\u00e1 cada vez m\u00e1s en desuso, en favor de soluciones de terceros (p.ej OPA Gatekeeper ). Ejemplo Este es el ejemplo de la PodSecurityPolicy m\u00e1s permisiva que se puede crear: apiVersion : policy/v1beta1 kind : PodSecurityPolicy metadata : name : privileged annotations : seccomp.security.alpha.kubernetes.io/allowedProfileNames : '*' spec : privileged : true allowPrivilegeEscalation : true allowedCapabilities : - '*' volumes : - '*' hostNetwork : true hostPorts : - min : 0 max : 65535 hostIPC : true hostPID : true runAsUser : rule : 'RunAsAny' seLinux : rule : 'RunAsAny' supplementalGroups : rule : 'RunAsAny' fsGroup : rule : 'RunAsAny'","title":"Pod Security Policies"},{"location":"policies/security-policies/#security-context-vs-pod-security-policy","text":"El Security Context son un conjunto de especificaciones de seguridad que se aplican en tiempo de ejecuci\u00f3n sobre los Pods y sus contenedores. Forman parte de la configuraci\u00f3n de los contenedores del Pod. Por otra parte las Pod Security Policies son un mecanismo del Control Plane de Kubernetes, que se encarga de asegurarse de forma centralizada, que solo se puedan desplegar en el cluster Pods que configuren Security Contexts concretos.","title":"Security Context vs Pod Security Policy"},{"location":"scheduling/assign-pods/","text":"Asignar Pods a nodos Existen ocasiones en las que se desea restringir los nodos en los que se alojan ciertos Pods, o seleccionar nodos en los que preferimos que sean alojados esos Pods. Por ejemplo, puede ser que se desee que un Pod que se vaya a comunicar mucho con otro se aloje en un nodo dentro de la misma zona de disponibilidad nodeSelector En la definici\u00f3n del propio Pod se puede indicar en qu\u00e9 nodo se desea que sea desplegado, mediante la stanza spec.nodeSelector . Dentro de esta stanza se indicar\u00e1n un conjunto de parejas clave-valor, que contendr\u00e1n las labels que tengan definidas los nodos en los que se desee desplegar (no tienen porqu\u00e9 ser todas las labels que tenga definidas el nodo, s\u00f3lo las que permitan identificarlo). Ejemplo apiVersion : v1 kind : Pod metadata : name : nginx labels : env : test spec : containers : - name : nginx image : nginx imagePullPolicy : IfNotPresent nodeSelector : disktype : ssd # Label del nodo Informaci\u00f3n Alternativamente se puede utilizar la stanza nodeName con el nombre de un nodo en concreto, pero dadas las limitaciones de este m\u00e9todo de seleccionar nodos, no se recomienda. Affinity y anti-affinity A diferencia de la propiedad nodeSelector , las propiedades de afinidad permiten definir reglas de selecci\u00f3n de nodos m\u00e1s complejas. Entre otras opciones, est\u00e1 la de indicar al Scheduler , que las reglas que se definan pueden tomarse como recomendaciones y no como reglas absolutas (permitiendo a un Pod ser desplegado aunque el nodo seleccionado no cumpla las especificaciones). Existen dos tipos de afinidad: node affinitty y pod affinity/anti-affinity . Node affinity Este tipo de afinidad es similar al nodeSelector : te permite limitar en qu\u00e9 nodos debe ser desplegado el Pod, en funci\u00f3n de las labels del nodo. Para definir reglas que deban cumplirse para que un Pod pueda ser desplegado, se utiliza la stanza spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution . Esta stanza contiene la propiedad nodeSelectorTerms que contiene un array de matchExpressions . El Pod s\u00f3lo se desplegar\u00e1 en los nodos que cumplan todas las matchExpressions de alguno de los nodeSelectorTerms . Para definir reglas de preferencia (y no de obligatoriedad como la stanza anterior), se utiliza la stanza spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution . Esta stanza contiene un array en el que se le especifican preference que a su vez contiene un matchExpressions . El Scheduler intentar\u00e1 desplegar el Pod en un nodo que cumpla estas reglas, pero si no encuentra ninguno, intentar\u00e1 desplegarlo en cualquier otro nodo. Notas A diferencia de la stanza nodeSelector , estas stanzas permiten definir reglas m\u00e1s complejas, mediante las operaciones In , NotIn , Exists , DoesNotExists , Gt y Lt , que se aplicar\u00e1n sobre los posibles valores que pueda contener una label concreta. En la definici\u00f3n del Pod se pueden combinar las propiedades spec.nodeSelector y spec.affinity.nodeAffinity , en cuyo caso, ambas condiciones se deben cumplir. Si posteriormente al despliegue de un Pod con nodeAffinity , se elimina alguna de las labels del nodo, el Pod seguir\u00e1 en el mismo nodo, ya que este cambio solo afectar\u00e1 a los nuevos Pods que vayan a ser desplegados. A los valores de preferredDuringSchedulingIgnoredDuringExecution , se les puede definir la propiedad weight para dar m\u00e1s prioridad a unas preferencias que a otras (puede tomar valor 1-100). Contra m\u00e1s alto es el valor definido, m\u00e1s importancia se dar\u00e1 a esa preferencia. Ejemplo apiVersion : v1 kind : Pod metadata : name : with-node-affinity spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name # Desplegar en nodos que tengan esta operator : In # label con alguno de estos valores values : - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : another-node-label-key # Dentro del grupo de nodos anterior, operator : In # dar preferencia a los que tengan values : # esta label con este valor - another-node-label-value containers : - name : with-node-affinity image : k8s.gcr.io/pause:2.0 Pod affinity/anti-affinity Este tipo de afinidad, a diferencia de la anterior, selecciona en qu\u00e9 nodos puede desplegarse un Pod, bas\u00e1ndose en las labels de los Pods que ya hay desplegados en los nodos, en vez de las labels de los propios nodos. De esta manera, se permite evitar desplegar dos instancias de un Pod en un mismo nodo con spec.affinity.podAntiAffinity , o forzar a que un Pod s\u00f3lo pueda desplegarse en aquellos nodos en los que ya exista un Pod concreto con spec.affinity.podAffinity . Al igual que con los node affinity , puede utilizar las stanzas requiredDuringSchedulingIgnoredDuringExecution o preferredDuringSchedulingIgnoredDuringExecution , para definir normas a cumplir o recomendaciones. Para seleccionar los nodos en los que evaluar estas reglas se utiliza la propiedad topologyKey . En esta propiedad se indica la label que deba contener el nodo. Notas En este caso, los operadores que se pueden utilizar son In , NotIn , Exists y DoesNotExists . Se puede indicar un listado de Namespaces con la propiedad namespaces , para evaluar solo ciertos Pods. Ejemplo apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : security operator : In values : - S1 topologyKey : failure-domain.beta.kubernetes.io/zone podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 100 podAffinityTerm : labelSelector : matchExpressions : - key : security operator : In values : - S2 topologyKey : failure-domain.beta.kubernetes.io/zone containers : - name : with-pod-affinity image : k8s.gcr.io/pause:2.0","title":"Asignar Pods a nodos"},{"location":"scheduling/assign-pods/#asignar-pods-a-nodos","text":"Existen ocasiones en las que se desea restringir los nodos en los que se alojan ciertos Pods, o seleccionar nodos en los que preferimos que sean alojados esos Pods. Por ejemplo, puede ser que se desee que un Pod que se vaya a comunicar mucho con otro se aloje en un nodo dentro de la misma zona de disponibilidad","title":"Asignar Pods a nodos"},{"location":"scheduling/assign-pods/#nodeselector","text":"En la definici\u00f3n del propio Pod se puede indicar en qu\u00e9 nodo se desea que sea desplegado, mediante la stanza spec.nodeSelector . Dentro de esta stanza se indicar\u00e1n un conjunto de parejas clave-valor, que contendr\u00e1n las labels que tengan definidas los nodos en los que se desee desplegar (no tienen porqu\u00e9 ser todas las labels que tenga definidas el nodo, s\u00f3lo las que permitan identificarlo). Ejemplo apiVersion : v1 kind : Pod metadata : name : nginx labels : env : test spec : containers : - name : nginx image : nginx imagePullPolicy : IfNotPresent nodeSelector : disktype : ssd # Label del nodo Informaci\u00f3n Alternativamente se puede utilizar la stanza nodeName con el nombre de un nodo en concreto, pero dadas las limitaciones de este m\u00e9todo de seleccionar nodos, no se recomienda.","title":"nodeSelector"},{"location":"scheduling/assign-pods/#affinity-y-anti-affinity","text":"A diferencia de la propiedad nodeSelector , las propiedades de afinidad permiten definir reglas de selecci\u00f3n de nodos m\u00e1s complejas. Entre otras opciones, est\u00e1 la de indicar al Scheduler , que las reglas que se definan pueden tomarse como recomendaciones y no como reglas absolutas (permitiendo a un Pod ser desplegado aunque el nodo seleccionado no cumpla las especificaciones). Existen dos tipos de afinidad: node affinitty y pod affinity/anti-affinity .","title":"Affinity y anti-affinity"},{"location":"scheduling/assign-pods/#node-affinity","text":"Este tipo de afinidad es similar al nodeSelector : te permite limitar en qu\u00e9 nodos debe ser desplegado el Pod, en funci\u00f3n de las labels del nodo. Para definir reglas que deban cumplirse para que un Pod pueda ser desplegado, se utiliza la stanza spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution . Esta stanza contiene la propiedad nodeSelectorTerms que contiene un array de matchExpressions . El Pod s\u00f3lo se desplegar\u00e1 en los nodos que cumplan todas las matchExpressions de alguno de los nodeSelectorTerms . Para definir reglas de preferencia (y no de obligatoriedad como la stanza anterior), se utiliza la stanza spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution . Esta stanza contiene un array en el que se le especifican preference que a su vez contiene un matchExpressions . El Scheduler intentar\u00e1 desplegar el Pod en un nodo que cumpla estas reglas, pero si no encuentra ninguno, intentar\u00e1 desplegarlo en cualquier otro nodo. Notas A diferencia de la stanza nodeSelector , estas stanzas permiten definir reglas m\u00e1s complejas, mediante las operaciones In , NotIn , Exists , DoesNotExists , Gt y Lt , que se aplicar\u00e1n sobre los posibles valores que pueda contener una label concreta. En la definici\u00f3n del Pod se pueden combinar las propiedades spec.nodeSelector y spec.affinity.nodeAffinity , en cuyo caso, ambas condiciones se deben cumplir. Si posteriormente al despliegue de un Pod con nodeAffinity , se elimina alguna de las labels del nodo, el Pod seguir\u00e1 en el mismo nodo, ya que este cambio solo afectar\u00e1 a los nuevos Pods que vayan a ser desplegados. A los valores de preferredDuringSchedulingIgnoredDuringExecution , se les puede definir la propiedad weight para dar m\u00e1s prioridad a unas preferencias que a otras (puede tomar valor 1-100). Contra m\u00e1s alto es el valor definido, m\u00e1s importancia se dar\u00e1 a esa preferencia. Ejemplo apiVersion : v1 kind : Pod metadata : name : with-node-affinity spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name # Desplegar en nodos que tengan esta operator : In # label con alguno de estos valores values : - e2e-az1 - e2e-az2 preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : another-node-label-key # Dentro del grupo de nodos anterior, operator : In # dar preferencia a los que tengan values : # esta label con este valor - another-node-label-value containers : - name : with-node-affinity image : k8s.gcr.io/pause:2.0","title":"Node affinity"},{"location":"scheduling/assign-pods/#pod-affinityanti-affinity","text":"Este tipo de afinidad, a diferencia de la anterior, selecciona en qu\u00e9 nodos puede desplegarse un Pod, bas\u00e1ndose en las labels de los Pods que ya hay desplegados en los nodos, en vez de las labels de los propios nodos. De esta manera, se permite evitar desplegar dos instancias de un Pod en un mismo nodo con spec.affinity.podAntiAffinity , o forzar a que un Pod s\u00f3lo pueda desplegarse en aquellos nodos en los que ya exista un Pod concreto con spec.affinity.podAffinity . Al igual que con los node affinity , puede utilizar las stanzas requiredDuringSchedulingIgnoredDuringExecution o preferredDuringSchedulingIgnoredDuringExecution , para definir normas a cumplir o recomendaciones. Para seleccionar los nodos en los que evaluar estas reglas se utiliza la propiedad topologyKey . En esta propiedad se indica la label que deba contener el nodo. Notas En este caso, los operadores que se pueden utilizar son In , NotIn , Exists y DoesNotExists . Se puede indicar un listado de Namespaces con la propiedad namespaces , para evaluar solo ciertos Pods. Ejemplo apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : security operator : In values : - S1 topologyKey : failure-domain.beta.kubernetes.io/zone podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 100 podAffinityTerm : labelSelector : matchExpressions : - key : security operator : In values : - S2 topologyKey : failure-domain.beta.kubernetes.io/zone containers : - name : with-pod-affinity image : k8s.gcr.io/pause:2.0","title":"Pod affinity/anti-affinity"},{"location":"scheduling/scheduler/","text":"Kubernetes Scheduler El Scheduler es el encargado de observar los Pods reci\u00e9n creados, que no tengan nodo asignado, y de emplazarlos en un nodo. La decisi\u00f3n de emplazar los Pods en un nodo u otro se hace en base a unos criterios que tienen en cuenta las necesidades de cada uno de los contenedores del Pod, as\u00ed como las propias necesidades que pueda tener el Pod. Estas necesidades que influyen en la decisi\u00f3n del Scheduler pueden ser por ejemplo: Recursos requeridos Pol\u00edticas que restrinjan cierto software o hardware Especificaciones de afinidad y anti-afinidad Localizaci\u00f3n de los datos Interferencias entre workloads Proceso de selecci\u00f3n de nodo El proceso de selecci\u00f3n de nodo para un Pod, consta de los siguientes pasos: Filtrado : se filtra el listado de nodos, quedando s\u00f3lo aquellos en los que sea factible desplegar el Pod. Por ejemplo, aquellos nodos que tengan suficientes recursos disponibles para alojar el Pod. Scoring : se punt\u00faa cada uno de los nodos en funci\u00f3n de las reglas de scheduling definidas y se ordenan, para acabar eligiendo el primer nodo de la lista (si hay m\u00e1s de un nodo con la m\u00e1xima puntuaci\u00f3n, se elige uno de ellos al azar). Binding : el Scheduler notifica al servidor API de Kubernetes, la decisi\u00f3n tomada, para que este despliegue el Pod en el nodo seleccionado. Existen dos formas de configurar el comportamiento de \"filtrado\" y \" scoring \": Mediante Scheduling Policies , se pueden definir Predicates para el \"filtrado\" y Priorities para el \" scoring \". Mediante Scheduling Profiles , se pueden configurar plugins, que permiten definir diferentes scheduling stages (p.ej. QueueSort , Filter , Score , Bind , Reserve ...).","title":"Kubernetes Scheduler"},{"location":"scheduling/scheduler/#kubernetes-scheduler","text":"El Scheduler es el encargado de observar los Pods reci\u00e9n creados, que no tengan nodo asignado, y de emplazarlos en un nodo. La decisi\u00f3n de emplazar los Pods en un nodo u otro se hace en base a unos criterios que tienen en cuenta las necesidades de cada uno de los contenedores del Pod, as\u00ed como las propias necesidades que pueda tener el Pod. Estas necesidades que influyen en la decisi\u00f3n del Scheduler pueden ser por ejemplo: Recursos requeridos Pol\u00edticas que restrinjan cierto software o hardware Especificaciones de afinidad y anti-afinidad Localizaci\u00f3n de los datos Interferencias entre workloads","title":"Kubernetes Scheduler"},{"location":"scheduling/scheduler/#proceso-de-seleccion-de-nodo","text":"El proceso de selecci\u00f3n de nodo para un Pod, consta de los siguientes pasos: Filtrado : se filtra el listado de nodos, quedando s\u00f3lo aquellos en los que sea factible desplegar el Pod. Por ejemplo, aquellos nodos que tengan suficientes recursos disponibles para alojar el Pod. Scoring : se punt\u00faa cada uno de los nodos en funci\u00f3n de las reglas de scheduling definidas y se ordenan, para acabar eligiendo el primer nodo de la lista (si hay m\u00e1s de un nodo con la m\u00e1xima puntuaci\u00f3n, se elige uno de ellos al azar). Binding : el Scheduler notifica al servidor API de Kubernetes, la decisi\u00f3n tomada, para que este despliegue el Pod en el nodo seleccionado. Existen dos formas de configurar el comportamiento de \"filtrado\" y \" scoring \": Mediante Scheduling Policies , se pueden definir Predicates para el \"filtrado\" y Priorities para el \" scoring \". Mediante Scheduling Profiles , se pueden configurar plugins, que permiten definir diferentes scheduling stages (p.ej. QueueSort , Filter , Score , Bind , Reserve ...).","title":"Proceso de selecci\u00f3n de nodo"},{"location":"scheduling/taints-tolerations/","text":"Taints & Tolerations Mientras que con las propiedades de node affinity de los Pods, se especifica en qu\u00e9 nodos pueden ser desplegados, con los taints y tolerations es lo contrario: se configura el nodo para que repela todos los Pods que no cumplan unas reglas. Por un lado est\u00e1n los Taints que son labels que se definen en el nodo para repeler Pods. Por otro lados, en los Pods se definen Tolerations que permitan desplegar al Pod en los nodos que tengan definidas unas Taints concretas (esto no implica que los Pods deban ser desplegados en esos nodos). Para a\u00f1adir un taint a un nodo: kubectl taint nodes node1 key = value:NoSchedule Junto al label se indica el taint effect . Esto indica el comportamiento asociado a ese taint . Existen 3 tipos: NoSchedule : No se permite alojar Pods que no tengan definida una toleration para este taint . PreferNoSchedule : El Scheduler intentar\u00e1 que no puedan desplegarse Pods si no tienen la correspondiente toleration definida, pero puede asegurarse. NoExecute : Igual que NoSchedule , pero no s\u00f3lo afecta a los nuevos Pods. Si hubiera alg\u00fan Pod en el nodo, desplegado antes de que se defina el taint , ser\u00e1 desalojado si no tiene definida la toleration correspondiente. Para eliminar un taint de un nodo: kubectl taint nodes node1 key:NoSchedule- Ejemplo de toleration apiVersion : v1 kind : Pod metadata : name : nginx labels : env : test spec : containers : - name : nginx image : nginx imagePullPolicy : IfNotPresent tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\" Notas Las toleration s\u00f3lo admiten las operaciones Exists y Equal . Se puede definir una propiedad tolerationSeconds a las tolerations de tipo NoExecute para definir el tiempo que dure la toleration . Pasado ese tiempo el Pod ser\u00e1 desalojado del nodo. El cluster puede crear taints especiales en los nodos, en funci\u00f3n de su estado (p.ej. out-of-disk , unschedulable ...).","title":"Taints & Tolerations"},{"location":"scheduling/taints-tolerations/#taints-tolerations","text":"Mientras que con las propiedades de node affinity de los Pods, se especifica en qu\u00e9 nodos pueden ser desplegados, con los taints y tolerations es lo contrario: se configura el nodo para que repela todos los Pods que no cumplan unas reglas. Por un lado est\u00e1n los Taints que son labels que se definen en el nodo para repeler Pods. Por otro lados, en los Pods se definen Tolerations que permitan desplegar al Pod en los nodos que tengan definidas unas Taints concretas (esto no implica que los Pods deban ser desplegados en esos nodos). Para a\u00f1adir un taint a un nodo: kubectl taint nodes node1 key = value:NoSchedule Junto al label se indica el taint effect . Esto indica el comportamiento asociado a ese taint . Existen 3 tipos: NoSchedule : No se permite alojar Pods que no tengan definida una toleration para este taint . PreferNoSchedule : El Scheduler intentar\u00e1 que no puedan desplegarse Pods si no tienen la correspondiente toleration definida, pero puede asegurarse. NoExecute : Igual que NoSchedule , pero no s\u00f3lo afecta a los nuevos Pods. Si hubiera alg\u00fan Pod en el nodo, desplegado antes de que se defina el taint , ser\u00e1 desalojado si no tiene definida la toleration correspondiente. Para eliminar un taint de un nodo: kubectl taint nodes node1 key:NoSchedule- Ejemplo de toleration apiVersion : v1 kind : Pod metadata : name : nginx labels : env : test spec : containers : - name : nginx image : nginx imagePullPolicy : IfNotPresent tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\" Notas Las toleration s\u00f3lo admiten las operaciones Exists y Equal . Se puede definir una propiedad tolerationSeconds a las tolerations de tipo NoExecute para definir el tiempo que dure la toleration . Pasado ese tiempo el Pod ser\u00e1 desalojado del nodo. El cluster puede crear taints especiales en los nodos, en funci\u00f3n de su estado (p.ej. out-of-disk , unschedulable ...).","title":"Taints &amp; Tolerations"},{"location":"security/cloud-security/","text":"Cloud Native Security Las 4 C's de la seguridad Cloud Native Este modelo divide la seguridad en cuatro capas diferentes. Cada una de ellas se sustenta en las capas m\u00e1s exteriores del modelo, de tal forma que no tiene sentido enforcar grandes esfuerzos en securizar una de las capas m\u00e1s internas, si las externas est\u00e1n mas expuestas a ataques. Cloud Esta capa representa toda la infraestructura en la que se sustenta la computaci\u00f3n de Kubernetes, ya sea en un Data Center propio o en un proveedor Cloud. Si esta capa no es segura, el resto van a ser vulnerables a ataques. Recomendaciones Concepto a revisar Recomendaci\u00f3n Acceso red al API Server El acceso al Control Plane de Kubernetes no deber\u00eda ser p\u00fablico. Habr\u00eda que permitir el acceso solo desde un listado de IP's concreto Acceso red a los nodos A ser posible los nodos no deber\u00edan tener acceso p\u00fablico. Deber\u00edan permitir \u00fanicamente conexiones a los puertos que requiera el Control Plane y las conexiones de los Service de tipo NodePort y LoadBalancer . Acceso de Kubernetes a la API del Cloud Provider Limitar el acceso que tiene Kubernetes \u00fanicamente a los recursos del proveedor Cloud que requiere gestionar. Acceso al etcd El acceso al etcd debe estar limitado \u00fanicamente al Control Plane de Kubernetes. Dependiendo de la configuraci\u00f3n, el etcd se deber\u00eda ser accedido via HTTPS. Encriptaci\u00f3n del etcd Dado que el etcd almacena el estado de los API objects (incluyendo los Secrets ), se deber\u00edan de encriptar los datos ( encryption at rest ). Cluster A nivel de cluster hay dos areas de actuaci\u00f3n: Securizar los componentes del cluster que son configurables. Securizar las aplicaciones que se ejecutan en el cluster Securizar componentes del cluster Recomendaciones API de Kubernetes | Recomendaciones | | ------------------ | | Realizar todas las conexiones a la API de Kubernetes a trav\u00e9s de HTTPS | | Habilitar un mecanismo de autenticaci\u00f3n para hacer uso de la API | | Habilitar un mecanismo de autorizaci\u00f3n para configurar qu\u00e9 endpoints de la API se pueden utilizar | Kubelet | Recomendaciones | | ------------------ | | Por defecto el kubelet permite realizar llamadas a la API sin necesidad de autenticarse. Habr\u00eda que habilitar un mecanismo de autenticaci\u00f3n y autorizaci\u00f3n para poder hacer uso de la API del kubelet | Capabilities en tiempo de ejecuci\u00f3n | Recomendaciones | | ------------------ | | Limitar el uso de recursos del cluster mediante Resource Quota y Limit Ranges | | Controlar mediante Security Context o Security Policies , los privilegios con los que se ejecutan los contenedores | | Evitar que el kernel cargue m\u00f3dulos innecesarios que puedan suponer una vulnerabilidad. Esto se puede evitar por ejemplo con SELinux | | Restringir el acceso a red mediante Network Policies | | Limitar el acceso a la API de de metadatos del Cloud, para que solo ciertos Pods tengan acceso a los datos sensibles | | Controlar a qu\u00e9 nodos tienen acceso los Pods (en qu\u00e9 nodos pueden desplegarse), mediante el uso de taints o nodeSelectors . | Securizar componentes de las aplicaciones Recomendaciones Concepto a revisar Recomendaci\u00f3n Acceso a la API de Kubernetes Limitar mediante la autorizaci\u00f3n RBAC, qu\u00e9 operaciones pueden realizar ciertos usuarios o aplicaciones, sobre los recursos de Kubernetes (API objects). Autenticaci\u00f3n Gestionar correctamente la autenticaci\u00f3n de los usuarios (dado que no se almacenan datos del usuario, no existe un API object que lo represente) y de las aplicaciones ( Service Accounts ). Gestionar datos sensibles de las aplicaciones Almacenar datos sensibles de las aplicaciones en Secrets y encriptarlos en el etcd ( encryption at rest ). Admisi\u00f3n de Pods Crear Pod Security Policies que definan qu\u00e9 reglas deben cumplir los Pods para poder ser admitidos en el sistema. Recursos de los Pods Limitar el uso de recursos que pueden hacer los Pods mediante request y limits , y establecer diferentes clases de QoS en funci\u00f3n de la criticidad del Pod. Acceso red de los Pods Limitar mediante Network Policies las conexiones que pueden admitir y/o realizar los Pods. Securizar Ingress Exponer el Ingress via HTTPS para securizar las conexiones que se realicen desde el exterior del cluster. Container Recomendaciones Concepto a revisar Recomendaci\u00f3n Vulnerabilidades del contenedor y dependencias del Sistema Operativo En el proceso de construcci\u00f3n de la im\u00e1gen, escanear el contenedor en b\u00fasqueda de vulnerabilidades. Firmar im\u00e1genes Firmar las im\u00e1genes de contenedor, para confiar \u00fanicamente en los proveedores de im\u00e1genes de confianza. Usuarios con privilegios Limitar los permisos que tienen los usuarios de los contenedores, para que s\u00f3lo puedan ejecutar las operaciones necesarias para el correcto funcionamiento de las aplicaciones. Code Recomendaciones Concepto a revisar Recomendaci\u00f3n Securizar conexiones Las aplicaciones s\u00f3lo deber\u00edan establecer conexiones a trav\u00e9s de TLS. Adicionalmente, se puede encriptar el tr\u00e1fico de red entre dos servicios con mTLS. Exposici\u00f3n de puertos Limitar el rango de puertos expuestos, \u00fanicamente a aquellos que sean esencialmente necesarios para que opere la aplicaci\u00f3n o para que sea monitorizada. Vulnerabilidades en librer\u00edas de terceros Escanear regularmente las librer\u00edas de terceros en b\u00fasqueda de vulnerabilidades. An\u00e1lisis de c\u00f3digo est\u00e1tico Mediante una herramienta de an\u00e1lisis de c\u00f3digo est\u00e1tico, analizar el c\u00f3digo fuente en b\u00fasqueda de malas practicas. Ataques din\u00e1micos Utilizar herramientas para automatizar pruebas de ataques din\u00e1micos contra las aplicaciones, tales como SQL Injection, CSR o XSS.","title":"Cloud Native Security"},{"location":"security/cloud-security/#cloud-native-security","text":"","title":"Cloud Native Security"},{"location":"security/cloud-security/#las-4-cs-de-la-seguridad-cloud-native","text":"Este modelo divide la seguridad en cuatro capas diferentes. Cada una de ellas se sustenta en las capas m\u00e1s exteriores del modelo, de tal forma que no tiene sentido enforcar grandes esfuerzos en securizar una de las capas m\u00e1s internas, si las externas est\u00e1n mas expuestas a ataques.","title":"Las 4 C's de la seguridad Cloud Native"},{"location":"security/cloud-security/#cloud","text":"Esta capa representa toda la infraestructura en la que se sustenta la computaci\u00f3n de Kubernetes, ya sea en un Data Center propio o en un proveedor Cloud. Si esta capa no es segura, el resto van a ser vulnerables a ataques. Recomendaciones Concepto a revisar Recomendaci\u00f3n Acceso red al API Server El acceso al Control Plane de Kubernetes no deber\u00eda ser p\u00fablico. Habr\u00eda que permitir el acceso solo desde un listado de IP's concreto Acceso red a los nodos A ser posible los nodos no deber\u00edan tener acceso p\u00fablico. Deber\u00edan permitir \u00fanicamente conexiones a los puertos que requiera el Control Plane y las conexiones de los Service de tipo NodePort y LoadBalancer . Acceso de Kubernetes a la API del Cloud Provider Limitar el acceso que tiene Kubernetes \u00fanicamente a los recursos del proveedor Cloud que requiere gestionar. Acceso al etcd El acceso al etcd debe estar limitado \u00fanicamente al Control Plane de Kubernetes. Dependiendo de la configuraci\u00f3n, el etcd se deber\u00eda ser accedido via HTTPS. Encriptaci\u00f3n del etcd Dado que el etcd almacena el estado de los API objects (incluyendo los Secrets ), se deber\u00edan de encriptar los datos ( encryption at rest ).","title":"Cloud"},{"location":"security/cloud-security/#cluster","text":"A nivel de cluster hay dos areas de actuaci\u00f3n: Securizar los componentes del cluster que son configurables. Securizar las aplicaciones que se ejecutan en el cluster","title":"Cluster"},{"location":"security/cloud-security/#securizar-componentes-del-cluster","text":"Recomendaciones API de Kubernetes | Recomendaciones | | ------------------ | | Realizar todas las conexiones a la API de Kubernetes a trav\u00e9s de HTTPS | | Habilitar un mecanismo de autenticaci\u00f3n para hacer uso de la API | | Habilitar un mecanismo de autorizaci\u00f3n para configurar qu\u00e9 endpoints de la API se pueden utilizar | Kubelet | Recomendaciones | | ------------------ | | Por defecto el kubelet permite realizar llamadas a la API sin necesidad de autenticarse. Habr\u00eda que habilitar un mecanismo de autenticaci\u00f3n y autorizaci\u00f3n para poder hacer uso de la API del kubelet | Capabilities en tiempo de ejecuci\u00f3n | Recomendaciones | | ------------------ | | Limitar el uso de recursos del cluster mediante Resource Quota y Limit Ranges | | Controlar mediante Security Context o Security Policies , los privilegios con los que se ejecutan los contenedores | | Evitar que el kernel cargue m\u00f3dulos innecesarios que puedan suponer una vulnerabilidad. Esto se puede evitar por ejemplo con SELinux | | Restringir el acceso a red mediante Network Policies | | Limitar el acceso a la API de de metadatos del Cloud, para que solo ciertos Pods tengan acceso a los datos sensibles | | Controlar a qu\u00e9 nodos tienen acceso los Pods (en qu\u00e9 nodos pueden desplegarse), mediante el uso de taints o nodeSelectors . |","title":"Securizar componentes del cluster"},{"location":"security/cloud-security/#securizar-componentes-de-las-aplicaciones","text":"Recomendaciones Concepto a revisar Recomendaci\u00f3n Acceso a la API de Kubernetes Limitar mediante la autorizaci\u00f3n RBAC, qu\u00e9 operaciones pueden realizar ciertos usuarios o aplicaciones, sobre los recursos de Kubernetes (API objects). Autenticaci\u00f3n Gestionar correctamente la autenticaci\u00f3n de los usuarios (dado que no se almacenan datos del usuario, no existe un API object que lo represente) y de las aplicaciones ( Service Accounts ). Gestionar datos sensibles de las aplicaciones Almacenar datos sensibles de las aplicaciones en Secrets y encriptarlos en el etcd ( encryption at rest ). Admisi\u00f3n de Pods Crear Pod Security Policies que definan qu\u00e9 reglas deben cumplir los Pods para poder ser admitidos en el sistema. Recursos de los Pods Limitar el uso de recursos que pueden hacer los Pods mediante request y limits , y establecer diferentes clases de QoS en funci\u00f3n de la criticidad del Pod. Acceso red de los Pods Limitar mediante Network Policies las conexiones que pueden admitir y/o realizar los Pods. Securizar Ingress Exponer el Ingress via HTTPS para securizar las conexiones que se realicen desde el exterior del cluster.","title":"Securizar componentes de las aplicaciones"},{"location":"security/cloud-security/#container","text":"Recomendaciones Concepto a revisar Recomendaci\u00f3n Vulnerabilidades del contenedor y dependencias del Sistema Operativo En el proceso de construcci\u00f3n de la im\u00e1gen, escanear el contenedor en b\u00fasqueda de vulnerabilidades. Firmar im\u00e1genes Firmar las im\u00e1genes de contenedor, para confiar \u00fanicamente en los proveedores de im\u00e1genes de confianza. Usuarios con privilegios Limitar los permisos que tienen los usuarios de los contenedores, para que s\u00f3lo puedan ejecutar las operaciones necesarias para el correcto funcionamiento de las aplicaciones.","title":"Container"},{"location":"security/cloud-security/#code","text":"Recomendaciones Concepto a revisar Recomendaci\u00f3n Securizar conexiones Las aplicaciones s\u00f3lo deber\u00edan establecer conexiones a trav\u00e9s de TLS. Adicionalmente, se puede encriptar el tr\u00e1fico de red entre dos servicios con mTLS. Exposici\u00f3n de puertos Limitar el rango de puertos expuestos, \u00fanicamente a aquellos que sean esencialmente necesarios para que opere la aplicaci\u00f3n o para que sea monitorizada. Vulnerabilidades en librer\u00edas de terceros Escanear regularmente las librer\u00edas de terceros en b\u00fasqueda de vulnerabilidades. An\u00e1lisis de c\u00f3digo est\u00e1tico Mediante una herramienta de an\u00e1lisis de c\u00f3digo est\u00e1tico, analizar el c\u00f3digo fuente en b\u00fasqueda de malas practicas. Ataques din\u00e1micos Utilizar herramientas para automatizar pruebas de ataques din\u00e1micos contra las aplicaciones, tales como SQL Injection, CSR o XSS.","title":"Code"},{"location":"security/rbac/","text":"RBAC Role-based Access Control (RBAC) es uno de los m\u00e9todos de autorizaci\u00f3n que admite Kubernetes. Informaci\u00f3n Para activar este m\u00e9todo de autorizaci\u00f3n, debe indicarse al iniciar el kube-apiserver , a\u00f1adi\u00e9ndolo en el listado de m\u00e9todos habilitados en el flag --authorization-mode . Role y ClusterRole Los API objects Role y ClusterRole , definen el conjunto de reglas (permisos) que se garantizan en el rol. Los Role se crean dentro de un Namespace y definen qu\u00e9 permisos se tiene sobre los recursos de \u00e9se mismo Namespace. Por otro lado los ClusterRole , no se definen dentro de un Namespace, y sirven para definir qu\u00e9 permisos se tiene sobre los recursos de un Namespace concreto o de todos los Namespace. Adicionalmente sirve para definir permisos sobre los recursos que no pertenecen a un Namespace (p.ej. PersistentVolume ) y non-resource endpoints , es decir endpoints de la API de Kubernetes (p.ej. /healthz ). Ejemplo Role que define permisos de lectura sobre los Pods del Namespace default : apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] ClusterRole que define permisos de lectura para todos los Secrets del cluster: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : # \"namespace\" omitted since ClusterRoles are not namespaced name : secret-reader rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - nonResourceURLs : [ \"/healthz\" , \"/healthz/*\" ] # '*' in a nonResourceURL is a suffix glob match verbs : [ \"get\" , \"post\" ] Referenciar Recursos Mediante la stanza rules[].resources se indican los recursos sobre los cuales se dar\u00e1 permisos a un usuario. Esta stanza debe ir acompa\u00f1ada de rules[].apiGroups para indicar a qu\u00e9 grupo API pertenecen los recursos indicados. Se puede ver un listado b\u00e1sico de los recursos sobre los que se pueden aplicar permisos, ejecutando kubectl api-resources | awk '{print $1}' . Hay que tener en cuenta que en el listado no aparecen CRD's (Custom Resource Definition). En ocasiones existen subrecursos, como por ejemplo los logs de un Pod. En estos casos el nombre del recurso estar\u00e1 compuesto por el recurso \"padre\" seguido del recurso \"hijo\", separados por / (p.ej. pods/logs ). Adem\u00e1s de por tipo de recurso, se pueden dar permisos sobre recursos concretos que ya est\u00e9n creados, referenci\u00e1ndolos por su nombre. Esto se consigue mediante la stanza rules[].resourceNames . Advertencia Al referenciar por el nombre de un recurso, no se pueden limitar los permisos create y deletecollection , Ejemplo apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : configmap-updater rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"my-configmap\" ] verbs : [ \"update\" , \"get\" ] Determinar permisos En funci\u00f3n del recurso sobre el que se quieran dar permisos, las acciones permitidas pueden ser unas u otras. En la mayor\u00eda de casos se podr\u00e1n dar permisos para realizar las siguientes acciones: HTTP Verb Verb Descripci\u00f3n GET, HEAD get list watch Obtener un recurso individual Obtener un conjunto de recursos, incluyendo su contenido Observar un recurso individual o un conjunto POST create Crear un nuevo recurso PUT update Actualizar un recurso PATCH patch Editar un recurso (mediante kubectl patch ) DELETE delete deletecollection Eliminar un recurso individual Eliminar un conjunto de recursos Adicionalmente existen los siguientes verbs : use : para los recursos podsecuritypolicies . bind y escalate : para los recursos roles y clusterroles . impersonate : para los recursos users , groups , serviceaccounts y userextras . get , post , put y delete : son los verbs que pueden utilizar los non-resource endpoints . ClusterRoles agregados Se pueden agregar m\u00faltiples ClusterRoles en uno solo, mediante el uso de Labels y Selectors. Mediante la stanza aggregationRule se puede definir un Selector para seleccionar el conjunto de ClusterRoles que se utilizar\u00e1 para definir los permisos del ClusterRole agregado. Este tipo de ClusterRole normalmente tienen definida la stanza rules vac\u00eda, dado que un Controller de Kubernetes se encarga de de rellenar la propiedad con los permisos obtenidos de todos los ClusterRoles seleccionados. Ejemplo ClusterRole agregado: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring aggregationRule : clusterRoleSelectors : - matchLabels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : [] # The control plane automatically fills in the rules ClusterRole que seleccionar\u00e1 el ClusterRole agregado: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-endpoints labels : rbac.example.com/aggregate-to-monitoring : \"true\" # When you create the \"monitoring-endpoints\" ClusterRole, # the rules below will be added to the \"monitoring\" ClusterRole. rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] RoleBinding y ClusterRoleBinding Los RoleBinding y ClusterRoleBinding conceden a un usuario o conjunto de usuarios, los permisos que haya definidos en un rol. Estos recursos de Kubernetes contienen una referencia a un rol y un listado de sujetos a los que se les aplican los permisos (pueden ser usuarios, grupos o ServiceAccount ). Los RoleBinding pueden referenciar cualquier Role de su mismo Namespace. Tambi\u00e9n pueden referenciar a un ClusterRole , pero s\u00f3lo se aplicar\u00e1n los permisos del rol sobre los recursos del Namespace del RoleBinding . Esto puede ser \u00fatil para definir un rol que vaya a ser reutilizado para varios Namespace. Para que los permisos asignados apliquen para todos los Namespace del cluster hay que utilizar un ClusterRoleBinding que haga referencia a un ClusterRole . Notas Los usuarios o grupos pueden tener cualquier formato (texto plano, email...), pero no pueden utilizar el prefijo system: dado que est\u00e1 reservado para elementos de Kubernetes. Los ServiceAccount llevan siempre el prefijo system:serviceaccount: y los grupos de ServiceAccount el prefijo system:serviceaccounts: . Advertencia Una vez creado el RoleBinding / ClusterRoleBinding la stanza roleRef no puede ser modificada. Si se desea modificar hay que eliminar el API object y crear uno nuevo. Lo \u00fanico que se puede modificar del objecto, es el listado de sujetos de la stanza subjects , a los que se les aplican los permisos del rol. Ejemplo RoleBinding que permite al usuario jane , al ServiceAccount default del Namespace kube-system y a todos los ServiceAccount del Namespace qa , leer Pods en el Namespace default : # You need to already have a Role named \"pod-reader\" in that namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-pods namespace : default subjects : - kind : User name : jane apiGroup : rbac.authorization.k8s.io - kind : ServiceAccount name : default namespace : kube-system - kind : Group name : system:serviceaccounts:qa apiGroup : rbac.authorization.k8s.io roleRef : kind : Role #this must be Role or ClusterRole name : pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup : rbac.authorization.k8s.io RoleBinding que permite al usuario dave leer los Secret del Namespace default : # You need to already have a ClusterRole named \"secret-reader\". apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-secrets # This only grants permissions within the \"development\" namespace. namespace : development subjects : - kind : User name : dave apiGroup : rbac.authorization.k8s.io roleRef : kind : ClusterRole name : secret-reader apiGroup : rbac.authorization.k8s.io ClusterRoleBinding que permite a todos los usuarios del grupo manager leer los Secret de cualquier Namespace: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-secrets-global subjects : - kind : Group name : manager # Name is case sensitive apiGroup : rbac.authorization.k8s.io roleRef : kind : ClusterRole name : secret-reader apiGroup : rbac.authorization.k8s.io Prevenci\u00f3n ante escalada de privilegios La API de RBAC previene que los usuarios puedan escalar sus privilegios, modificando los roles o los role bindings . Cuidado Si el kube-apiserver tiene el puerto inseguro habilitado, cualquier petici\u00f3n a la API, enviada a ese puerto, ser\u00e1 aceptada sin pasar ning\u00fan proceso de autenticaci\u00f3n o autorizaci\u00f3n. Restricciones en la creaci\u00f3n/actualizaci\u00f3n de roles Un usuario puede crear/actualizar un rol s\u00f3lo si se cumple alguna de las siguientes condiciones: El usuario ya tiene asignados los permisos que quiere modificar del rol y al mismo nivel (a nivel de cluster si es un ClusterRole , y a nivel de cluster o a nivel del mismo Namespace si es un Role ). Es decir, el usuario s\u00f3lo puede crear/editar roles con los mismos permisos que posea. El usuario tiene expl\u00edcitamente el permiso escalate sobre los recursos roles o clusterroles . Restricciones en la creaci\u00f3n/actualizaci\u00f3n de role bindings Un usuario puede crear/actualizar un rol binding s\u00f3lo si se cumple alguna de las siguientes condiciones: El usuario tiene asignados los mismos permisos que el rol asociado al role binding que pretende modificar y al mismo nivel (a nivel de cluster si es un ClusterRole , y a nivel de cluster o a nivel del mismo Namespace si es un Role ). El usuario tiene expl\u00edcitamente el permiso bind sobre los recursos roles o clusterroles . Declaraci\u00f3n imperativa con kubectl Crear un Role kubectl create role pod-reader --verb = get,list,watch --resource = pods --resource-name = my-pod --resource-name = other-pod Crear un ClusterRole kubectl create clusterrole replicaset-reader --verb = get,list,watch --resource = replicasets.apps Para dar permisos sobre un endpoint de la API: kubectl create clusterrole logs-creator --verb = get,post --non-resource-url = /logs/* Para a\u00f1adir una aggregationRule : kubectl create clusterrole monitoring --aggregation-rule = \"rbac.example.com/aggregate-to-monitoring=true\" Crear un RoleBinding kubectl create rolebinding bob-admin-binding --role = acme-admin --user = bob --namespace = acme Para dar permisos de lectura sobre el Namespace \"acme\", a un ServiceAccount de un Namespace \"myappnamespace\": kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole = view --serviceaccount = myappnamespace:myapp --namespace = acme Crear un ClusterRoleBinding kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole = cluster-admin --user = root Para dar permisos de lectura sobre todos los Namespace, a un ServiceAccount de un Namespace \"acme\": kubectl create clusterrolebinding myapp-view-binding --clusterrole = view --serviceaccount = acme:myapp Actualizar cualquier objeto de la API rbac.authorization.k8s.io/v1 Actualizar objetos RBAC a partir de un manifest, manteniendo los permisos y sujetos extra que pudieran ya tener los objetos: kubectl auth reconcile -f my-rbac-rules.yaml Actualizar objetos RBAC a partir de un manifest, eliminando los permisos y sujetos que no aparezcan en el manifest: kubectl auth reconcile -f my-rbac-rules.yaml --remove-extra-subjects --remove-extra-permissions","title":"RBAC"},{"location":"security/rbac/#rbac","text":"Role-based Access Control (RBAC) es uno de los m\u00e9todos de autorizaci\u00f3n que admite Kubernetes. Informaci\u00f3n Para activar este m\u00e9todo de autorizaci\u00f3n, debe indicarse al iniciar el kube-apiserver , a\u00f1adi\u00e9ndolo en el listado de m\u00e9todos habilitados en el flag --authorization-mode .","title":"RBAC"},{"location":"security/rbac/#role-y-clusterrole","text":"Los API objects Role y ClusterRole , definen el conjunto de reglas (permisos) que se garantizan en el rol. Los Role se crean dentro de un Namespace y definen qu\u00e9 permisos se tiene sobre los recursos de \u00e9se mismo Namespace. Por otro lado los ClusterRole , no se definen dentro de un Namespace, y sirven para definir qu\u00e9 permisos se tiene sobre los recursos de un Namespace concreto o de todos los Namespace. Adicionalmente sirve para definir permisos sobre los recursos que no pertenecen a un Namespace (p.ej. PersistentVolume ) y non-resource endpoints , es decir endpoints de la API de Kubernetes (p.ej. /healthz ). Ejemplo Role que define permisos de lectura sobre los Pods del Namespace default : apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] ClusterRole que define permisos de lectura para todos los Secrets del cluster: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : # \"namespace\" omitted since ClusterRoles are not namespaced name : secret-reader rules : - apiGroups : [ \"\" ] resources : [ \"secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - nonResourceURLs : [ \"/healthz\" , \"/healthz/*\" ] # '*' in a nonResourceURL is a suffix glob match verbs : [ \"get\" , \"post\" ]","title":"Role y ClusterRole"},{"location":"security/rbac/#referenciar-recursos","text":"Mediante la stanza rules[].resources se indican los recursos sobre los cuales se dar\u00e1 permisos a un usuario. Esta stanza debe ir acompa\u00f1ada de rules[].apiGroups para indicar a qu\u00e9 grupo API pertenecen los recursos indicados. Se puede ver un listado b\u00e1sico de los recursos sobre los que se pueden aplicar permisos, ejecutando kubectl api-resources | awk '{print $1}' . Hay que tener en cuenta que en el listado no aparecen CRD's (Custom Resource Definition). En ocasiones existen subrecursos, como por ejemplo los logs de un Pod. En estos casos el nombre del recurso estar\u00e1 compuesto por el recurso \"padre\" seguido del recurso \"hijo\", separados por / (p.ej. pods/logs ). Adem\u00e1s de por tipo de recurso, se pueden dar permisos sobre recursos concretos que ya est\u00e9n creados, referenci\u00e1ndolos por su nombre. Esto se consigue mediante la stanza rules[].resourceNames . Advertencia Al referenciar por el nombre de un recurso, no se pueden limitar los permisos create y deletecollection , Ejemplo apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : configmap-updater rules : - apiGroups : [ \"\" ] resources : [ \"configmaps\" ] resourceNames : [ \"my-configmap\" ] verbs : [ \"update\" , \"get\" ]","title":"Referenciar Recursos"},{"location":"security/rbac/#determinar-permisos","text":"En funci\u00f3n del recurso sobre el que se quieran dar permisos, las acciones permitidas pueden ser unas u otras. En la mayor\u00eda de casos se podr\u00e1n dar permisos para realizar las siguientes acciones: HTTP Verb Verb Descripci\u00f3n GET, HEAD get list watch Obtener un recurso individual Obtener un conjunto de recursos, incluyendo su contenido Observar un recurso individual o un conjunto POST create Crear un nuevo recurso PUT update Actualizar un recurso PATCH patch Editar un recurso (mediante kubectl patch ) DELETE delete deletecollection Eliminar un recurso individual Eliminar un conjunto de recursos Adicionalmente existen los siguientes verbs : use : para los recursos podsecuritypolicies . bind y escalate : para los recursos roles y clusterroles . impersonate : para los recursos users , groups , serviceaccounts y userextras . get , post , put y delete : son los verbs que pueden utilizar los non-resource endpoints .","title":"Determinar permisos"},{"location":"security/rbac/#clusterroles-agregados","text":"Se pueden agregar m\u00faltiples ClusterRoles en uno solo, mediante el uso de Labels y Selectors. Mediante la stanza aggregationRule se puede definir un Selector para seleccionar el conjunto de ClusterRoles que se utilizar\u00e1 para definir los permisos del ClusterRole agregado. Este tipo de ClusterRole normalmente tienen definida la stanza rules vac\u00eda, dado que un Controller de Kubernetes se encarga de de rellenar la propiedad con los permisos obtenidos de todos los ClusterRoles seleccionados. Ejemplo ClusterRole agregado: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring aggregationRule : clusterRoleSelectors : - matchLabels : rbac.example.com/aggregate-to-monitoring : \"true\" rules : [] # The control plane automatically fills in the rules ClusterRole que seleccionar\u00e1 el ClusterRole agregado: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : monitoring-endpoints labels : rbac.example.com/aggregate-to-monitoring : \"true\" # When you create the \"monitoring-endpoints\" ClusterRole, # the rules below will be added to the \"monitoring\" ClusterRole. rules : - apiGroups : [ \"\" ] resources : [ \"services\" , \"endpoints\" , \"pods\" ] verbs : [ \"get\" , \"list\" , \"watch\" ]","title":"ClusterRoles agregados"},{"location":"security/rbac/#rolebinding-y-clusterrolebinding","text":"Los RoleBinding y ClusterRoleBinding conceden a un usuario o conjunto de usuarios, los permisos que haya definidos en un rol. Estos recursos de Kubernetes contienen una referencia a un rol y un listado de sujetos a los que se les aplican los permisos (pueden ser usuarios, grupos o ServiceAccount ). Los RoleBinding pueden referenciar cualquier Role de su mismo Namespace. Tambi\u00e9n pueden referenciar a un ClusterRole , pero s\u00f3lo se aplicar\u00e1n los permisos del rol sobre los recursos del Namespace del RoleBinding . Esto puede ser \u00fatil para definir un rol que vaya a ser reutilizado para varios Namespace. Para que los permisos asignados apliquen para todos los Namespace del cluster hay que utilizar un ClusterRoleBinding que haga referencia a un ClusterRole . Notas Los usuarios o grupos pueden tener cualquier formato (texto plano, email...), pero no pueden utilizar el prefijo system: dado que est\u00e1 reservado para elementos de Kubernetes. Los ServiceAccount llevan siempre el prefijo system:serviceaccount: y los grupos de ServiceAccount el prefijo system:serviceaccounts: . Advertencia Una vez creado el RoleBinding / ClusterRoleBinding la stanza roleRef no puede ser modificada. Si se desea modificar hay que eliminar el API object y crear uno nuevo. Lo \u00fanico que se puede modificar del objecto, es el listado de sujetos de la stanza subjects , a los que se les aplican los permisos del rol. Ejemplo RoleBinding que permite al usuario jane , al ServiceAccount default del Namespace kube-system y a todos los ServiceAccount del Namespace qa , leer Pods en el Namespace default : # You need to already have a Role named \"pod-reader\" in that namespace. apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-pods namespace : default subjects : - kind : User name : jane apiGroup : rbac.authorization.k8s.io - kind : ServiceAccount name : default namespace : kube-system - kind : Group name : system:serviceaccounts:qa apiGroup : rbac.authorization.k8s.io roleRef : kind : Role #this must be Role or ClusterRole name : pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup : rbac.authorization.k8s.io RoleBinding que permite al usuario dave leer los Secret del Namespace default : # You need to already have a ClusterRole named \"secret-reader\". apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : read-secrets # This only grants permissions within the \"development\" namespace. namespace : development subjects : - kind : User name : dave apiGroup : rbac.authorization.k8s.io roleRef : kind : ClusterRole name : secret-reader apiGroup : rbac.authorization.k8s.io ClusterRoleBinding que permite a todos los usuarios del grupo manager leer los Secret de cualquier Namespace: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : read-secrets-global subjects : - kind : Group name : manager # Name is case sensitive apiGroup : rbac.authorization.k8s.io roleRef : kind : ClusterRole name : secret-reader apiGroup : rbac.authorization.k8s.io","title":"RoleBinding y ClusterRoleBinding"},{"location":"security/rbac/#prevencion-ante-escalada-de-privilegios","text":"La API de RBAC previene que los usuarios puedan escalar sus privilegios, modificando los roles o los role bindings . Cuidado Si el kube-apiserver tiene el puerto inseguro habilitado, cualquier petici\u00f3n a la API, enviada a ese puerto, ser\u00e1 aceptada sin pasar ning\u00fan proceso de autenticaci\u00f3n o autorizaci\u00f3n.","title":"Prevenci\u00f3n ante escalada de privilegios"},{"location":"security/rbac/#restricciones-en-la-creacionactualizacion-de-roles","text":"Un usuario puede crear/actualizar un rol s\u00f3lo si se cumple alguna de las siguientes condiciones: El usuario ya tiene asignados los permisos que quiere modificar del rol y al mismo nivel (a nivel de cluster si es un ClusterRole , y a nivel de cluster o a nivel del mismo Namespace si es un Role ). Es decir, el usuario s\u00f3lo puede crear/editar roles con los mismos permisos que posea. El usuario tiene expl\u00edcitamente el permiso escalate sobre los recursos roles o clusterroles .","title":"Restricciones en la creaci\u00f3n/actualizaci\u00f3n de roles"},{"location":"security/rbac/#restricciones-en-la-creacionactualizacion-de-role-bindings","text":"Un usuario puede crear/actualizar un rol binding s\u00f3lo si se cumple alguna de las siguientes condiciones: El usuario tiene asignados los mismos permisos que el rol asociado al role binding que pretende modificar y al mismo nivel (a nivel de cluster si es un ClusterRole , y a nivel de cluster o a nivel del mismo Namespace si es un Role ). El usuario tiene expl\u00edcitamente el permiso bind sobre los recursos roles o clusterroles .","title":"Restricciones en la creaci\u00f3n/actualizaci\u00f3n de role bindings"},{"location":"security/rbac/#declaracion-imperativa-con-kubectl","text":"","title":"Declaraci\u00f3n imperativa con kubectl"},{"location":"security/rbac/#crear-un-role","text":"kubectl create role pod-reader --verb = get,list,watch --resource = pods --resource-name = my-pod --resource-name = other-pod","title":"Crear un Role"},{"location":"security/rbac/#crear-un-clusterrole","text":"kubectl create clusterrole replicaset-reader --verb = get,list,watch --resource = replicasets.apps Para dar permisos sobre un endpoint de la API: kubectl create clusterrole logs-creator --verb = get,post --non-resource-url = /logs/* Para a\u00f1adir una aggregationRule : kubectl create clusterrole monitoring --aggregation-rule = \"rbac.example.com/aggregate-to-monitoring=true\"","title":"Crear un ClusterRole"},{"location":"security/rbac/#crear-un-rolebinding","text":"kubectl create rolebinding bob-admin-binding --role = acme-admin --user = bob --namespace = acme Para dar permisos de lectura sobre el Namespace \"acme\", a un ServiceAccount de un Namespace \"myappnamespace\": kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole = view --serviceaccount = myappnamespace:myapp --namespace = acme","title":"Crear un RoleBinding"},{"location":"security/rbac/#crear-un-clusterrolebinding","text":"kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole = cluster-admin --user = root Para dar permisos de lectura sobre todos los Namespace, a un ServiceAccount de un Namespace \"acme\": kubectl create clusterrolebinding myapp-view-binding --clusterrole = view --serviceaccount = acme:myapp","title":"Crear un ClusterRoleBinding"},{"location":"security/rbac/#actualizar-cualquier-objeto-de-la-api-rbacauthorizationk8siov1","text":"Actualizar objetos RBAC a partir de un manifest, manteniendo los permisos y sujetos extra que pudieran ya tener los objetos: kubectl auth reconcile -f my-rbac-rules.yaml Actualizar objetos RBAC a partir de un manifest, eliminando los permisos y sujetos que no aparezcan en el manifest: kubectl auth reconcile -f my-rbac-rules.yaml --remove-extra-subjects --remove-extra-permissions","title":"Actualizar cualquier objeto de la API rbac.authorization.k8s.io/v1"},{"location":"security/security-context/","text":"Security Context Un Security Context define un conjunto de configuraciones de privilegios y controles de acceso sobre un Pod o Container. Entre otras, estas son algunas de las configuraciones que pueden contener: Control de Acceso Discrecional (DAC) a un objeto (p.ej. un fichero). Se otorgan permisos (lectura, escritura, ejecuci\u00f3n) basado en el ID del usuario (UID) o el ID del grupo al que pertenece el usuario (GID). Security Enhanced Linux (SELinux): permite definir etiquetas de seguridad a los objetos del sistema (p.ej. ficheros). Ejecutar los procesos con privilegios o sin ellos (como administrador o no). Permitir a los procesos hacer uso de Linux Capabilities : permite a un proceso ejecutarse con alguno de los permisos del usuario root , para poder operar sobre ciertos m\u00f3dulos del kernel. AppArmor : permite hacer uso de perfiles que restrinjan el uso de capabilities de forma individual por aplicaci\u00f3n. Utilizar Seccomp para filtrar las llamadas a sistema que puede realizar el proceso. Habilitar o no la escalada de privilegios. Montar el sistema de ficheros ra\u00edz del contenedor s\u00f3lo como lectura. Para configurar el Security Context de un Pod se utiliza la stanza spec.securityContext . Este contexto de seguridad aplica a todos los contenedores del Pod. Ejemplo apiVersion : v1 kind : Pod metadata : name : security-context-demo spec : securityContext : runAsUser : 1000 runAsGroup : 3000 fsGroup : 2000 volumes : - name : sec-ctx-vol emptyDir : {} containers : - name : sec-ctx-demo image : busybox command : [ \"sh\" , \"-c\" , \"sleep 1h\" ] volumeMounts : - name : sec-ctx-vol mountPath : /data/demo securityContext : allowPrivilegeEscalation : false La propiedad runUser especifica que todos los procesos de los contenedores del Pod, ser\u00e1n ejecutados por el usuario con ID 1000. De la misma forma la propiedad runAsGroup indica que el grupo principal del usuario es el grupo con ID 3000. Si se omite esta propiedad, el valor por defecto es 0 (root), por lo que los procesos podr\u00e1n interactuar con todos los ficheros que pertenezcan a ese grupo. La propiedad fsGroup indica el ID del grupo al que pertenecer\u00e1n los ficheros del sistema de ficheros (inclu\u00eddos los vol\u00famenes). En el caso de los vol\u00famenes, Kubernetes se encarga de sobrescribir el propietario de todos los ficheros, antes de montarlo en el Pod, para que coincida con la propiedad fsGroup . Esto significa que los procesos que se ejecuten en este contenedor pertenecer\u00e1n al usuario con ID 1000 y grupo 3000, y los ficheros que generen esos procesos, pertenecer\u00e1n al usuario con ID 1000 y grupo 2000. Security Context para contenedores Se pueden especificar Security Context que apliquen \u00fanicamente a ciertos contenedores. Estos contextos de seguridad sobrescribir\u00e1n los que haya definidos a nivel de Pod. Ejemplo En este ejemplo los procesos del contenedor ser\u00e1n ejecutados por el usuario con ID 2000, sobrescribiendo as\u00ed las especificaciones globales del Pod: apiVersion : v1 kind : Pod metadata : name : security-context-demo-2 spec : securityContext : runAsUser : 1000 containers : - name : sec-ctx-demo-2 image : gcr.io/google-samples/node-hello:1.0 securityContext : runAsUser : 2000 allowPrivilegeEscalation : false Asignar Linux Capabilities a un contenedor Las Linux Capabilities se pueden asignar \u00fanicamente a nivel de contenedor a traves de la stanza spec.containers[].securityContext.capabilities . Esto permite a un proceso disponer de ciertos permisos sobre el kernel, sin necesidad de disponer de todos los permisos del usuario root (p.ej. ). Informaci\u00f3n Se puede consultar el listado completo de Linux Capabilities en el fichero /usr/include/linux/capability.h . Ejemplo apiVersion : v1 kind : Pod metadata : name : security-context-demo-4 spec : containers : - name : sec-ctx-4 image : gcr.io/google-samples/node-hello:1.0 securityContext : capabilities : add : [ \"NET_ADMIN\" , \"SYS_TIME\" ]","title":"Security Context"},{"location":"security/security-context/#security-context","text":"Un Security Context define un conjunto de configuraciones de privilegios y controles de acceso sobre un Pod o Container. Entre otras, estas son algunas de las configuraciones que pueden contener: Control de Acceso Discrecional (DAC) a un objeto (p.ej. un fichero). Se otorgan permisos (lectura, escritura, ejecuci\u00f3n) basado en el ID del usuario (UID) o el ID del grupo al que pertenece el usuario (GID). Security Enhanced Linux (SELinux): permite definir etiquetas de seguridad a los objetos del sistema (p.ej. ficheros). Ejecutar los procesos con privilegios o sin ellos (como administrador o no). Permitir a los procesos hacer uso de Linux Capabilities : permite a un proceso ejecutarse con alguno de los permisos del usuario root , para poder operar sobre ciertos m\u00f3dulos del kernel. AppArmor : permite hacer uso de perfiles que restrinjan el uso de capabilities de forma individual por aplicaci\u00f3n. Utilizar Seccomp para filtrar las llamadas a sistema que puede realizar el proceso. Habilitar o no la escalada de privilegios. Montar el sistema de ficheros ra\u00edz del contenedor s\u00f3lo como lectura. Para configurar el Security Context de un Pod se utiliza la stanza spec.securityContext . Este contexto de seguridad aplica a todos los contenedores del Pod. Ejemplo apiVersion : v1 kind : Pod metadata : name : security-context-demo spec : securityContext : runAsUser : 1000 runAsGroup : 3000 fsGroup : 2000 volumes : - name : sec-ctx-vol emptyDir : {} containers : - name : sec-ctx-demo image : busybox command : [ \"sh\" , \"-c\" , \"sleep 1h\" ] volumeMounts : - name : sec-ctx-vol mountPath : /data/demo securityContext : allowPrivilegeEscalation : false La propiedad runUser especifica que todos los procesos de los contenedores del Pod, ser\u00e1n ejecutados por el usuario con ID 1000. De la misma forma la propiedad runAsGroup indica que el grupo principal del usuario es el grupo con ID 3000. Si se omite esta propiedad, el valor por defecto es 0 (root), por lo que los procesos podr\u00e1n interactuar con todos los ficheros que pertenezcan a ese grupo. La propiedad fsGroup indica el ID del grupo al que pertenecer\u00e1n los ficheros del sistema de ficheros (inclu\u00eddos los vol\u00famenes). En el caso de los vol\u00famenes, Kubernetes se encarga de sobrescribir el propietario de todos los ficheros, antes de montarlo en el Pod, para que coincida con la propiedad fsGroup . Esto significa que los procesos que se ejecuten en este contenedor pertenecer\u00e1n al usuario con ID 1000 y grupo 3000, y los ficheros que generen esos procesos, pertenecer\u00e1n al usuario con ID 1000 y grupo 2000.","title":"Security Context"},{"location":"security/security-context/#security-context-para-contenedores","text":"Se pueden especificar Security Context que apliquen \u00fanicamente a ciertos contenedores. Estos contextos de seguridad sobrescribir\u00e1n los que haya definidos a nivel de Pod. Ejemplo En este ejemplo los procesos del contenedor ser\u00e1n ejecutados por el usuario con ID 2000, sobrescribiendo as\u00ed las especificaciones globales del Pod: apiVersion : v1 kind : Pod metadata : name : security-context-demo-2 spec : securityContext : runAsUser : 1000 containers : - name : sec-ctx-demo-2 image : gcr.io/google-samples/node-hello:1.0 securityContext : runAsUser : 2000 allowPrivilegeEscalation : false","title":"Security Context para contenedores"},{"location":"security/security-context/#asignar-linux-capabilities-a-un-contenedor","text":"Las Linux Capabilities se pueden asignar \u00fanicamente a nivel de contenedor a traves de la stanza spec.containers[].securityContext.capabilities . Esto permite a un proceso disponer de ciertos permisos sobre el kernel, sin necesidad de disponer de todos los permisos del usuario root (p.ej. ). Informaci\u00f3n Se puede consultar el listado completo de Linux Capabilities en el fichero /usr/include/linux/capability.h . Ejemplo apiVersion : v1 kind : Pod metadata : name : security-context-demo-4 spec : containers : - name : sec-ctx-4 image : gcr.io/google-samples/node-hello:1.0 securityContext : capabilities : add : [ \"NET_ADMIN\" , \"SYS_TIME\" ]","title":"Asignar Linux Capabilities a un contenedor"},{"location":"security/service-account/","text":"ServiceAccount Los ServiceAccount sirven para dar una identidad a los procesos que se ejecuten dentro de un Pod, y de esta manera poder darles permisos sobre el cluster. Por defecto, en todos los Namespace existe un ServiceAccount de nombre default con los permisos m\u00ednimos que puede necesitar un Pod para interactuar con la API de Kubernetes. Si en la definici\u00f3n del Pod no se especifica la propiedad spec.serviceAccountName , se utilizar\u00e1 el default del Namespace. Con la creaci\u00f3n de cada ServiceAccount , Kubernetes crea un Secret asociado, que contendr\u00e1 un token que se habr\u00e1 generado, para que el ServiceAccount pueda autenticarse contra la API de Kubernetes. Este Secret se monta en cada uno de los Pods que utilicen el ServiceAccount . Notas Se puede desactivar el uso por defecto del ServiceAccount default en los Pods que no tengan definida la propiedad spec.serviceAccountName , a\u00f1adiendo la propiedad automountServiceAccountToken: false . Si en un Pod se a\u00f1ade la propiedad automountServiceAccountToken: false , el Secret que contiene el token del ServiceAccount no se montar\u00e1 autom\u00e1ticamente. Ejemplo apiVersion : v1 kind : ServiceAccount metadata : name : build-robot A\u00f1adir ImagePullSecrets al ServiceAccount Se pueden a\u00f1adir ImagePullSecrets a un ServiceAccount para que todo Pod que utilice el ServiceAccount tenga acceso a esos Secret para poder realizar los pull de las imagenes Docker. Para ello hay que configurar la stanza imagePullSecrets . Ejemplo apiVersion : v1 kind : ServiceAccount metadata : name : build-robot imagePullSecrets : - name : myregistrykey Declaraci\u00f3n imperativa con kubectl kubectl create serviceaccount my-service-account","title":"ServiceAccount"},{"location":"security/service-account/#serviceaccount","text":"Los ServiceAccount sirven para dar una identidad a los procesos que se ejecuten dentro de un Pod, y de esta manera poder darles permisos sobre el cluster. Por defecto, en todos los Namespace existe un ServiceAccount de nombre default con los permisos m\u00ednimos que puede necesitar un Pod para interactuar con la API de Kubernetes. Si en la definici\u00f3n del Pod no se especifica la propiedad spec.serviceAccountName , se utilizar\u00e1 el default del Namespace. Con la creaci\u00f3n de cada ServiceAccount , Kubernetes crea un Secret asociado, que contendr\u00e1 un token que se habr\u00e1 generado, para que el ServiceAccount pueda autenticarse contra la API de Kubernetes. Este Secret se monta en cada uno de los Pods que utilicen el ServiceAccount . Notas Se puede desactivar el uso por defecto del ServiceAccount default en los Pods que no tengan definida la propiedad spec.serviceAccountName , a\u00f1adiendo la propiedad automountServiceAccountToken: false . Si en un Pod se a\u00f1ade la propiedad automountServiceAccountToken: false , el Secret que contiene el token del ServiceAccount no se montar\u00e1 autom\u00e1ticamente. Ejemplo apiVersion : v1 kind : ServiceAccount metadata : name : build-robot","title":"ServiceAccount"},{"location":"security/service-account/#anadir-imagepullsecrets-al-serviceaccount","text":"Se pueden a\u00f1adir ImagePullSecrets a un ServiceAccount para que todo Pod que utilice el ServiceAccount tenga acceso a esos Secret para poder realizar los pull de las imagenes Docker. Para ello hay que configurar la stanza imagePullSecrets . Ejemplo apiVersion : v1 kind : ServiceAccount metadata : name : build-robot imagePullSecrets : - name : myregistrykey","title":"A\u00f1adir ImagePullSecrets al ServiceAccount"},{"location":"security/service-account/#declaracion-imperativa-con-kubectl","text":"kubectl create serviceaccount my-service-account","title":"Declaraci\u00f3n imperativa con kubectl"},{"location":"storage/persistent-volume/","text":"Persistent Volume & Claim La API de Kubernetes dispone de dos recursos que permiten abstraer y separar el aprovisionamiento de almacenamiento, respecto a la forma en que se consume. Por una parte, est\u00e1n los PersistentVolume (PV), que representan secciones de almacenamiento dentro del cluster. Estas secciones de almacenamientos puedes ser aprovisionadas por un administrador o de forma din\u00e1mica mediante un StorageClass . Los PersistentVolume a diferencia de los Volume , tienen un ciclo de vida propio, por lo que su tiempo de vida no depende del Pod que lo utiliza. Por otro lado, est\u00e1n los PersistentVolumeClaim (PVC), que son las peticiones de almacenamiento que realizan los usuarios. Permiten solicitar la cantidad de almacenamiento requerida, as\u00ed como el modo de acceso al almacenamiento ( ReadWriteOnce , ReadOnlyMany o ReadWriteMany ). De esta forma los PersistentVolumeClaim se vinculan con un PersistentVolume que cumpla sus requisitos. Los administradores del cluster suelen aprovisionar distintas tipolog\u00edas de PersistentVolume (o StorageClass ) para cubrir diferentes necesidades que puedan tener los usuarios (tama\u00f1o, rendimiento...). Ciclo de vida de un PV y su PVC Aprovisionamiento El aprovisionamiento de los PV puede llevarlo a cabo de forma est\u00e1tica el administrador del cluster o de forma din\u00e1mica si el PVC tiene configurada una StorageClass . Si el PVC no encuentra un PV creado anteriormente, que cumpla sus necesidades, solicitar\u00e1 a la StorageClass que genere uno nuevo. El StorageClass debe haber sido previamente configurado por el administrador para que por ejemplo pida los nuevos recursos a un proveedor Cloud. Binding Cuando un usuario crea un PersistentVolumeClaim (o se genera de forma din\u00e1mica por el Pod), el cluster busca un PV ya creado que cumpla los requisitos de almacenamiento y modo de acceso solicitados. Si lo encuentra los vincula. De lo contrario, se aprovisiona un PV de forma din\u00e1mica para que cubra las necesidades del PVC y se vinculan. La vinculaci\u00f3n entre PV y PVC es exclusivamente 1-to-1, mediante una propiedad claimRef , que los vincula de forma bidireccional. Advertencia Un PersistentVolumeClaim puede utilizar cualquier PersistentVolume que cumpla sus demandas. Incluso si ofrece m\u00e1s almacenamiento que el requerido. En ese caso, el PersistentVolume se infrautilizar\u00e1, ya que el Pod \u00fanicamente utilizar\u00e1 el almacenamiento solicitado. Uso Los Pods utilizan los Claims como Volume. El cluster se encarga de inspeccionar el Claim, buscar el PV asociado a \u00e9l y montarlo en el Pod con el modo de acceso configurado. Protecci\u00f3n \"Storage Object in Use\" Esta protecci\u00f3n sirve para evitar la p\u00e9rdida de datos. Esta caracter\u00edstica impide que se elimine un PVC que est\u00e9 siendo utilizado por un Pod. Se puede su eliminaci\u00f3n a la API, pero hasta que el Pod no deje de utilizar el PVC, no se cursar\u00e1 la petici\u00f3n. De la misma forma sucede con los PV que est\u00e9n vinculados a un PVC. Hasta que no se desvincule del PVC, la petici\u00f3n de eliminaci\u00f3n del PV no se cursar\u00e1. Reclaiming Cuando un usuario ya no necesita un PVC, deber\u00e1 eliminarlo manualmente. Entonces, la reclaim policy del PV que hubiera vinculado con ese PVC, indicar\u00e1 al cl\u00faster qu\u00e9 hacer con el PersistentVolume . Existen 3 tipos de reclaim policy : Retain, Recycle o Delete. Retain Con esta policy, el PV no se eliminar\u00e1 para que sea realice de forma manual por el administrador. Pese a que se libere el recurso, el PV seguir\u00e1 teniendo definido el ClaimRef del PVC con el que estaba vinculado y el contenido que haya sido almacenado por el usuario. Si se utiliza un Volume plugin que aprovisione recursos externos (p.ej. en un Cloud provider), una vez se elimine el PV, los recursos generados no ser\u00e1n eliminados. Deber\u00e1n eliminarse manualmente desde fuera del cluster. Recycle (Deprecated) Con esta policy, se elimina todo el contenido del PV, para que sea reutilizado por otro PVC. Delete Con esta policy, una vez se elimina el PersistentVolumeClaim tambi\u00e9n se elimina el PersistentVolume , as\u00ed como todo recurso externo que haya podido generar el Volume plugin. Expandir almacenamiento Existen ciertos Volume plugin que permiten expandir el almacenamiento solicitado por el PVC. De esta forma, las StorageClass que utilicen estos Volume plugin y tengan la propiedad allowVolumeExpansion: true , ampliar\u00e1n de forma din\u00e1mica el PV y su recurso asociado. Redimensionar Persistent Volume con sistema de ficheros Al ampliar un PV que contenga un sistema de ficheros, el redimensionamiento solo se realizar\u00e1 cuando el PVC se utilice por un nuevo Pod y sea en modo ReadWrite . Redimensionar Persistent Volume Claim en uso Esta caracter\u00edstica actualmente esta en versi\u00f3n beta y debe ser activada en las feature gates . Los Volume de tipo FlexVolume permiten su ampliaci\u00f3n mientras est\u00e1n siendo utilizados por un Pod. PersistentVolume Los PV contienen las siguientes propiedades que configuran el recurso de almacenamiento a utilizar: Capacity : Tama\u00f1o de almacenamiento. Volume Mode : Existen dos modos disponibles: Filesystem : (Por defecto) El Volume se monta dentro de un directorio del Pod. Si el Volume se respalda en un dispositivo por bloques y el dispositivo se encuentra vac\u00edo, se crea un sistema de ficheros antes de montarlo por primera vez. Block : Este modo permite montar un dispositivo por bloques directamente en un Pod sin necesidad de sistema de ficheros. Esto acelera el acceso del Pod al Volume, pero la aplicaci\u00f3n debe saber trabajar directamente con un dispositivo por bloques. Access Modes : Modos de acceso del Volume (depende del Volume plugin): ReadWriteOnce : El PV puede montarse con permiso de lectura-escritura por un \u00fanico nodo. ReadOnlyMany : El PV solo se puede montar con permiso de lectura por varios nodos. ReadWriteMany : El PV puede montarse con permiso de lectura-escritura por varios nodos. Advertencia Solo puede utilizarse un Access Mode a la vez StorageClass : Indica el nombre de la StorageClass a la que pertenece. Los PV con una StorageClass asignada solo pueden ser vinculados por los PVC que utilicen la misma StorageClass . Por otro lado, los Pv que no definan una StorageClass solo podr\u00e1n vincularse con los PVC que tampoco tengan una definida. Mount Options : Opciones adicionales para montar el PV en el nodo. PersistentVolume Type : Al igual que con los Volume de los Pods, existen varios tipos de PersistentVolume . Node Affinity : Necesario s\u00f3lo para los Volume de tipo Local . Sirve para limitar desde qu\u00e9 nodos se puede acceder al PV. Esto tambi\u00e9n implica que los Pods que utilicen este PV s\u00f3lo podr\u00e1n desplegarse en los nodos seleccionados por la affinity. Ejemplo Ejemplo de PersistentVolume de tipo nfs : apiVersion : v1 kind : PersistentVolume metadata : name : pv0003 spec : capacity : storage : 5Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Delete storageClassName : slow mountOptions : - hard - nfsvers=4.1 nfs : path : /tmp server : 172.17.0.2 PersistentVolumeClaim Los PVC contienen las siguientes propiedades que determinan qu\u00e9 tipo de almacenamiento solicitar: Access Modes : Modos de acceso del Volume. (Son los mismos que para los PV) Volume Mode : Modo en el que se va a consumir el PV. (Son los mismos que para los PV) Resources : Recursos de almacenamiento que solicitan. Selector : Permite filtrar a\u00fan m\u00e1s que PV utilizar. S\u00f3lo se vincular\u00e1n los PV que dispongan (o no) de ciertas Labels. StorageClass : Permite seleccionar s\u00f3lo los PV de una cierta StorageClass . Si se especifica un valor \"\" , solo se podr\u00e1n utilizar PV sin una StorageClass definida. Si no se define StorageClass , se utilizar\u00e1n los PV de la StorageClass \"por defecto\", si esta configurada, o de lo contrario s\u00f3lo los PV sin clase. Ejemplo apiVersion : v1 kind : PersistentVolumeClaim metadata : name : myclaim spec : accessModes : - ReadWriteOnce volumeMode : Filesystem resources : requests : storage : 8Gi storageClassName : slow selector : matchLabels : release : \"stable\" matchExpressions : - { key : environment , operator : In , values : [ dev ]}","title":"Persistent Volume & Claim"},{"location":"storage/persistent-volume/#persistent-volume-claim","text":"La API de Kubernetes dispone de dos recursos que permiten abstraer y separar el aprovisionamiento de almacenamiento, respecto a la forma en que se consume. Por una parte, est\u00e1n los PersistentVolume (PV), que representan secciones de almacenamiento dentro del cluster. Estas secciones de almacenamientos puedes ser aprovisionadas por un administrador o de forma din\u00e1mica mediante un StorageClass . Los PersistentVolume a diferencia de los Volume , tienen un ciclo de vida propio, por lo que su tiempo de vida no depende del Pod que lo utiliza. Por otro lado, est\u00e1n los PersistentVolumeClaim (PVC), que son las peticiones de almacenamiento que realizan los usuarios. Permiten solicitar la cantidad de almacenamiento requerida, as\u00ed como el modo de acceso al almacenamiento ( ReadWriteOnce , ReadOnlyMany o ReadWriteMany ). De esta forma los PersistentVolumeClaim se vinculan con un PersistentVolume que cumpla sus requisitos. Los administradores del cluster suelen aprovisionar distintas tipolog\u00edas de PersistentVolume (o StorageClass ) para cubrir diferentes necesidades que puedan tener los usuarios (tama\u00f1o, rendimiento...).","title":"Persistent Volume &amp; Claim"},{"location":"storage/persistent-volume/#ciclo-de-vida-de-un-pv-y-su-pvc","text":"","title":"Ciclo de vida de un PV y su PVC"},{"location":"storage/persistent-volume/#aprovisionamiento","text":"El aprovisionamiento de los PV puede llevarlo a cabo de forma est\u00e1tica el administrador del cluster o de forma din\u00e1mica si el PVC tiene configurada una StorageClass . Si el PVC no encuentra un PV creado anteriormente, que cumpla sus necesidades, solicitar\u00e1 a la StorageClass que genere uno nuevo. El StorageClass debe haber sido previamente configurado por el administrador para que por ejemplo pida los nuevos recursos a un proveedor Cloud.","title":"Aprovisionamiento"},{"location":"storage/persistent-volume/#binding","text":"Cuando un usuario crea un PersistentVolumeClaim (o se genera de forma din\u00e1mica por el Pod), el cluster busca un PV ya creado que cumpla los requisitos de almacenamiento y modo de acceso solicitados. Si lo encuentra los vincula. De lo contrario, se aprovisiona un PV de forma din\u00e1mica para que cubra las necesidades del PVC y se vinculan. La vinculaci\u00f3n entre PV y PVC es exclusivamente 1-to-1, mediante una propiedad claimRef , que los vincula de forma bidireccional. Advertencia Un PersistentVolumeClaim puede utilizar cualquier PersistentVolume que cumpla sus demandas. Incluso si ofrece m\u00e1s almacenamiento que el requerido. En ese caso, el PersistentVolume se infrautilizar\u00e1, ya que el Pod \u00fanicamente utilizar\u00e1 el almacenamiento solicitado.","title":"Binding"},{"location":"storage/persistent-volume/#uso","text":"Los Pods utilizan los Claims como Volume. El cluster se encarga de inspeccionar el Claim, buscar el PV asociado a \u00e9l y montarlo en el Pod con el modo de acceso configurado.","title":"Uso"},{"location":"storage/persistent-volume/#proteccion-storage-object-in-use","text":"Esta protecci\u00f3n sirve para evitar la p\u00e9rdida de datos. Esta caracter\u00edstica impide que se elimine un PVC que est\u00e9 siendo utilizado por un Pod. Se puede su eliminaci\u00f3n a la API, pero hasta que el Pod no deje de utilizar el PVC, no se cursar\u00e1 la petici\u00f3n. De la misma forma sucede con los PV que est\u00e9n vinculados a un PVC. Hasta que no se desvincule del PVC, la petici\u00f3n de eliminaci\u00f3n del PV no se cursar\u00e1.","title":"Protecci\u00f3n \"Storage Object in Use\""},{"location":"storage/persistent-volume/#reclaiming","text":"Cuando un usuario ya no necesita un PVC, deber\u00e1 eliminarlo manualmente. Entonces, la reclaim policy del PV que hubiera vinculado con ese PVC, indicar\u00e1 al cl\u00faster qu\u00e9 hacer con el PersistentVolume . Existen 3 tipos de reclaim policy : Retain, Recycle o Delete.","title":"Reclaiming"},{"location":"storage/persistent-volume/#retain","text":"Con esta policy, el PV no se eliminar\u00e1 para que sea realice de forma manual por el administrador. Pese a que se libere el recurso, el PV seguir\u00e1 teniendo definido el ClaimRef del PVC con el que estaba vinculado y el contenido que haya sido almacenado por el usuario. Si se utiliza un Volume plugin que aprovisione recursos externos (p.ej. en un Cloud provider), una vez se elimine el PV, los recursos generados no ser\u00e1n eliminados. Deber\u00e1n eliminarse manualmente desde fuera del cluster.","title":"Retain"},{"location":"storage/persistent-volume/#recycle-deprecated","text":"Con esta policy, se elimina todo el contenido del PV, para que sea reutilizado por otro PVC.","title":"Recycle (Deprecated)"},{"location":"storage/persistent-volume/#delete","text":"Con esta policy, una vez se elimina el PersistentVolumeClaim tambi\u00e9n se elimina el PersistentVolume , as\u00ed como todo recurso externo que haya podido generar el Volume plugin.","title":"Delete"},{"location":"storage/persistent-volume/#expandir-almacenamiento","text":"Existen ciertos Volume plugin que permiten expandir el almacenamiento solicitado por el PVC. De esta forma, las StorageClass que utilicen estos Volume plugin y tengan la propiedad allowVolumeExpansion: true , ampliar\u00e1n de forma din\u00e1mica el PV y su recurso asociado.","title":"Expandir almacenamiento"},{"location":"storage/persistent-volume/#redimensionar-persistent-volume-con-sistema-de-ficheros","text":"Al ampliar un PV que contenga un sistema de ficheros, el redimensionamiento solo se realizar\u00e1 cuando el PVC se utilice por un nuevo Pod y sea en modo ReadWrite .","title":"Redimensionar Persistent Volume con sistema de ficheros"},{"location":"storage/persistent-volume/#redimensionar-persistent-volume-claim-en-uso","text":"Esta caracter\u00edstica actualmente esta en versi\u00f3n beta y debe ser activada en las feature gates . Los Volume de tipo FlexVolume permiten su ampliaci\u00f3n mientras est\u00e1n siendo utilizados por un Pod.","title":"Redimensionar Persistent Volume Claim en uso"},{"location":"storage/persistent-volume/#persistentvolume","text":"Los PV contienen las siguientes propiedades que configuran el recurso de almacenamiento a utilizar: Capacity : Tama\u00f1o de almacenamiento. Volume Mode : Existen dos modos disponibles: Filesystem : (Por defecto) El Volume se monta dentro de un directorio del Pod. Si el Volume se respalda en un dispositivo por bloques y el dispositivo se encuentra vac\u00edo, se crea un sistema de ficheros antes de montarlo por primera vez. Block : Este modo permite montar un dispositivo por bloques directamente en un Pod sin necesidad de sistema de ficheros. Esto acelera el acceso del Pod al Volume, pero la aplicaci\u00f3n debe saber trabajar directamente con un dispositivo por bloques. Access Modes : Modos de acceso del Volume (depende del Volume plugin): ReadWriteOnce : El PV puede montarse con permiso de lectura-escritura por un \u00fanico nodo. ReadOnlyMany : El PV solo se puede montar con permiso de lectura por varios nodos. ReadWriteMany : El PV puede montarse con permiso de lectura-escritura por varios nodos. Advertencia Solo puede utilizarse un Access Mode a la vez StorageClass : Indica el nombre de la StorageClass a la que pertenece. Los PV con una StorageClass asignada solo pueden ser vinculados por los PVC que utilicen la misma StorageClass . Por otro lado, los Pv que no definan una StorageClass solo podr\u00e1n vincularse con los PVC que tampoco tengan una definida. Mount Options : Opciones adicionales para montar el PV en el nodo. PersistentVolume Type : Al igual que con los Volume de los Pods, existen varios tipos de PersistentVolume . Node Affinity : Necesario s\u00f3lo para los Volume de tipo Local . Sirve para limitar desde qu\u00e9 nodos se puede acceder al PV. Esto tambi\u00e9n implica que los Pods que utilicen este PV s\u00f3lo podr\u00e1n desplegarse en los nodos seleccionados por la affinity. Ejemplo Ejemplo de PersistentVolume de tipo nfs : apiVersion : v1 kind : PersistentVolume metadata : name : pv0003 spec : capacity : storage : 5Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Delete storageClassName : slow mountOptions : - hard - nfsvers=4.1 nfs : path : /tmp server : 172.17.0.2","title":"PersistentVolume"},{"location":"storage/persistent-volume/#persistentvolumeclaim","text":"Los PVC contienen las siguientes propiedades que determinan qu\u00e9 tipo de almacenamiento solicitar: Access Modes : Modos de acceso del Volume. (Son los mismos que para los PV) Volume Mode : Modo en el que se va a consumir el PV. (Son los mismos que para los PV) Resources : Recursos de almacenamiento que solicitan. Selector : Permite filtrar a\u00fan m\u00e1s que PV utilizar. S\u00f3lo se vincular\u00e1n los PV que dispongan (o no) de ciertas Labels. StorageClass : Permite seleccionar s\u00f3lo los PV de una cierta StorageClass . Si se especifica un valor \"\" , solo se podr\u00e1n utilizar PV sin una StorageClass definida. Si no se define StorageClass , se utilizar\u00e1n los PV de la StorageClass \"por defecto\", si esta configurada, o de lo contrario s\u00f3lo los PV sin clase. Ejemplo apiVersion : v1 kind : PersistentVolumeClaim metadata : name : myclaim spec : accessModes : - ReadWriteOnce volumeMode : Filesystem resources : requests : storage : 8Gi storageClassName : slow selector : matchLabels : release : \"stable\" matchExpressions : - { key : environment , operator : In , values : [ dev ]}","title":"PersistentVolumeClaim"},{"location":"storage/storage-classes/","text":"Storage Classes Kubernetes permite a los administradores del cluster ofrecer distintas \"clases\" de almacenamiento, a trav\u00e9s de las StorageClass . Cada clase puede ofrecer caracter\u00edsticas diferentes, p.ej. distintos niveles de QoS, distintos medios de almacenamiento, etc. El recurso StorageClass proporciona la informaci\u00f3n necesaria par que los PersistentVolume puedan aprovisionarse de forma din\u00e1mica. Para ello, los usuarios deben indicar una StorageClass en la definici\u00f3n de su PersistentVolumeClaim . Sin el uso de esta funcionalidad, los administradores del cluster deber\u00edan aprovisionar manualmente vol\u00famenes de almacenamiento en un proveedor Cloud o de almacenamiento y luego representarlo en Kubernetes mediante la creaci\u00f3n de PersistentVolumes . De esta forma, los administradores pueden crear diferentes StorageClass , ofreciendo distintas posibilidades de almacenamiento de diferentes proveedores o con diferentes caracter\u00edsticas utilizando un mismo proveedor. De esta forma los usuarios pueden aprovisionar de forma din\u00e1mica los recursos necesarios, sin tener que conocer la complejidad de c\u00f3mo se aprovisionan, pero teniendo la oportunidad de elegir. Los StorageClass est\u00e1n compuestos por los siguientes campos: provisioner : Determina que Volume plugin se utilizar\u00e1 para aprovisionar los nuevos PV. parameters : Propiedades adicionales que pueda necesitar el Volume plugin para crear el PV. reclaimPolicy : Tipo de reclaim policy que deber\u00e1 tener configurado el PV que se genere din\u00e1micamente. (Por defecto: Delete ) allowVolumeExpansion : Si esta propiedad est\u00e1 a true y el Volume plugin lo permite, el usuario podr\u00e1 ampliar el PV. mountOptions : Propiedades adicionales para configurar c\u00f3mo el Volume plugin deber\u00e1 montar el PV. volumeBindingMode : Esta propiedad se puede utilizar para que el proceso de creaci\u00f3n y vinculaci\u00f3n del PV se espere a que el Pod que utilice el PVC haya sido creado. De esta manera el PV se crear\u00e1 para que cumpla con las restricciones de topolog\u00eda del Pod (node affinity, pod affinity o anti-affinity...). Por defecto tiene valor Immediate , pero se puede configurar con valor WaitForFirstConsumer . allowedTopologies : En ocasiones muy concretas, cuando se utiliza el modo de vinculaci\u00f3n WaitForFirstConsumer , es necesario aplicar restricciones de topolog\u00eda adicionales. Esta propiedad permite definir qu\u00e9 topolog\u00edas se permiten. Ejemplo apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : standard provisioner : kubernetes.io/gce-pd parameters : type : pd-standard reclaimPolicy : Retain allowVolumeExpansion : true volumeBindingMode : WaitForFirstConsumer allowedTopologies : - matchLabelExpressions : - key : failure-domain.beta.kubernetes.io/zone values : - us-central1-a - us-central1-b StorageClass por defecto El administrador puede configurar el cluster para que haya una StorageClass por defecto que se encargue de aprovisionar los PersistentVolume de los PersistentVolumeClaim que no tengan ning\u00fan StorageClass definido. Para ello, el StorageClass debe tener la anotaci\u00f3n storageclass.kubernetes.io/is-default-class . Advertencia Solo puede existir un StorageClass por defecto. De lo contrario, esta funcionalidad deja de operar.","title":"Storage Classes"},{"location":"storage/storage-classes/#storage-classes","text":"Kubernetes permite a los administradores del cluster ofrecer distintas \"clases\" de almacenamiento, a trav\u00e9s de las StorageClass . Cada clase puede ofrecer caracter\u00edsticas diferentes, p.ej. distintos niveles de QoS, distintos medios de almacenamiento, etc. El recurso StorageClass proporciona la informaci\u00f3n necesaria par que los PersistentVolume puedan aprovisionarse de forma din\u00e1mica. Para ello, los usuarios deben indicar una StorageClass en la definici\u00f3n de su PersistentVolumeClaim . Sin el uso de esta funcionalidad, los administradores del cluster deber\u00edan aprovisionar manualmente vol\u00famenes de almacenamiento en un proveedor Cloud o de almacenamiento y luego representarlo en Kubernetes mediante la creaci\u00f3n de PersistentVolumes . De esta forma, los administradores pueden crear diferentes StorageClass , ofreciendo distintas posibilidades de almacenamiento de diferentes proveedores o con diferentes caracter\u00edsticas utilizando un mismo proveedor. De esta forma los usuarios pueden aprovisionar de forma din\u00e1mica los recursos necesarios, sin tener que conocer la complejidad de c\u00f3mo se aprovisionan, pero teniendo la oportunidad de elegir. Los StorageClass est\u00e1n compuestos por los siguientes campos: provisioner : Determina que Volume plugin se utilizar\u00e1 para aprovisionar los nuevos PV. parameters : Propiedades adicionales que pueda necesitar el Volume plugin para crear el PV. reclaimPolicy : Tipo de reclaim policy que deber\u00e1 tener configurado el PV que se genere din\u00e1micamente. (Por defecto: Delete ) allowVolumeExpansion : Si esta propiedad est\u00e1 a true y el Volume plugin lo permite, el usuario podr\u00e1 ampliar el PV. mountOptions : Propiedades adicionales para configurar c\u00f3mo el Volume plugin deber\u00e1 montar el PV. volumeBindingMode : Esta propiedad se puede utilizar para que el proceso de creaci\u00f3n y vinculaci\u00f3n del PV se espere a que el Pod que utilice el PVC haya sido creado. De esta manera el PV se crear\u00e1 para que cumpla con las restricciones de topolog\u00eda del Pod (node affinity, pod affinity o anti-affinity...). Por defecto tiene valor Immediate , pero se puede configurar con valor WaitForFirstConsumer . allowedTopologies : En ocasiones muy concretas, cuando se utiliza el modo de vinculaci\u00f3n WaitForFirstConsumer , es necesario aplicar restricciones de topolog\u00eda adicionales. Esta propiedad permite definir qu\u00e9 topolog\u00edas se permiten. Ejemplo apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : standard provisioner : kubernetes.io/gce-pd parameters : type : pd-standard reclaimPolicy : Retain allowVolumeExpansion : true volumeBindingMode : WaitForFirstConsumer allowedTopologies : - matchLabelExpressions : - key : failure-domain.beta.kubernetes.io/zone values : - us-central1-a - us-central1-b","title":"Storage Classes"},{"location":"storage/storage-classes/#storageclass-por-defecto","text":"El administrador puede configurar el cluster para que haya una StorageClass por defecto que se encargue de aprovisionar los PersistentVolume de los PersistentVolumeClaim que no tengan ning\u00fan StorageClass definido. Para ello, el StorageClass debe tener la anotaci\u00f3n storageclass.kubernetes.io/is-default-class . Advertencia Solo puede existir un StorageClass por defecto. De lo contrario, esta funcionalidad deja de operar.","title":"StorageClass por defecto"},{"location":"storage/volumes/","text":"Volumes Un Volume es una abstracci\u00f3n de Kubernetes que permite persistir los datos de los Containers y evitar la p\u00e9rdida de datos ante un reinicio, as\u00ed como que varios Containers dentro de un mismo Pod puedan compartir ficheros. Un Volume tiene el mismo tiempo de vida que un Pod. Es decir, cuando un Pod se crea, el Volume es una de las primeras cosas en crearse y cuando el Pod se destruye tambi\u00e9n lo hace el Volume. Para que un Pod pueda utilizar un Volume, debe especificar en su manifest qu\u00e9 Volumes tiene disponibles mediante la stanza .spec.volumes y dentro de la definici\u00f3n de cada Container d\u00f3nde se va a montar el Volume con volumeMounts . Cuidado! Un Volume no se puede montar dentro de otro Volume, ni pueden tener hard links hacia otros Volumes. Tipos de Volume Existen muchos tipos de Volume disponibles en Kubernetes. Algunos de los m\u00e1s comunes s\u00f3n: emptyDir : Permite montar un Volume en blanco en el almacenamiento del nodo del Pod. Si el Pod se redespliega en otro nodo, el Volume se destruye. Informaci\u00f3n Se puede montar un Volume tmpfs (sistema de ficheros en la RAM), que es m\u00e1s r\u00e1pido que un Volume sobre un disco, pero los datos se pueden perder si el nodo se reinicia. Por defecto el Volume se monta en el mismo medio (disco, SSD...) en el que est\u00e9 instalado el kubelet . Ejemplo apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : k8s.gcr.io/test-webserver name : test-container volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} configMap : Permite inyectar la informaci\u00f3n de un ConfigMap como un fichero dentro del contenedor para que pueda ser consumida por las aplicaciones. Ejemplo En este ejemplo se inyecta la entrada log_level de un ConfigMap de nombre log-config en un fichero /etc/config/log_level : apiVersion : v1 kind : Pod metadata : name : configmap-pod spec : containers : - name : test image : busybox volumeMounts : - name : config-vol mountPath : /etc/config volumes : - name : config-vol configMap : name : log-config items : - key : log_level path : log_level secret : Similar a los Volume configMap , pero para montar un Secret en un Pod. Ejemplo En este ejemplo se inyecta la entrada username de un Secret de nombre mysecret en un fichero /etc/foo/my-group/my-username : apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : mypod image : redis volumeMounts : - name : foo mountPath : \"/etc/foo\" readOnly : true volumes : - name : foo secret : secretName : mysecret items : - key : username path : my-group/my-username hostPath : Este tipo de Volume monta en el Pod, un fichero o directorio del sistema de ficheros del nodo en el que est\u00e1 el Pod. Esto puede ser \u00fatil para utilizar en uno de los Container alg\u00fan fichero de configuraci\u00f3n del nodo. Informaci\u00f3n Existen varios tipos de hostPath Volume : \"\" : (Por defecto) No se realiza ning\u00fan tipo de comprobaci\u00f3n a la hora de crear el Volume. DirectoryOrCreate : Comprueba que el directorio del host existe. Si no existe, el kubelet crea un directorio vac\u00edo con los permisos a 0755 . Directory : Comprueba que el directorio del host existe. FileOrCreate : Comprueba que el fichero del host existe. Si no existe, el kubelet crea un fichero vac\u00edo con los permisos a 0755 . El directorio en el que crear el fichero debe existir, de lo contrario dar\u00e1 error. File : Comprueba que el fichero del host existe. Socket : Comprueba que un socket UNIX existe en el path indicado. CharDevice : Comprueba que un dispositivo de car\u00e1cteres existe en el path indicado. BlockDevice : Comprueba que un dispositivo de bloque existe en el path indicado. Advertencia Los ficheros y directorios creados en el host, solo pueden ser editados por el usuario root . Para que un Container pueda escribir en un hostPath Volume debe de ejecutarse con privilegios o modificarse los permisos de los ficheros del host. Ejemplo apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : k8s.gcr.io/test-webserver name : test-container volumeMounts : - mountPath : /test-pd name : test-volume volumes : - name : test-volume hostPath : # directory location on host path : /data # this field is optional type : Directory PersistentVolumeClaim : Este tipo de Volume permite montar un PersistentVolume en un Pod mediante un PersistentVolumeClaim . Ejemplo apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : myfrontend image : nginx volumeMounts : - mountPath : \"/var/www/html\" name : mypd volumes : - name : mypd persistentVolumeClaim : claimName : myclaim local : Este Volume consiste en crear un PersistentVolume sobre el almacenamiento del nodo del Pod y usarlo en el Pod mediante un PersistentVolumeClaim (se recomienda el uso de StorageClass ). A diferencia del hostPath , los datos de este Volume no se eliminan si el Pod se despliega en otro nodo (ya que debe eliminarse de forma manual). As\u00ed mismo, el Scheduler siempre intentar\u00e1 desplegar el Pod en el mismo nodo en el que tiene creado el PersistentVolume , mientras que con hostPath , el Pod puede ser redesplegado en cualquier otro nodo. Ejemplo PersistentVolume : apiVersion : v1 kind : PersistentVolume metadata : name : example-pv spec : capacity : storage : 100Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Delete storageClassName : local-storage local : path : /mnt/disks/ssd1 nodeAffinity : required : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/hostname operator : In values : - example-node StorageClass : apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : local-storage provisioner : kubernetes.io/no-provisioner volumeBindingMode : WaitForFirstConsumer # Esta propiedad indica a Kubernetes # que debe de esperar a que el Pod haya sido # desplegado en un nodo, para poder vincular # el PersistentVolumeClaim del Pod con un # PersistentVolume StatefulSet : apiVersion : apps/v1 kind : StatefulSet metadata : name : local-test spec : serviceName : \"local-service\" replicas : 3 selector : matchLabels : app : local-test template : metadata : labels : app : local-test spec : containers : - name : test-container image : k8s.gcr.io/busybox command : - \"/bin/sh\" args : - \"-c\" - \"sleep 100000\" volumeMounts : - name : local-vol mountPath : /usr/test-pod volumeClaimTemplates : - metadata : name : local-vol spec : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"local-storage\" resources : requests : storage : 368Gi projected : Este tipo de Volume permite montar varias fuentes de datos en un mismo directorio. Informaci\u00f3n Estos son los tipos de fuentes que soporta: secret downwardAPI configMap serviceAccountToken Ejemplo apiVersion : v1 kind : Pod metadata : name : volume-test spec : containers : - name : container-test image : busybox volumeMounts : - name : all-in-one mountPath : \"/projected-volume\" readOnly : true volumes : - name : all-in-one projected : sources : - secret : name : mysecret items : - key : username path : my-group/my-username - secret : name : mysecret2 items : - key : password path : my-group/my-password mode : 511 - downwardAPI : items : - path : \"labels\" fieldRef : fieldPath : metadata.labels - path : \"cpu_limit\" resourceFieldRef : containerName : container-test resource : limits.cpu - configMap : name : myconfigmap items : - key : config path : my-group/my-config Propiedad subPath Mediante esta propiedad de los volumeMounts , se puede montar en el Container un subdirectorio del Volume, en vez de su ra\u00edz. Esto es \u00fatil cuando un Volume va a tener m\u00faltiples usos dentro de un mismo Pod. Advertencia Si esta propiedad se utiliza con un Volume de los siguientes tipos, el Volume montado no se actualizar\u00e1 cuando si lo hagan sus fuentes: configMap secret downwardAPI projected Ejemplo apiVersion : v1 kind : Pod metadata : name : my-lamp-site spec : containers : - name : mysql image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"rootpasswd\" volumeMounts : - mountPath : /var/lib/mysql name : site-data subPath : mysql - name : php image : php:7.0-apache volumeMounts : - mountPath : /var/www/html name : site-data subPath : html volumes : - name : site-data persistentVolumeClaim : claimName : my-lamp-site-data Propiedad subPathExpr Esta propiedad se puede utilizar en vez de la anterior, para expandir variables de entorno de la Downward API. Ejemplo En este ejemplo, se crea un directorio pod1 en el path /var/log/pods del host, y el directorio resultante /var/los/pods/pod1 se monta en el directorio /logs del \u0106ontainer: apiVersion : v1 kind : Pod metadata : name : pod1 spec : containers : - name : container1 env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name image : busybox command : [ \"sh\" , \"-c\" , \"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt\" ] volumeMounts : - name : workdir1 mountPath : /logs subPathExpr : $(POD_NAME) restartPolicy : Never volumes : - name : workdir1 hostPath : path : /var/log/pods","title":"Volumes"},{"location":"storage/volumes/#volumes","text":"Un Volume es una abstracci\u00f3n de Kubernetes que permite persistir los datos de los Containers y evitar la p\u00e9rdida de datos ante un reinicio, as\u00ed como que varios Containers dentro de un mismo Pod puedan compartir ficheros. Un Volume tiene el mismo tiempo de vida que un Pod. Es decir, cuando un Pod se crea, el Volume es una de las primeras cosas en crearse y cuando el Pod se destruye tambi\u00e9n lo hace el Volume. Para que un Pod pueda utilizar un Volume, debe especificar en su manifest qu\u00e9 Volumes tiene disponibles mediante la stanza .spec.volumes y dentro de la definici\u00f3n de cada Container d\u00f3nde se va a montar el Volume con volumeMounts . Cuidado! Un Volume no se puede montar dentro de otro Volume, ni pueden tener hard links hacia otros Volumes.","title":"Volumes"},{"location":"storage/volumes/#tipos-de-volume","text":"Existen muchos tipos de Volume disponibles en Kubernetes. Algunos de los m\u00e1s comunes s\u00f3n: emptyDir : Permite montar un Volume en blanco en el almacenamiento del nodo del Pod. Si el Pod se redespliega en otro nodo, el Volume se destruye. Informaci\u00f3n Se puede montar un Volume tmpfs (sistema de ficheros en la RAM), que es m\u00e1s r\u00e1pido que un Volume sobre un disco, pero los datos se pueden perder si el nodo se reinicia. Por defecto el Volume se monta en el mismo medio (disco, SSD...) en el que est\u00e9 instalado el kubelet . Ejemplo apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : k8s.gcr.io/test-webserver name : test-container volumeMounts : - mountPath : /cache name : cache-volume volumes : - name : cache-volume emptyDir : {} configMap : Permite inyectar la informaci\u00f3n de un ConfigMap como un fichero dentro del contenedor para que pueda ser consumida por las aplicaciones. Ejemplo En este ejemplo se inyecta la entrada log_level de un ConfigMap de nombre log-config en un fichero /etc/config/log_level : apiVersion : v1 kind : Pod metadata : name : configmap-pod spec : containers : - name : test image : busybox volumeMounts : - name : config-vol mountPath : /etc/config volumes : - name : config-vol configMap : name : log-config items : - key : log_level path : log_level secret : Similar a los Volume configMap , pero para montar un Secret en un Pod. Ejemplo En este ejemplo se inyecta la entrada username de un Secret de nombre mysecret en un fichero /etc/foo/my-group/my-username : apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : mypod image : redis volumeMounts : - name : foo mountPath : \"/etc/foo\" readOnly : true volumes : - name : foo secret : secretName : mysecret items : - key : username path : my-group/my-username hostPath : Este tipo de Volume monta en el Pod, un fichero o directorio del sistema de ficheros del nodo en el que est\u00e1 el Pod. Esto puede ser \u00fatil para utilizar en uno de los Container alg\u00fan fichero de configuraci\u00f3n del nodo. Informaci\u00f3n Existen varios tipos de hostPath Volume : \"\" : (Por defecto) No se realiza ning\u00fan tipo de comprobaci\u00f3n a la hora de crear el Volume. DirectoryOrCreate : Comprueba que el directorio del host existe. Si no existe, el kubelet crea un directorio vac\u00edo con los permisos a 0755 . Directory : Comprueba que el directorio del host existe. FileOrCreate : Comprueba que el fichero del host existe. Si no existe, el kubelet crea un fichero vac\u00edo con los permisos a 0755 . El directorio en el que crear el fichero debe existir, de lo contrario dar\u00e1 error. File : Comprueba que el fichero del host existe. Socket : Comprueba que un socket UNIX existe en el path indicado. CharDevice : Comprueba que un dispositivo de car\u00e1cteres existe en el path indicado. BlockDevice : Comprueba que un dispositivo de bloque existe en el path indicado. Advertencia Los ficheros y directorios creados en el host, solo pueden ser editados por el usuario root . Para que un Container pueda escribir en un hostPath Volume debe de ejecutarse con privilegios o modificarse los permisos de los ficheros del host. Ejemplo apiVersion : v1 kind : Pod metadata : name : test-pd spec : containers : - image : k8s.gcr.io/test-webserver name : test-container volumeMounts : - mountPath : /test-pd name : test-volume volumes : - name : test-volume hostPath : # directory location on host path : /data # this field is optional type : Directory PersistentVolumeClaim : Este tipo de Volume permite montar un PersistentVolume en un Pod mediante un PersistentVolumeClaim . Ejemplo apiVersion : v1 kind : Pod metadata : name : mypod spec : containers : - name : myfrontend image : nginx volumeMounts : - mountPath : \"/var/www/html\" name : mypd volumes : - name : mypd persistentVolumeClaim : claimName : myclaim local : Este Volume consiste en crear un PersistentVolume sobre el almacenamiento del nodo del Pod y usarlo en el Pod mediante un PersistentVolumeClaim (se recomienda el uso de StorageClass ). A diferencia del hostPath , los datos de este Volume no se eliminan si el Pod se despliega en otro nodo (ya que debe eliminarse de forma manual). As\u00ed mismo, el Scheduler siempre intentar\u00e1 desplegar el Pod en el mismo nodo en el que tiene creado el PersistentVolume , mientras que con hostPath , el Pod puede ser redesplegado en cualquier otro nodo. Ejemplo PersistentVolume : apiVersion : v1 kind : PersistentVolume metadata : name : example-pv spec : capacity : storage : 100Gi volumeMode : Filesystem accessModes : - ReadWriteOnce persistentVolumeReclaimPolicy : Delete storageClassName : local-storage local : path : /mnt/disks/ssd1 nodeAffinity : required : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/hostname operator : In values : - example-node StorageClass : apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : local-storage provisioner : kubernetes.io/no-provisioner volumeBindingMode : WaitForFirstConsumer # Esta propiedad indica a Kubernetes # que debe de esperar a que el Pod haya sido # desplegado en un nodo, para poder vincular # el PersistentVolumeClaim del Pod con un # PersistentVolume StatefulSet : apiVersion : apps/v1 kind : StatefulSet metadata : name : local-test spec : serviceName : \"local-service\" replicas : 3 selector : matchLabels : app : local-test template : metadata : labels : app : local-test spec : containers : - name : test-container image : k8s.gcr.io/busybox command : - \"/bin/sh\" args : - \"-c\" - \"sleep 100000\" volumeMounts : - name : local-vol mountPath : /usr/test-pod volumeClaimTemplates : - metadata : name : local-vol spec : accessModes : [ \"ReadWriteOnce\" ] storageClassName : \"local-storage\" resources : requests : storage : 368Gi projected : Este tipo de Volume permite montar varias fuentes de datos en un mismo directorio. Informaci\u00f3n Estos son los tipos de fuentes que soporta: secret downwardAPI configMap serviceAccountToken Ejemplo apiVersion : v1 kind : Pod metadata : name : volume-test spec : containers : - name : container-test image : busybox volumeMounts : - name : all-in-one mountPath : \"/projected-volume\" readOnly : true volumes : - name : all-in-one projected : sources : - secret : name : mysecret items : - key : username path : my-group/my-username - secret : name : mysecret2 items : - key : password path : my-group/my-password mode : 511 - downwardAPI : items : - path : \"labels\" fieldRef : fieldPath : metadata.labels - path : \"cpu_limit\" resourceFieldRef : containerName : container-test resource : limits.cpu - configMap : name : myconfigmap items : - key : config path : my-group/my-config","title":"Tipos de Volume"},{"location":"storage/volumes/#propiedad-subpath","text":"Mediante esta propiedad de los volumeMounts , se puede montar en el Container un subdirectorio del Volume, en vez de su ra\u00edz. Esto es \u00fatil cuando un Volume va a tener m\u00faltiples usos dentro de un mismo Pod. Advertencia Si esta propiedad se utiliza con un Volume de los siguientes tipos, el Volume montado no se actualizar\u00e1 cuando si lo hagan sus fuentes: configMap secret downwardAPI projected Ejemplo apiVersion : v1 kind : Pod metadata : name : my-lamp-site spec : containers : - name : mysql image : mysql env : - name : MYSQL_ROOT_PASSWORD value : \"rootpasswd\" volumeMounts : - mountPath : /var/lib/mysql name : site-data subPath : mysql - name : php image : php:7.0-apache volumeMounts : - mountPath : /var/www/html name : site-data subPath : html volumes : - name : site-data persistentVolumeClaim : claimName : my-lamp-site-data","title":"Propiedad subPath"},{"location":"storage/volumes/#propiedad-subpathexpr","text":"Esta propiedad se puede utilizar en vez de la anterior, para expandir variables de entorno de la Downward API. Ejemplo En este ejemplo, se crea un directorio pod1 en el path /var/log/pods del host, y el directorio resultante /var/los/pods/pod1 se monta en el directorio /logs del \u0106ontainer: apiVersion : v1 kind : Pod metadata : name : pod1 spec : containers : - name : container1 env : - name : POD_NAME valueFrom : fieldRef : apiVersion : v1 fieldPath : metadata.name image : busybox command : [ \"sh\" , \"-c\" , \"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt\" ] volumeMounts : - name : workdir1 mountPath : /logs subPathExpr : $(POD_NAME) restartPolicy : Never volumes : - name : workdir1 hostPath : path : /var/log/pods","title":"Propiedad subPathExpr"}]}